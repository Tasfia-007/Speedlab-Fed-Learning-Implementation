{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"How to Use This Version for Checkpoint Resume in Kaggle\n\nBefore running this notebook for the second time (to resume training from a checkpoint), follow the steps below carefully.\n\n1. Download the latest checkpoint file\nDownload the file named checkpoint_latest.pth from the previous version of your notebook or experiment.\n\n2. Upload the checkpoint to Kaggle Input Directory\nPlace the downloaded file inside your Kaggle input path, for example:\n/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\n\n3. Run the following code cell before starting training\nThis code will copy the checkpoint file to the working directory (/kaggle/working/checkpoints) so that training can resume from the saved state.\n\n4. Resume Training\nAfter the checkpoint file is copied successfully, running the rest of the notebook will automatically start training from the previous checkpoint instead of starting from scratch.\n\n\n\n\n## you can change the dataset and the attack type by simply changing the name in the args.py file.no need to modify anything else.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Source and destination paths\nsrc = \"/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\"\ndst_dir = \"/kaggle/working/checkpoints\"\ndst = os.path.join(dst_dir, \"checkpoint_latest.pth\")\n\n# Step 1: Check if source file exists\nif not os.path.exists(src):\n    print(f\"âŒ Source file not found: {src}\")\nelse:\n    print(f\"âœ… Found source file: {src}\")\n\n    # Step 2: Ensure destination directory exists\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(f\"ğŸ“‚ Created destination directory: {dst_dir}\")\n    else:\n        print(f\"ğŸ“ Destination directory already exists: {dst_dir}\")\n\n    # Step 3: Copy the file\n    shutil.copy(src, dst)\n    print(f\"âœ… Copied file to: {dst}\")\n\n    # Step 4: List all files in destination\n    files = os.listdir(dst_dir)\n    if files:\n        print(\"\\nğŸ“„ Files in /kaggle/working/checkpoints:\")\n        for f in files:\n            print(\" â”œâ”€â”€\", f)\n    else:\n        print(\"âš  Destination directory is empty (unexpected).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Target directory\nbase_dir = \"/kaggle/working/FedAvg-PyTorch\"\nos.makedirs(base_dir, exist_ok=True)\n\n# Python files to create\nfiles = [\"main.py\", \"server.py\", \"client.py\", \"model.py\", \"get_data.py\", \"args.py\"]\n\n# Create each file if not exists\nfor file in files:\n    file_path = os.path.join(base_dir, file)\n    if not os.path.exists(file_path):\n        with open(file_path, \"w\") as f:\n            f.write(f\"# {file} â€” auto-created placeholder\\n\")\n        print(f\" Created: {file_path}\")\n    else:\n        print(f\" Already exists: {file_path}\")\n\nprint(\"\\n Folder and files ready in:\", base_dir)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-11T02:59:56.116186Z","iopub.execute_input":"2025-11-11T02:59:56.116974Z","iopub.status.idle":"2025-11-11T02:59:56.125997Z","shell.execute_reply.started":"2025-11-11T02:59:56.116948Z","shell.execute_reply":"2025-11-11T02:59:56.125279Z"},"trusted":true},"outputs":[{"name":"stdout","text":" Created: /kaggle/working/FedAvg-PyTorch/main.py\n Created: /kaggle/working/FedAvg-PyTorch/server.py\n Created: /kaggle/working/FedAvg-PyTorch/client.py\n Created: /kaggle/working/FedAvg-PyTorch/model.py\n Created: /kaggle/working/FedAvg-PyTorch/get_data.py\n Created: /kaggle/working/FedAvg-PyTorch/args.py\n\n Folder and files ready in: /kaggle/working/FedAvg-PyTorch\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"rm -rf /kaggle/working/data\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.127239Z","iopub.execute_input":"2025-11-11T02:59:56.127450Z","iopub.status.idle":"2025-11-11T02:59:56.248320Z","shell.execute_reply.started":"2025-11-11T02:59:56.127433Z","shell.execute_reply":"2025-11-11T02:59:56.247460Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedAvg-PyTorch/args.py\"\n\nnew_code = '''\n# ========================================\n# args.py â€” FedAvg Configuration (with dataset configs)\n# ========================================\nimport argparse\nimport torch\n\n# Dataset configurations - Change dataset name here to switch\nDATASET_CONFIGS = {\n    'pathmnist': {\n        'num_classes': 9,\n        'class_names': [\n            \"adipose tissue\", \"background\", \"debris\", \"lymphocytes\",\n            \"mucus\", \"smooth muscle\", \"normal colon mucosa\",\n            \"cancer-associated stroma\", \"colorecal adenocarcinoma epithelium\"\n        ],\n        'input_channels': 3\n    },\n    'tissuemnist': {\n        'num_classes': 8,\n        'class_names': [\n            \"collecting duct\", \"distal convoluted tubule\",\n            \"glomerular endothelial cells\", \"interstitial endothelial cells\",\n            \"leukocytes\", \"podocytes\", \"proximal tubule\", \"thick ascending limb\"\n        ],\n        'input_channels': 1\n    },\n    'organamnist': {\n        'num_classes': 11,\n        'class_names': [\n            \"bladder\", \"femur-left\", \"femur-right\", \"heart\",\n            \"kidney-left\", \"kidney-right\", \"liver\",\n            \"lung-left\", \"lung-right\", \"spleen\", \"pelvis\"\n        ],\n        'input_channels': 1\n    },\n    'octmnist': {\n        'num_classes': 4,\n        'class_names': [\n            \"choroidal neovascularization\", \"diabetic macular edema\",\n            \"drusen\", \"normal\"\n        ],\n        'input_channels': 1\n    }\n}\n\ndef args_parser():\n    parser = argparse.ArgumentParser(description=\"FedAvg - Config File\")\n    \n    # -------------------------------\n    # MAIN: Change this to switch dataset\n    # -------------------------------\n    parser.add_argument('--dataset', type=str, default='organamnist', \n                        choices=['pathmnist', 'tissuemnist', 'organamnist', 'octmnist'],\n                        help='MedMNIST dataset to use')\n    \n    # -------------------------------\n    # Federated Learning Parameters\n    # -------------------------------\n    parser.add_argument('--E', type=int, default=5, help='local epochs')\n    parser.add_argument('--r', type=int, default=50, help='number of communication rounds')\n    parser.add_argument('--K', type=int, default=5, help='total number of clients')\n    parser.add_argument('--C', type=float, default=1, help='client sampling rate per round')\n    parser.add_argument('--B', type=int, default=32, help='batch size')\n    parser.add_argument('--use_combined', action='store_true', \n                        help='Use train+val+test combined (all data)')\n\n    # -------------------------------\n    # Model parameters (auto-configured)\n    # -------------------------------\n    parser.add_argument('--clip_model', type=str, default='ViT-B/32', help='CLIP model variant')\n    parser.add_argument('--freeze_clip', action='store_true', help='Freeze CLIP backbone')\n    parser.add_argument('--dropout', type=float, default=0.5, help='dropout (stronger regularization)')\n\n    # -------------------------------\n    # Optimizer Settings\n    # -------------------------------\n    parser.add_argument('--lr', type=float, default=0.003, help='learning rate')\n    parser.add_argument('--optimizer', type=str, default='sgd', help='optimizer')\n    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9, help='momentum for SGD')\n\n    # -------------------------------\n    # Checkpoint Settings\n    # -------------------------------\n    parser.add_argument('--checkpoint_dir', type=str,\n                        default='./checkpoints',\n                        help='directory to save checkpoints')\n    \n    # -------------------------------\n    # Device\n    # -------------------------------\n    parser.add_argument('--device', default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n                        help='cuda or cpu')\n    \n    # -------------------------------\n    # Balanced Non-IID Settings\n    # -------------------------------\n    parser.add_argument('--dominant_ratio', type=float, default=0.7,\n                        help='Fraction of dominant class per client (0-1)')\n\n    args = parser.parse_args(args=[])\n    \n    # Auto-configure based on selected dataset\n    if args.dataset in DATASET_CONFIGS:\n        config = DATASET_CONFIGS[args.dataset]\n        args.num_classes = config['num_classes']\n        args.input_channels = config['input_channels']\n        args.class_names = config['class_names']\n    else:\n        raise ValueError(f\"Dataset {args.dataset} not configured!\")\n    \n    return args\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" args.py updated!\")\nprint(f\" File path: {file_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.249418Z","iopub.execute_input":"2025-11-11T02:59:56.250000Z","iopub.status.idle":"2025-11-11T02:59:56.258231Z","shell.execute_reply.started":"2025-11-11T02:59:56.249972Z","shell.execute_reply":"2025-11-11T02:59:56.257487Z"},"trusted":true},"outputs":[{"name":"stdout","text":" args.py updated!\n File path: /kaggle/working/FedAvg-PyTorch/args.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedAvg-PyTorch/get_data.py\"\n\nnew_code = r'''\n# ========================================\n# get_data.py â€” True Balanced Non-IID Distribution\n# Supports: pathmnist, tissuemnist, organamnist, octmnist\n# ========================================\nimport os\nimport numpy as np\nimport torch\nimport pickle\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset\nfrom torchvision import transforms\nfrom medmnist import INFO\nfrom medmnist.dataset import PathMNIST, TissueMNIST, OrganAMNIST, OCTMNIST\n\n# Map dataset names to classes\nDATASET_MAP = {\n    'pathmnist': PathMNIST,\n    'tissuemnist': TissueMNIST,\n    'organamnist': OrganAMNIST,\n    'octmnist': OCTMNIST\n}\n\ndef get_transforms():\n    \"\"\"Returns train and test transforms (handles grayscale and RGB)\"\"\"\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.Grayscale(num_output_channels=3),  # ensures 3 channels\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n    \n    return train_transform, test_transform\n\ndef balanced_noniid_split(dataset, num_clients, dominant_ratio=0.7):\n    \"\"\"Balanced Non-IID split: dominant_ratio% to main client, rest distributed\"\"\"\n    labels = np.array([dataset[i][1].item() for i in range(len(dataset))])\n    num_classes = len(np.unique(labels))\n    client_indices = [[] for _ in range(num_clients)]\n\n    # Prepare class indices\n    class_indices = {c: np.where(labels == c)[0].tolist() for c in range(num_classes)}\n    for c in class_indices:\n        np.random.shuffle(class_indices[c])\n\n    # Step 1: assign dominant_ratio% of dominant class to primary client\n    for client_id in range(num_clients):\n        dominant_class = client_id % num_classes\n        n_dominant = int(len(class_indices[dominant_class]) * dominant_ratio)\n        if n_dominant > 0:\n            client_indices[client_id].extend(class_indices[dominant_class][:n_dominant])\n            class_indices[dominant_class] = class_indices[dominant_class][n_dominant:]\n\n    # Step 2: distribute remaining samples equally among other clients\n    for c in range(num_classes):\n        remaining = class_indices[c]\n        np.random.shuffle(remaining)\n        other_clients = [i for i in range(num_clients) if i % num_classes != c]\n        for i, idx in enumerate(remaining):\n            client_id = other_clients[i % len(other_clients)]\n            client_indices[client_id].append(idx)\n\n    # Shuffle each client's indices\n    for i in range(num_clients):\n        np.random.shuffle(client_indices[i])\n\n    return client_indices\n\ndef load_medmnist_data(args):\n    \"\"\"Load MedMNIST data - automatically configured based on args.dataset\"\"\"\n    train_transform, test_transform = get_transforms()\n    data_root = './data/medmnist'\n    os.makedirs(data_root, exist_ok=True)\n\n    data_flag = args.dataset.lower()\n    if data_flag not in DATASET_MAP:\n        raise ValueError(f\"Dataset {data_flag} not supported. Choose from {list(DATASET_MAP.keys())}\")\n\n    n_classes = args.num_classes\n\n    print(\"\\n\" + \"=\"*60)\n    print(f\" Loading {data_flag.upper()} Dataset\")\n    print(\"=\"*60)\n    print(f\"Number of classes: {n_classes}\")\n\n    DatasetClass = DATASET_MAP[data_flag]\n\n    # Load datasets\n    train_dataset = DatasetClass(root=data_root, split='train', download=True, transform=train_transform)\n    val_dataset = DatasetClass(root=data_root, split='val', download=True, transform=train_transform)\n    test_dataset = DatasetClass(root=data_root, split='test', download=True, transform=test_transform)\n\n    combined_train = ConcatDataset([train_dataset, val_dataset])\n    print(f\" Using Train+Val: {len(combined_train)} samples for federated learning\")\n\n    # Load or create client indices\n    cache_file = f'./data/medmnist/client_indices_{data_flag}_K{args.K}_dr{args.dominant_ratio}.pkl'\n    if os.path.exists(cache_file):\n        print(f\"\\n Loading cached split from: {cache_file}\")\n        with open(cache_file, 'rb') as f:\n            client_indices = pickle.load(f)\n    else:\n        print(f\"\\n Creating balanced Non-IID split (dominant_ratio={args.dominant_ratio})...\")\n        client_indices = balanced_noniid_split(combined_train, args.K, dominant_ratio=args.dominant_ratio)\n        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n        with open(cache_file, 'wb') as f:\n            pickle.dump(client_indices, f)\n        print(f\" Split cached to: {cache_file}\")\n\n    # Print client-wise class distribution\n    print(\"\\n\" + \"=\"*60)\n    print(\" Client-wise Image Distribution\")\n    print(\"=\"*60)\n    total_samples = 0\n    for client_id, indices in enumerate(client_indices):\n        client_labels = [combined_train[i][1].item() for i in indices]\n        label_counts = np.bincount(client_labels, minlength=n_classes)\n        total_client = len(indices)\n        total_samples += total_client\n        distribution_str = \", \".join([f\"C{c}:{label_counts[c]}\" for c in range(n_classes)])\n        dominant_class = np.argmax(label_counts)\n        dominant_pct = (label_counts[dominant_class] / total_client) * 100\n        print(f\"Client {client_id:2d}: {total_client:5d} samples | Dominant: Class {dominant_class} ({dominant_pct:.1f}%)\")\n        print(f\"           [{distribution_str}]\")\n\n    print(f\"\\nTotal samples: {total_samples}\")\n    print(\"=\"*60 + \"\\n\")\n\n    # Create DataLoaders\n    client_loaders = []\n    for i, indices in enumerate(client_indices):\n        subset = Subset(combined_train, indices)\n        loader = DataLoader(subset, batch_size=args.B, shuffle=True, num_workers=0, pin_memory=True)\n        client_loaders.append(loader)\n\n    test_loader = DataLoader(test_dataset, batch_size=args.B, shuffle=False, num_workers=0, pin_memory=True)\n    return client_loaders, test_loader\n\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" get_data.py updated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.269931Z","iopub.execute_input":"2025-11-11T02:59:56.270143Z","iopub.status.idle":"2025-11-11T02:59:56.277709Z","shell.execute_reply.started":"2025-11-11T02:59:56.270126Z","shell.execute_reply":"2025-11-11T02:59:56.277055Z"},"trusted":true},"outputs":[{"name":"stdout","text":" get_data.py updated!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedAvg-PyTorch/client.py\"\nnew_code = \"\"\"\n# ========================================\n# client.py â€” Client-side helper for FedAvg \n# ========================================\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport copy\n\nclass Client:\n    def __init__(self, model, train_loader, device, val_loader=None, lr=0.0001, weight_decay=5e-4):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=self.weight_decay)\n    \n    def compute_accuracy(self, loader):\n        self.model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for images, labels in loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                labels = labels.squeeze()\n                outputs = self.model(images)\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        return 100 * correct / total if total > 0 else 0\n    \n    def train(self, epochs=1):\n        print(f\"\\\\n Training {self.model.name} with FedAvg objective...\")\n        for epoch in range(epochs):\n            self.model.train()\n            total_loss = 0.0\n            pbar = tqdm(self.train_loader,\n                        desc=f\"  Epoch {epoch+1}/{epochs}\",\n                        ncols=100,\n                        leave=False)\n            \n            for images, labels in pbar:\n                images, labels = images.to(self.device), labels.to(self.device)\n                labels = labels.squeeze()\n                \n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                \n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n            \n            avg_loss = total_loss / len(self.train_loader)\n            train_acc = self.compute_accuracy(self.train_loader)\n            log_msg = f\"  Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\"\n            \n            if self.val_loader is not None:\n                val_acc = self.compute_accuracy(self.val_loader)\n                val_loss = 0\n                self.model.eval()\n                with torch.no_grad():\n                    for images, labels in self.val_loader:\n                        images, labels = images.to(self.device), labels.to(self.device)\n                        labels = labels.squeeze()\n                        outputs = self.model(images)\n                        if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                            outputs = outputs.logits\n                        loss = self.criterion(outputs, labels)\n                        val_loss += loss.item()\n                val_loss /= len(self.val_loader)\n                log_msg += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n            \n            print(log_msg)\n        \n        print(f\" {self.model.name} local training complete (FedAvg)\\\\n\")\n        return self.model.state_dict()\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" client.py updated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.279123Z","iopub.execute_input":"2025-11-11T02:59:56.279461Z","iopub.status.idle":"2025-11-11T02:59:56.293575Z","shell.execute_reply.started":"2025-11-11T02:59:56.279438Z","shell.execute_reply":"2025-11-11T02:59:56.292657Z"},"trusted":true},"outputs":[{"name":"stdout","text":" client.py updated!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedAvg-PyTorch/model.py\"\nnew_code = '''\n# ========================================\n# model.py â€” CLIP-based Classifier for MedMNIST (Auto-configured)\n# ========================================\nimport torch\nfrom torch import nn\nimport clip\n\nclass CLIPMedMNISTClassifier(nn.Module):\n    def __init__(self, args, name='clip_model'):\n        super(CLIPMedMNISTClassifier, self).__init__()\n        self.name = name\n        \n        # Load pretrained CLIP\n        self.clip_model, self.preprocess = clip.load(\"ViT-B/32\", device=args.device)\n        \n        # Get class names from args (auto-configured based on dataset)\n        self.class_names = args.class_names\n        self.num_classes = args.num_classes\n        \n        # Freeze text encoder completely\n        for param in self.clip_model.transformer.parameters():\n            param.requires_grad = False\n        for param in self.clip_model.token_embedding.parameters():\n            param.requires_grad = False\n        for param in self.clip_model.ln_final.parameters():\n            param.requires_grad = False\n        self.clip_model.positional_embedding.requires_grad = False\n        self.clip_model.text_projection.requires_grad = False\n        \n        # Precompute and freeze text embeddings\n        with torch.no_grad():\n            text_tokens = clip.tokenize([f\"a microscopic image of {c}\" for c in self.class_names]).to(args.device)\n            text_features = self.clip_model.encode_text(text_tokens)\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n        \n        # Register as buffer\n        self.register_buffer('text_features', text_features)\n    \n    def forward(self, images):\n        image_features = self.clip_model.encode_image(images)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        logits = 100.0 * image_features @ self.text_features.T\n        return logits\n    \n    def get_trainable_params(self):\n        return [p for p in self.parameters() if p.requires_grad]\n    \n    def freeze_layers(self, num_layers_to_freeze):\n        if num_layers_to_freeze > 0:\n            for i, layer in enumerate(self.clip_model.visual.transformer.resblocks):\n                if i < num_layers_to_freeze:\n                    for param in layer.parameters():\n                        param.requires_grad = False\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" model.py updated!\")\nprint(f\" File saved at: {file_path}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.294352Z","iopub.execute_input":"2025-11-11T02:59:56.294611Z","iopub.status.idle":"2025-11-11T02:59:56.307539Z","shell.execute_reply.started":"2025-11-11T02:59:56.294588Z","shell.execute_reply":"2025-11-11T02:59:56.306920Z"},"trusted":true},"outputs":[{"name":"stdout","text":" model.py updated!\n File saved at: /kaggle/working/FedAvg-PyTorch/model.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedAvg-PyTorch/server.py\"\n\nnew_code = \"\"\"\n# ========================================\n# server.py â€” FedAvg \n# ========================================\nimport copy\nimport random\nimport numpy as np\nimport torch\nimport os\nimport json\nfrom sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\nfrom model import CLIPMedMNISTClassifier as ImageClassifier\nfrom get_data import load_medmnist_data\nfrom client import Client\n\nclass FedAvgServer:\n    def __init__(self, args):\n        self.args = args\n        os.makedirs(args.checkpoint_dir, exist_ok=True)\n        self.current_round = 0\n        self.best_global_acc = 0\n        self.history = {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []}\n        self.global_model = ImageClassifier(args, name=\"server\").to(args.device)\n        self.client_models = []\n        for i in range(self.args.K):\n            model = copy.deepcopy(self.global_model)\n            model.name = f\"Client_{i}\"\n            self.client_models.append(model)\n        self.client_loaders, self.test_loader = load_medmnist_data(args)\n\n        latest_ckpt = os.path.join(args.checkpoint_dir, \"checkpoint_latest.pth\")\n        if os.path.exists(latest_ckpt):\n            self.load_checkpoint(latest_ckpt)\n            print(f\"\\\\n Resuming training automatically from Round {self.current_round}\")\n        else:\n            print(\"\\\\n Starting new training session from Round 0\")\n\n    def load_checkpoint(self, checkpoint_path):\n        print(f\"\\\\n Loading checkpoint from: {checkpoint_path}\")\n        checkpoint = torch.load(latest_checkpoint, map_location=args.device, weights_only=False)\n        self.global_model.load_state_dict(checkpoint['server_state_dict'])\n        if 'client_state_dicts' in checkpoint:\n            for i, state_dict in enumerate(checkpoint['client_state_dicts']):\n                self.client_models[i].load_state_dict(state_dict)\n        self.current_round = checkpoint.get('round', 0)\n        self.best_global_acc = checkpoint.get('best_global_acc', 0)\n        self.history = checkpoint.get('history', {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []})\n\n    def dispatch(self, selected_clients):\n        for idx in selected_clients:\n            client_model = self.client_models[idx]\n            for client_param, global_param in zip(client_model.parameters(), self.global_model.parameters()):\n                client_param.data = global_param.data.clone()\n\n    def aggregate(self, selected_clients):\n        total_samples = sum([len(self.client_loaders[idx].dataset) for idx in selected_clients])\n        global_params = {k: torch.zeros_like(v.data) for k, v in self.global_model.named_parameters()}\n        for idx in selected_clients:\n            weight = len(self.client_loaders[idx].dataset) / total_samples\n            client_params = dict(self.client_models[idx].named_parameters())\n            for k in global_params.keys():\n                global_params[k] += client_params[k].data * weight\n        for k, v in self.global_model.named_parameters():\n            v.data = global_params[k].data.clone()\n\n    def client_update(self, idx):\n        client_model = self.client_models[idx]\n        client_loader = self.client_loaders[idx]\n        client_obj = Client(model=client_model,\n                            train_loader=client_loader,\n                            device=self.args.device,\n                            lr=self.args.lr,\n                            weight_decay=self.args.weight_decay)\n        client_obj.train(epochs=self.args.E)\n\n    def test_global_model(self):\n        self.global_model.eval()\n        all_labels = []\n        all_preds = []\n\n        with torch.no_grad():\n            for images, labels in self.test_loader:\n                images, labels = images.to(self.args.device), labels.to(self.args.device)\n                labels = labels.squeeze()\n                outputs = self.global_model(images)\n\n                # Handle CLIP-like outputs\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    if hasattr(outputs, 'logits'):\n                        outputs = outputs.logits\n                    else:\n                        outputs = outputs[0]\n\n                _, predicted = torch.max(outputs, 1)\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(predicted.cpu().numpy())\n\n        # Convert to numpy arrays\n        all_labels = np.array(all_labels)\n        all_preds = np.array(all_preds)\n\n        # compute metrics\n        acc = 100.0 * np.mean(all_labels == all_preds)\n        # for multi-class, use macro averaging; zero_division=0 to avoid exceptions\n        try:\n            precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n            recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n            f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        except Exception:\n            precision, recall, f1 = 0.0, 0.0, 0.0\n\n        rmse = float(np.sqrt(mean_squared_error(all_labels, all_preds)))\n\n        metrics = {\n            'accuracy': acc,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n            'rmse': rmse\n        }\n        return metrics\n\n    def save_checkpoint(self, round_num, metrics):\n        avg_acc = metrics['accuracy']\n        checkpoint = {\n            'round': round_num,\n            'server_state_dict': self.global_model.state_dict(),\n            'client_state_dicts': [model.state_dict() for model in self.client_models],\n            'best_global_acc': self.best_global_acc,\n            'history': self.history,\n            'args': vars(self.args)\n        }\n        \n        torch.save(checkpoint, os.path.join(self.args.checkpoint_dir, 'checkpoint_latest.pth'))\n        \n        with open(os.path.join(self.args.checkpoint_dir, 'training_history.json'), 'w') as f:\n            json.dump(self.history, f, indent=4)\n\n    def run(self):\n        start_round = self.current_round\n        for r in range(start_round, self.args.r):\n            print(f\"\\\\n{'='*60}\")\n            print(f\" Round [{r+1}/{self.args.r}]\")\n            print('='*60)\n            \n            m = max(int(self.args.C * self.args.K), 1)\n            selected_clients = random.sample(range(self.args.K), m)\n            \n            #  SHOW SELECTED CLIENTS ARRAY\n            print(f\" Selected Clients: {selected_clients}\")\n            print(f\" Total Clients Selected: {len(selected_clients)}/{self.args.K}\")\n            \n            self.dispatch(selected_clients)\n            \n            for idx in selected_clients:\n                print(f\"\\\\n Training Client {idx}...\")\n                self.client_update(idx)\n            \n            self.aggregate(selected_clients)\n            \n            metrics = self.test_global_model()\n            avg_acc = metrics['accuracy']\n            \n            self.history['rounds'].append(r+1)\n            self.history['avg_accuracy'].append(avg_acc)\n            self.history['best_accuracy'].append(max(self.best_global_acc, avg_acc))\n            \n            if avg_acc > self.best_global_acc:\n                self.best_global_acc = avg_acc\n            \n            self.current_round = r + 1\n            self.save_checkpoint(r+1, metrics)\n            \n            #  PRINT ALL METRICS AFTER EACH ROUND\n            print(f\"\\\\n{'='*60}\")\n            print(f\" Round [{r+1}/{self.args.r}] Test Results:\")\n            print(f\"{'='*60}\")\n            print(f\"    Accuracy:  {metrics['accuracy']:.2f}%\")\n            print(f\"    Precision: {metrics['precision']:.4f}\")\n            print(f\"    Recall:    {metrics['recall']:.4f}\")\n            print(f\"    F1-Score:  {metrics['f1']:.4f}\")\n            print(f\"    RMSE:      {metrics['rmse']:.4f}\")\n            print(f\"    Best Acc:  {self.best_global_acc:.2f}%\")\n            print('='*60)\n            \n        return self.global_model\n\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" server.py updated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.308917Z","iopub.execute_input":"2025-11-11T02:59:56.309118Z","iopub.status.idle":"2025-11-11T02:59:56.324883Z","shell.execute_reply.started":"2025-11-11T02:59:56.309096Z","shell.execute_reply":"2025-11-11T02:59:56.324216Z"},"trusted":true},"outputs":[{"name":"stdout","text":" server.py updated!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedAvg-PyTorch/main.py\"\nnew_code = \"\"\"\n# ========================================\n# main.py â€” Run FedAvg \n# ========================================\nfrom args import args_parser\nfrom server import FedAvgServer\nimport torch\nimport os\nimport json\nimport matplotlib.pyplot as plt\n\ndef plot_training_history(history, save_path):\n    \n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Average Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history['rounds'], history['avg_accuracy'], 'b-', label='Avg Accuracy', linewidth=2)\n    plt.plot(history['rounds'], history['best_accuracy'], 'r--', label='Best Accuracy', linewidth=2)\n    plt.xlabel('Communication Round')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Federated Learning Accuracy Progress')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Accuracy Improvement\n    plt.subplot(1, 2, 2)\n    if len(history['avg_accuracy']) > 1:\n        improvements = [history['avg_accuracy'][i] - history['avg_accuracy'][i-1] \n                        for i in range(1, len(history['avg_accuracy']))]\n        plt.bar(history['rounds'][1:], improvements, alpha=0.7)\n        plt.axhline(y=0, color='r', linestyle='-', linewidth=0.5)\n        plt.xlabel('Communication Round')\n        plt.ylabel('Accuracy Change (%)')\n        plt.title('Round-to-Round Accuracy Change')\n        plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f' Training plot saved at: {save_path}')\n\ndef main():\n    # Load arguments from args.py\n    args = args_parser()\n    \n    # Print configuration\n    print('\\\\n' + '='*60)\n    print(' FEDERATED LEARNING CONFIGURATION')\n    print('='*60)\n    print(f'Dataset: {args.dataset.upper()}')\n    print(f'Number of Classes: {args.num_classes}')\n    print(f'Input Channels: {args.input_channels}')\n    print(f'Clients: {args.K} | Rounds: {args.r} | Local Epochs: {args.E}')\n    print(f'Batch Size: {args.B} | Learning Rate: {args.lr}')\n    print(f'Weight Decay: {args.weight_decay}')\n    print(f'CLIP Model: {args.clip_model} | Device: {args.device}')\n    print(f'Non-IID: {args.dominant_ratio*100:.0f}% dominant class per client')\n    print('='*60)\n    \n    # Initialize FedAvg server (auto-resumes from checkpoint if available)\n    server = FedAvgServer(args)\n    \n    # Run federated training\n    final_model = server.run()\n    \n    # Save final global model\n    final_model_path = os.path.join(args.checkpoint_dir, 'final_global_model.pth')\n    torch.save(final_model.state_dict(), final_model_path)\n    print(f'\\\\n Final global model saved at: {final_model_path}')\n    \n    # Save best model if exists\n    best_model_path = os.path.join(args.checkpoint_dir, 'best_model.pth')\n    if os.path.exists(best_model_path):\n        print(f' Best global model saved at: {best_model_path}')\n    \n    # Save training history\n    history_path = os.path.join(args.checkpoint_dir, 'training_history.json')\n    with open(history_path, 'w') as f:\n        json.dump(server.history, f, indent=4)\n    print(f' Training history saved at: {history_path}')\n    \n    # Plot training curves\n    plot_path = os.path.join(args.checkpoint_dir, 'training_plot.png')\n    plot_training_history(server.history, plot_path)\n    \n    # Print summary\n    print('\\\\n' + '='*60)\n    print(' TRAINING SUMMARY')\n    print('='*60)\n    print(f' Best Global Accuracy: {server.best_global_acc:.2f}%')\n    print(f' Final Round Accuracy: {server.history[\"avg_accuracy\"][-1]:.2f}%')\n    print(f' Total Improvement: {server.history[\"avg_accuracy\"][-1] - server.history[\"avg_accuracy\"][0]:.2f}%')\n    print(f' All checkpoints saved in: {args.checkpoint_dir}')\n    print('='*60)\n\nif __name__ == '__main__':\n    main()\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" main.py updated!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.325575Z","iopub.execute_input":"2025-11-11T02:59:56.325837Z","iopub.status.idle":"2025-11-11T02:59:56.338829Z","shell.execute_reply.started":"2025-11-11T02:59:56.325813Z","shell.execute_reply":"2025-11-11T02:59:56.338120Z"},"trusted":true},"outputs":[{"name":"stdout","text":" main.py updated!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install medmnist --quiet\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:59:56.339579Z","iopub.execute_input":"2025-11-11T02:59:56.339919Z","iopub.status.idle":"2025-11-11T03:01:14.920504Z","shell.execute_reply.started":"2025-11-11T02:59:56.339896Z","shell.execute_reply":"2025-11-11T03:01:14.919764Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T03:01:14.921593Z","iopub.execute_input":"2025-11-11T03:01:14.921946Z","iopub.status.idle":"2025-11-11T03:01:25.708362Z","shell.execute_reply.started":"2025-11-11T03:01:14.921915Z","shell.execute_reply":"2025-11-11T03:01:25.707653Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-pjny1z_i\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-pjny1z_i\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=4b7650b95aba6e84d3f619a4e0ca9f26e32589980449b4b5e1a80cd7c1abbc59\n  Stored in directory: /tmp/pip-ephem-wheel-cache-im16y3on/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!python /kaggle/working/FedAvg-PyTorch/main.py","metadata":{"execution":{"iopub.status.busy":"2025-11-11T03:01:25.709286Z","iopub.execute_input":"2025-11-11T03:01:25.709490Z","execution_failed":"2025-11-11T03:50:24.473Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n============================================================\n FEDERATED LEARNING CONFIGURATION\n============================================================\nDataset: ORGANAMNIST\nNumber of Classes: 11\nInput Channels: 1\nClients: 5 | Rounds: 50 | Local Epochs: 5\nBatch Size: 32 | Learning Rate: 0.003\nWeight Decay: 0.0005\nCLIP Model: ViT-B/32 | Device: cuda\nNon-IID: 70% dominant class per client\n============================================================\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:02<00:00, 158MiB/s]\n\n============================================================\n Loading ORGANAMNIST Dataset\n============================================================\nNumber of classes: 11\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.2M/38.2M [00:43<00:00, 879kB/s]\n Using Train+Val: 41052 samples for federated learning\n\n Creating balanced Non-IID split (dominant_ratio=0.7)...\n Split cached to: ./data/medmnist/client_indices_organamnist_K5_dr0.7.pkl\n\n============================================================\n Client-wise Image Distribution\n============================================================\nClient  0:  8151 samples | Dominant: Class 0 (19.5%)\n           [C0:1593, C1:122, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\nClient  1:  7743 samples | Dominant: Class 6 (18.6%)\n           [C0:171, C1:1136, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\nClient  2:  7714 samples | Dominant: Class 6 (18.7%)\n           [C0:171, C1:122, C2:1107, C3:140, C4:340, C5:891, C6:1439, C7:990, C8:988, C9:712, C10:814]\nClient  3:  7891 samples | Dominant: Class 6 (18.2%)\n           [C0:171, C1:122, C2:119, C3:1306, C4:340, C5:891, C6:1439, C7:990, C8:987, C9:712, C10:814]\nClient  4:  9553 samples | Dominant: Class 4 (33.2%)\n           [C0:171, C1:121, C2:118, C3:140, C4:3171, C5:890, C6:1439, C7:990, C8:987, C9:712, C10:814]\n\nTotal samples: 41052\n============================================================\n\n\n Starting new training session from Round 0\n\n============================================================\n Round [1/50]\n============================================================\n Selected Clients: [2, 3, 0, 1, 4]\n Total Clients Selected: 5/5\n\n Training Client 2...\n\n Training Client_2 with FedAvg objective...\n  Epoch  1/5 | Loss: 2.0855 | Train Acc: 31.66%                                                     \n  Epoch  2/5 | Loss: 1.7072 | Train Acc: 37.06%                                                     \n  Epoch  3/5 | Loss: 1.6276 | Train Acc: 41.31%                                                     \n  Epoch  4/5 | Loss: 1.4790 | Train Acc: 40.54%                                                     \n  Epoch  5/5 | Loss: 1.4125 | Train Acc: 52.09%                                                     \n Client_2 local training complete (FedAvg)\n\n\n Training Client 3...\n\n Training Client_3 with FedAvg objective...\n  Epoch  1/5 | Loss: 2.2720 | Train Acc: 28.20%                                                     \n  Epoch  2/5 | Loss: 1.8414 | Train Acc: 26.30%                                                     \n  Epoch  3/5 | Loss: 1.7940 | Train Acc: 29.79%                                                     \n  Epoch  4/5 | Loss: 1.7845 | Train Acc: 35.42%                                                     \n  Epoch  5/5 | Loss: 1.7210 | Train Acc: 37.87%                                                     \n Client_3 local training complete (FedAvg)\n\n\n Training Client 0...\n\n Training Client_0 with FedAvg objective...\n  Epoch  1/5 | Loss: 2.0663 | Train Acc: 34.46%                                                     \n  Epoch  2/5 | Loss: 1.6155 | Train Acc: 44.29%                                                     \n  Epoch  3/5 | Loss: 1.4144 | Train Acc: 51.80%                                                     \n  Epoch  4/5 | Loss: 1.2876 | Train Acc: 54.90%                                                     \n  Epoch  5/5 | Loss: 1.1331 | Train Acc: 65.13%                                                     \n Client_0 local training complete (FedAvg)\n\n\n Training Client 1...\n\n Training Client_1 with FedAvg objective...\n  Epoch  1/5 | Loss: 2.0463 | Train Acc: 30.61%                                                     \n  Epoch  2/5 | Loss: 1.7193 | Train Acc: 38.86%                                                     \n  Epoch  3/5 | Loss: 1.6899 | Train Acc: 41.39%                                                     \n  Epoch  4/5 | Loss: 1.6707 | Train Acc: 39.48%                                                     \n  Epoch  5/5 | Loss: 1.6620 | Train Acc: 41.34%                                                     \n Client_1 local training complete (FedAvg)\n\n\n Training Client 4...\n\n Training Client_4 with FedAvg objective...\n  Epoch  1/5 | Loss: 2.1753 | Train Acc: 35.04%                                                     \n  Epoch  2/5 | Loss: 1.7066 | Train Acc: 37.49%                                                     \n  Epoch  3/5 | Loss: 1.6324 | Train Acc: 41.23%                                                     \n  Epoch  4/5 | Loss: 1.5653 | Train Acc: 46.22%                                                     \n  Epoch  5/5 | Loss: 1.4686 | Train Acc: 50.69%                                                     \n Client_4 local training complete (FedAvg)\n\n\n============================================================\n Round [1/50] Test Results:\n============================================================\n    Accuracy:  11.16%\n    Precision: 0.0438\n    Recall:    0.0918\n    F1-Score:  0.0203\n    RMSE:      2.9222\n    Best Acc:  11.16%\n============================================================\n\n============================================================\n Round [2/50]\n============================================================\n Selected Clients: [4, 1, 0, 3, 2]\n Total Clients Selected: 5/5\n\n Training Client 4...\n\n Training Client_4 with FedAvg objective...\n  Epoch  1/5 | Loss: 1.6255 | Train Acc: 46.45%                                                     \n  Epoch  2/5 | Loss: 1.2357 | Train Acc: 60.46%                                                     \n  Epoch  3/5 | Loss: 1.0267 | Train Acc: 71.26%                                                     \n  Epoch  4/5 | Loss: 0.8416 | Train Acc: 73.09%                                                     \n  Epoch  5/5 | Loss: 0.7639 | Train Acc: 72.25%                                                     \n Client_4 local training complete (FedAvg)\n\n\n Training Client 1...\n\n Training Client_1 with FedAvg objective...\n  Epoch  1/5 | Loss: 1.6131 | Train Acc: 46.78%                                                     \n  Epoch  2/5 | Loss: 1.2672 | Train Acc: 63.81%                                                     \n  Epoch  3/5 | Loss: 1.0236 | Train Acc: 68.23%                                                     \n  Epoch  4/5 | Loss: 0.8389 | Train Acc: 71.11%                                                     \n  Epoch  5/5 | Loss: 0.7505 | Train Acc: 73.56%                                                     \n Client_1 local training complete (FedAvg)\n\n\n Training Client 0...\n\n Training Client_0 with FedAvg objective...\n  Epoch  1/5 | Loss: 1.5221 | Train Acc: 56.56%                                                     \n  Epoch  2/5 | Loss: 1.1835 | Train Acc: 65.93%                                                     \n  Epoch  3/5 | Loss: 0.9528 | Train Acc: 69.33%                                                     \n  Epoch  4/5 | Loss: 0.8060 | Train Acc: 76.00%                                                     \n  Epoch  5/5 | Loss: 0.6757 | Train Acc: 78.74%                                                     \n Client_0 local training complete (FedAvg)\n\n\n Training Client 3...\n\n Training Client_3 with FedAvg objective...\n  Epoch  1/5 | Loss: 1.5479 | Train Acc: 53.07%                                                     \n  Epoch  2/5 | Loss: 1.1508 | Train Acc: 66.30%                                                     \n  Epoch  3/5 | Loss: 0.9840 | Train Acc: 67.43%                                                     \n  Epoch  4/5 | Loss: 0.7752 | Train Acc: 76.99%                                                     \n  Epoch  5/5 | Loss: 0.6774 | Train Acc: 73.02%                                                     \n Client_3 local training complete (FedAvg)\n\n\n Training Client 2...\n\n Training Client_2 with FedAvg objective...\n  Epoch  1/5 | Loss: 1.6110 | Train Acc: 36.00%                                                     \n  Epoch  2/5 | Loss: 1.3307 | Train Acc: 63.27%                                                     \n  Epoch  3/5 | Loss: 1.0259 | Train Acc: 69.07%                                                     \n  Epoch  4/5 | Loss: 0.8662 | Train Acc: 51.79%                                                     \n  Epoch  5/5 | Loss: 0.7660 | Train Acc: 69.64%                                                     \n Client_2 local training complete (FedAvg)\n\n\n============================================================\n Round [2/50] Test Results:\n============================================================\n    Accuracy:  61.55%\n    Precision: 0.6060\n    Recall:    0.5971\n    F1-Score:  0.5676\n    RMSE:      2.4345\n    Best Acc:  61.55%\n============================================================\n\n============================================================\n Round [3/50]\n============================================================\n Selected Clients: [1, 3, 2, 4, 0]\n Total Clients Selected: 5/5\n\n Training Client 1...\n\n Training Client_1 with FedAvg objective...\n  Epoch  1/5 | Loss: 0.7446 | Train Acc: 74.69%                                                     \n                                                                                                    \r","output_type":"stream"}],"execution_count":null}]}