{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"How to Use This Version for Checkpoint Resume in Kaggle\n\nBefore running this notebook for the second time (to resume training from a checkpoint), follow the steps below carefully.\n\n1. Download the latest checkpoint file\nDownload the file named checkpoint_latest.pth from the previous version of your notebook or experiment.\n\n2. Upload the checkpoint to Kaggle Input Directory\nPlace the downloaded file inside your Kaggle input path, for example:\n/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\n\n3. Run the following code cell before starting training\nThis code will copy the checkpoint file to the working directory (/kaggle/working/checkpoints) so that training can resume from the saved state.\n\n4. Resume Training\nAfter the checkpoint file is copied successfully, running the rest of the notebook will automatically start training from the previous checkpoint instead of starting from scratch.\n\n\n\n## you can change the dataset and the attack type by simply changing the name in the args.py file.no need to modify anything else.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Source and destination paths\nsrc = \"/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\"\ndst_dir = \"/kaggle/working/checkpoints\"\ndst = os.path.join(dst_dir, \"checkpoint_latest.pth\")\n\n# Step 1: Check if source file exists\nif not os.path.exists(src):\n    print(f\"âŒ Source file not found: {src}\")\nelse:\n    print(f\"âœ… Found source file: {src}\")\n\n    # Step 2: Ensure destination directory exists\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(f\"ğŸ“‚ Created destination directory: {dst_dir}\")\n    else:\n        print(f\"ğŸ“ Destination directory already exists: {dst_dir}\")\n\n    # Step 3: Copy the file\n    shutil.copy(src, dst)\n    print(f\"âœ… Copied file to: {dst}\")\n\n    # Step 4: List all files in destination\n    files = os.listdir(dst_dir)\n    if files:\n        print(\"\\nğŸ“„ Files in /kaggle/working/checkpoints:\")\n        for f in files:\n            print(\" â”œâ”€â”€\", f)\n    else:\n        print(\"âš  Destination directory is empty (unexpected).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Target directory\nbase_dir = \"/kaggle/working/FedBn-PyTorch\"\nos.makedirs(base_dir, exist_ok=True)\n\n# Python files to create\nfiles = [\"main.py\", \"server.py\", \"client.py\", \"model.py\", \"get_data.py\", \"args.py\"]\n\n# Create each file if not exists\nfor file in files:\n    file_path = os.path.join(base_dir, file)\n    if not os.path.exists(file_path):\n        with open(file_path, \"w\") as f:\n            f.write(f\"# {file} â€” auto-created placeholder\\n\")\n        print(f\" Created: {file_path}\")\n    else:\n        print(f\" Already exists: {file_path}\")\n\nprint(\"\\n Folder and files ready in:\", base_dir)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-11T06:45:25.310902Z","iopub.execute_input":"2025-11-11T06:45:25.311156Z","iopub.status.idle":"2025-11-11T06:45:25.321298Z","shell.execute_reply.started":"2025-11-11T06:45:25.311129Z","shell.execute_reply":"2025-11-11T06:45:25.320573Z"},"trusted":true},"outputs":[{"name":"stdout","text":" Created: /kaggle/working/FedBn-PyTorch/main.py\n Created: /kaggle/working/FedBn-PyTorch/server.py\n Created: /kaggle/working/FedBn-PyTorch/client.py\n Created: /kaggle/working/FedBn-PyTorch/model.py\n Created: /kaggle/working/FedBn-PyTorch/get_data.py\n Created: /kaggle/working/FedBn-PyTorch/args.py\n\n Folder and files ready in: /kaggle/working/FedBn-PyTorch\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedBn-PyTorch/args.py\"\n\nnew_code = '''\n# ========================================\n# args.py â€” FedBN Configuration \n# ========================================\nimport argparse\nimport torch\n\nDATASET_CONFIGS = {\n    'pathmnist': {'num_classes': 9, 'input_channels': 3},\n    'tissuemnist': {'num_classes': 8, 'input_channels': 1},\n    'organamnist': {'num_classes': 11, 'input_channels': 1},\n    'octmnist': {'num_classes': 4, 'input_channels': 1},\n}\n\ndef args_parser():\n    parser = argparse.ArgumentParser(description=\"FedBN Config (4 Dataset Compatible)\")\n    \n    # -------------------------------\n    # Dataset\n    # -------------------------------\n    parser.add_argument('--dataset', type=str, default='organamnist',\n                        choices=['pathmnist', 'tissuemnist', 'organamnist', 'octmnist'],\n                        help='MedMNIST dataset to use')\n    parser.add_argument('--use_combined', action='store_true', \n                        help='Use train+val+test combined')\n\n    # -------------------------------\n    # Federated Learning Parameters\n    # -------------------------------\n    parser.add_argument('--E', type=int, default=5, help='local epochs')\n    parser.add_argument('--r', type=int, default=50, help='communication rounds')\n    parser.add_argument('--K', type=int, default=5, help='number of clients')\n    parser.add_argument('--C', type=float, default=1.0, help='client sampling rate')\n    parser.add_argument('--B', type=int, default=32, help='batch size')\n\n    # -------------------------------\n    # Model Parameters\n    # -------------------------------\n    parser.add_argument('--clip_model', type=str, default='ViT-B/32', help='CLIP model variant')\n    parser.add_argument('--freeze_clip', action='store_true', help='Freeze CLIP backbone')\n    parser.add_argument('--dropout', type=float, default=0.5, help='dropout rate')\n\n    # -------------------------------\n    # Optimizer Settings\n    # -------------------------------\n    parser.add_argument('--lr', type=float, default=0.003, help='learning rate')\n    parser.add_argument('--optimizer', type=str, default='sgd', help='optimizer type')\n    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9, help='momentum for SGD')\n\n    # -------------------------------\n    # FedBN Specific\n    # -------------------------------\n    parser.add_argument('--exclude_bn_from_agg', action='store_true',\n                        help='Exclude BN params from aggregation')\n    parser.add_argument('--freeze_bn', action='store_true', help='Freeze BN layers')\n    \n    # -------------------------------\n    # Others\n    # -------------------------------\n    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints_fedbn')\n    parser.add_argument('--device', default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    parser.add_argument('--dominant_ratio', type=float, default=0.7, help='dominant ratio for Non-IID split')\n\n    args = parser.parse_args(args=[])\n\n    if args.dataset in DATASET_CONFIGS:\n        cfg = DATASET_CONFIGS[args.dataset]\n        args.num_classes = cfg['num_classes']\n        args.input_channels = cfg['input_channels']\n    else:\n        raise ValueError(f\"Unsupported dataset {args.dataset}\")\n\n    return args\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" args.py written successfully!\")\nprint(f\" File path: {file_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:45:25.322181Z","iopub.execute_input":"2025-11-11T06:45:25.322680Z","iopub.status.idle":"2025-11-11T06:45:25.340639Z","shell.execute_reply.started":"2025-11-11T06:45:25.322653Z","shell.execute_reply":"2025-11-11T06:45:25.339656Z"},"trusted":true},"outputs":[{"name":"stdout","text":" args.py written successfully!\n File path: /kaggle/working/FedBn-PyTorch/args.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedBn-PyTorch/get_data.py\"\n\nnew_code = r'''\n# ========================================\n# get_data.py â€” Balanced Non-IID Loader (4 Dataset Compatible)\n# ========================================\nimport os\nimport numpy as np\nimport torch\nimport pickle\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset\nfrom torchvision import transforms\nfrom medmnist.dataset import PathMNIST, TissueMNIST, OrganAMNIST, OCTMNIST\n\nDATASET_MAP = {\n    'pathmnist': PathMNIST,\n    'tissuemnist': TissueMNIST,\n    'organamnist': OrganAMNIST,\n    'octmnist': OCTMNIST,\n}\n\ndef get_transforms():\n    \"\"\"Unified transforms (grayscale â†’ RGB for CLIP)\"\"\"\n    train_t = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.Grayscale(num_output_channels=3),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n    ])\n    test_t = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n    ])\n    return train_t, test_t\n\ndef balanced_noniid_split(dataset, num_clients, dominant_ratio=0.7):\n    labels = np.array([dataset[i][1].item() for i in range(len(dataset))])\n    n_classes = len(np.unique(labels))\n    client_indices = [[] for _ in range(num_clients)]\n\n    class_indices = {c: np.where(labels == c)[0].tolist() for c in range(n_classes)}\n    for c in class_indices: np.random.shuffle(class_indices[c])\n\n    for client_id in range(num_clients):\n        dom_class = client_id % n_classes\n        n_dom = int(len(class_indices[dom_class]) * dominant_ratio)\n        client_indices[client_id].extend(class_indices[dom_class][:n_dom])\n        class_indices[dom_class] = class_indices[dom_class][n_dom:]\n\n    for c in range(n_classes):\n        rem = class_indices[c]\n        np.random.shuffle(rem)\n        others = [i for i in range(num_clients) if i % n_classes != c]\n        for i, idx in enumerate(rem):\n            client_indices[others[i % len(others)]].append(idx)\n    for i in range(num_clients):\n        np.random.shuffle(client_indices[i])\n    return client_indices\n\ndef load_medmnist_data(args):\n    train_t, test_t = get_transforms()\n    root = './data/medmnist'\n    os.makedirs(root, exist_ok=True)\n    Dataset = DATASET_MAP[args.dataset]\n\n    train = Dataset(root=root, split='train', download=True, transform=train_t)\n    val = Dataset(root=root, split='val', download=True, transform=train_t)\n    test = Dataset(root=root, split='test', download=True, transform=test_t)\n    combined = ConcatDataset([train, val])\n\n    cache = f'./data/medmnist/client_idx_{args.dataset}_K{args.K}_dr{args.dominant_ratio}_fedbn.pkl'\n    if os.path.exists(cache):\n        with open(cache, 'rb') as f:\n            client_idx = pickle.load(f)\n    else:\n        client_idx = balanced_noniid_split(combined, args.K, args.dominant_ratio)\n        with open(cache, 'wb') as f:\n            pickle.dump(client_idx, f)\n\n    client_loaders = [\n        DataLoader(Subset(combined, idx), batch_size=args.B, shuffle=True, num_workers=0)\n        for idx in client_idx\n    ]\n    test_loader = DataLoader(test, batch_size=args.B, shuffle=False, num_workers=0)\n    return client_loaders, test_loader\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" get_data.py created for FedBN!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:45:25.341936Z","iopub.execute_input":"2025-11-11T06:45:25.342310Z","iopub.status.idle":"2025-11-11T06:45:25.362905Z","shell.execute_reply.started":"2025-11-11T06:45:25.342292Z","shell.execute_reply":"2025-11-11T06:45:25.362141Z"},"trusted":true},"outputs":[{"name":"stdout","text":" get_data.py created for FedBN!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedBn-PyTorch/client.py\"\n\nnew_code = \"\"\"# ========================================\n# client.py â€” Client-side helper for FedBN (with gradient clipping)\n# ========================================\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport copy\n\nclass Client:\n    def __init__(self, model, train_loader, device, val_loader=None,\n                 lr=0.001, weight_decay=1e-4, use_fedbn=True):\n        \\\"\\\"\\\"\n        Args:\n            use_fedbn (bool): If True, BN layers remain local (not aggregated)\n        \\\"\\\"\\\"\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.use_fedbn = use_fedbn\n\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n\n    def compute_accuracy(self, loader):\n        \\\"\\\"\\\"Compute accuracy on given DataLoader\\\"\\\"\\\"\n        self.model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for images, labels in loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                labels = labels.squeeze()\n                if labels.dim() == 0:\n                    labels = labels.unsqueeze(0)\n                if labels.size(0) == 0:\n                    continue\n                outputs = self.model(images)\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        return 100 * correct / total if total > 0 else 0\n\n    def train(self, epochs=1):\n        \\\"\\\"\\\"Local training (BN layers remain local; only non-BN parameters aggregated)\\\"\\\"\\\"\n        print()\n        print(\" Training client model with FedBN objective...\")\n        \n        for epoch in range(epochs):\n            self.model.train()\n            total_loss = 0.0\n            batch_count = 0\n\n            pbar = tqdm(self.train_loader,\n                        desc=f\"  Epoch {epoch+1}/{epochs}\",\n                        ncols=100,\n                        leave=False)\n\n            for images, labels in pbar:\n                images, labels = images.to(self.device), labels.to(self.device)\n                labels = labels.squeeze()\n                if labels.dim() == 0:\n                    labels = labels.unsqueeze(0)\n                if labels.size(0) == 0:\n                    continue\n\n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                loss = self.criterion(outputs, labels)\n                \n                # Check for NaN and skip if found\n                if torch.isnan(loss):\n                    print(\"  NaN detected, skipping batch\")\n                    continue\n                \n                loss.backward()\n                \n                # Gradient clipping for stability\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                \n                self.optimizer.step()\n\n                total_loss += loss.item()\n                batch_count += 1\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n            \n            avg_loss = total_loss / batch_count if batch_count > 0 else 0\n            train_acc = self.compute_accuracy(self.train_loader)\n            log_msg = f\"  Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\"\n\n            if self.val_loader is not None:\n                val_acc = self.compute_accuracy(self.val_loader)\n                val_loss, val_batches = 0.0, 0\n                self.model.eval()\n                with torch.no_grad():\n                    for images, labels in self.val_loader:\n                        images, labels = images.to(self.device), labels.to(self.device)\n                        labels = labels.squeeze()\n                        if labels.dim() == 0:\n                            labels = labels.unsqueeze(0)\n                        if labels.size(0) == 0:\n                            continue\n                        outputs = self.model(images)\n                        if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                            outputs = outputs.logits\n                        loss = self.criterion(outputs, labels)\n                        val_loss += loss.item()\n                        val_batches += 1\n                val_loss = val_loss / val_batches if val_batches > 0 else 0\n                log_msg += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n\n            print(log_msg)\n        \n        print(\" Client local training complete (FedBN)\")\n        print()\n        return self.model.state_dict()\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" client.py updated!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:45:25.459561Z","iopub.execute_input":"2025-11-11T06:45:25.460116Z","iopub.status.idle":"2025-11-11T06:45:25.467223Z","shell.execute_reply.started":"2025-11-11T06:45:25.460095Z","shell.execute_reply":"2025-11-11T06:45:25.466579Z"},"trusted":true},"outputs":[{"name":"stdout","text":" client.py updated!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedBn-PyTorch/model.py\"\n\nnew_code = '''# ========================================\n# model.py â€” CLIP-based FedBN (BN Local Only)\n# ========================================\nimport torch\nfrom torch import nn\nimport clip\n\nclass CLIPFedBNClassifier(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.clip_model, _ = clip.load(args.clip_model, device=args.device)\n\n        # Freeze visual encoder\n        for p in self.clip_model.parameters():\n            p.requires_grad = False\n\n        feature_dim = self.clip_model.visual.output_dim\n        self.fc1 = nn.Linear(feature_dim, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(256, args.num_classes)\n        self.bn2 = nn.BatchNorm1d(args.num_classes)\n        self.relu = nn.ReLU()\n        self.drop = nn.Dropout(args.dropout)\n\n    def forward(self, x):\n        with torch.no_grad():\n            img_feat = self.clip_model.encode_image(x).float()\n        x = self.fc1(img_feat)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        return x\n\n    def get_bn_params(self):\n        \"\"\"Return BN params (excluded from aggregation)\"\"\"\n        bn_params = []\n        for n, m in self.named_modules():\n            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                for p in ['weight', 'bias', 'running_mean', 'running_var']:\n                    bn_params.append(f\"{n}.{p}\")\n        return bn_params\n\n'''\n\nwith open(file_path, \"w\", newline='\\n') as f:\n    f.write(new_code)\n\nprint(\"model.py updated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:45:25.468719Z","iopub.execute_input":"2025-11-11T06:45:25.468965Z","iopub.status.idle":"2025-11-11T06:45:25.487027Z","shell.execute_reply.started":"2025-11-11T06:45:25.468947Z","shell.execute_reply":"2025-11-11T06:45:25.486198Z"},"trusted":true},"outputs":[{"name":"stdout","text":"model.py updated!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedBn-PyTorch/server.py\"\n\nnew_code = '''# ========================================\n# server.py - FedBN Server (CLIP-based, Auto Resume)\n# ========================================\nimport copy\nimport random\nimport numpy as np\nimport torch\nimport os\nimport json\nfrom model import CLIPFedBNClassifier as ImageClassifier\nfrom get_data import load_medmnist_data\nfrom client import Client\nfrom sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n\nclass FedBNServer:\n    def __init__(self, args):\n        self.args = args\n        os.makedirs(args.checkpoint_dir, exist_ok=True)\n        self.current_round = 0\n        self.best_global_acc = 0\n        self.history = {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []}\n\n        #  Initialize global CLIP model (no 'name' param)\n        self.global_model = ImageClassifier(args).to(args.device)\n\n        # Collect BN parameter names for exclusion\n        self.bn_params = self._get_bn_param_names()\n        print(f\\\" Excluded BN parameters: {len(self.bn_params)} params\\\")\n\n        # Create client models\n        self.client_models = []\n        for i in range(self.args.K):\n            model = copy.deepcopy(self.global_model)\n            self.client_models.append(model)\n\n        # Load MedMNIST client datasets\n        self.client_loaders, self.test_loader = load_medmnist_data(args)\n\n        #  Auto resume if checkpoint exists\n        latest_ckpt = os.path.join(args.checkpoint_dir, 'checkpoint_latest.pth')\n        if os.path.exists(latest_ckpt):\n            print(f\\\" Found checkpoint at {latest_ckpt}. Resuming training...\\\")\n            self.load_checkpoint(latest_ckpt)\n        else:\n            print(\\\" No checkpoint found. Starting training from scratch.\\\")\n\n    def _get_bn_param_names(self):\n        bn_params = set()\n        for name, module in self.global_model.named_modules():\n            if isinstance(module, (torch.nn.BatchNorm2d, torch.nn.BatchNorm1d)):\n                bn_params.update({\n                    f\\\"{name}.weight\\\", f\\\"{name}.bias\\\",\n                    f\\\"{name}.running_mean\\\", f\\\"{name}.running_var\\\",\n                    f\\\"{name}.num_batches_tracked\\\"\n                })\n        return bn_params\n\n    def _is_bn_param(self, param_name):\n        return param_name in self.bn_params\n\n    def dispatch(self, selected_clients):\n        global_dict = self.global_model.state_dict()\n        for idx in selected_clients:\n            client_dict = self.client_models[idx].state_dict()\n            for key in global_dict.keys():\n                if not self._is_bn_param(key):\n                    client_dict[key] = global_dict[key].clone()\n            self.client_models[idx].load_state_dict(client_dict)\n\n    def aggregate(self, selected_clients):\n        total_samples = sum([len(self.client_loaders[idx].dataset) for idx in selected_clients])\n        global_dict = self.global_model.state_dict()\n        aggregated = {k: torch.zeros_like(v) for k, v in global_dict.items()}\n\n        for idx in selected_clients:\n            weight = len(self.client_loaders[idx].dataset) / total_samples\n            client_dict = self.client_models[idx].state_dict()\n            for key in aggregated.keys():\n                if not self._is_bn_param(key):\n                    aggregated[key] += client_dict[key] * weight\n                else:\n                    aggregated[key] = global_dict[key]\n\n        self.global_model.load_state_dict(aggregated)\n\n    def client_update(self, idx):\n        client_model = self.client_models[idx]\n        client_loader = self.client_loaders[idx]\n        client_obj = Client(\n            model=client_model,\n            train_loader=client_loader,\n            device=self.args.device,\n            lr=self.args.lr,\n            weight_decay=self.args.weight_decay,\n            use_fedbn=True\n        )\n        client_obj.train(epochs=self.args.E)\n\n    def test_global_model(self):\n        self.global_model.eval()\n        all_labels, all_preds = [], []\n\n        with torch.no_grad():\n            for images, labels in self.test_loader:\n                images, labels = images.to(self.args.device), labels.to(self.args.device)\n                labels = labels.squeeze()\n                outputs = self.global_model(images)\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                _, predicted = torch.max(outputs, 1)\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(predicted.cpu().numpy())\n\n        all_labels, all_preds = np.array(all_labels), np.array(all_preds)\n        acc = 100 * np.mean(all_labels == all_preds)\n        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n\n        print(f\\\" Global Test â€” Acc: {acc:.2f}% | Prec: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | RMSE: {rmse:.3f}\\\")\n        return acc\n\n    def load_checkpoint(self, checkpoint_path):\n        print(f\\\"\\\n Loading checkpoint: {checkpoint_path}\\\")\n        checkpoint = torch.load(checkpoint_path, map_location=self.args.device, weights_only=False)\n        self.global_model.load_state_dict(checkpoint['server_state_dict'])\n        if 'client_state_dicts' in checkpoint:\n            for i, state_dict in enumerate(checkpoint['client_state_dicts']):\n                self.client_models[i].load_state_dict(state_dict)\n        self.current_round = checkpoint.get('round', 0)\n        self.best_global_acc = checkpoint.get('best_global_acc', 0)\n        self.history = checkpoint.get('history', {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []})\n        print(f\\\" Resumed from Round {self.current_round} | Best Acc: {self.best_global_acc:.2f}%\\\")\n\n    def save_checkpoint(self, round_num, avg_acc):\n        checkpoint = {\n            'round': round_num,\n            'server_state_dict': self.global_model.state_dict(),\n            'client_state_dicts': [m.state_dict() for m in self.client_models],\n            'best_global_acc': self.best_global_acc,\n            'avg_accuracy': avg_acc,\n            'history': self.history,\n            'args': vars(self.args)\n        }\n       \n        torch.save(checkpoint, os.path.join(self.args.checkpoint_dir, 'checkpoint_latest.pth'))\n      \n        with open(os.path.join(self.args.checkpoint_dir, 'training_history.json'), 'a') as f:\n            json.dump(self.history, f, indent=4)\n\n    def run(self):\n        start_round = self.current_round\n        for r in range(start_round, self.args.r):\n            print(f'\\\n{\\\"=\\\"*60}')\n            print(f'Round {r+1}/{self.args.r}')\n            print(f'{\\\"=\\\"*60}')\n            m = max(int(self.args.C * self.args.K), 1)\n            selected_clients = random.sample(range(self.args.K), m)\n            print(f\\\"Selected clients: {selected_clients}\\\")\n            self.dispatch(selected_clients)\n            for idx in selected_clients:\n                self.client_update(idx)\n            self.aggregate(selected_clients)\n            avg_acc = self.test_global_model()\n            self.history['rounds'].append(r+1)\n            self.history['avg_accuracy'].append(avg_acc)\n            self.history['best_accuracy'].append(max(self.best_global_acc, avg_acc))\n            if avg_acc > self.best_global_acc:\n                self.best_global_acc = avg_acc\n                print(f\\\" NEW BEST: {self.best_global_acc:.2f}%\\\")\n            self.current_round = r + 1\n            self.save_checkpoint(r+1, avg_acc)\n    \n        print(f'\\\n{\\\"=\\\"*60}')\n        print(f' Training Complete! Best Accuracy: {self.best_global_acc:.2f}%')\n        print(f'{\\\"=\\\"*60}')\n        return self.global_model\n\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" FedBN server.py uppdated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:45:25.488030Z","iopub.execute_input":"2025-11-11T06:45:25.488396Z","iopub.status.idle":"2025-11-11T06:45:25.510831Z","shell.execute_reply.started":"2025-11-11T06:45:25.488346Z","shell.execute_reply":"2025-11-11T06:45:25.510084Z"},"trusted":true},"outputs":[{"name":"stdout","text":" FedBN server.py uppdated!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedBn-PyTorch/main.py\"\n\nnew_code = '''# ========================================\n# main.py â€” Run FedBN with CLIP model (Auto Resume)\n# ========================================\nfrom args import args_parser\nfrom server import FedBNServer\nimport torch\nimport os\nimport json\nimport matplotlib.pyplot as plt\n\ndef plot_training_history(history, save_path):\n    \"\"\"Plot training accuracy curves\"\"\"\n    if not history['rounds']:\n        print(\"  No training history to plot\")\n        return\n    \n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Average Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history['rounds'], history['avg_accuracy'], 'b-', label='Avg Accuracy', linewidth=2)\n    plt.plot(history['rounds'], history['best_accuracy'], 'r--', label='Best Accuracy', linewidth=2)\n    plt.xlabel('Communication Round')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Federated Learning Accuracy Progress')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Accuracy Improvement\n    plt.subplot(1, 2, 2)\n    if len(history['avg_accuracy']) > 1:\n        improvements = [history['avg_accuracy'][i] - history['avg_accuracy'][i-1] \n                        for i in range(1, len(history['avg_accuracy']))]\n        plt.bar(history['rounds'][1:], improvements, alpha=0.7)\n        plt.axhline(y=0, color='r', linestyle='-', linewidth=0.5)\n        plt.xlabel('Communication Round')\n        plt.ylabel('Accuracy Change (%)')\n        plt.title('Round-to-Round Accuracy Change')\n        plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f' Training plot saved at: {save_path}')\n\ndef main():\n    # Load arguments\n    args = args_parser()\n    \n    # Print configuration\n    print('\\\\n' + '='*60)\n    print(' FEDBN TRAINING CONFIGURATION (CLIP Model)')\n    print('='*60)\n    print(f'Device: {args.device}')\n    print(f'Clients: {args.K} | Rounds: {args.r} | Local Epochs: {args.E}')\n    print(f'Batch Size: {args.B} | Learning Rate: {args.lr}')\n    print(f'Non-IID: {args.dominant_ratio*100:.0f}% dominant class per client')\n    print(f'Checkpoint Directory: {args.checkpoint_dir}')\n    print('='*60)\n    \n    # Check if checkpoint exists (just for info - server will auto-resume)\n    latest_checkpoint = os.path.join(args.checkpoint_dir, 'checkpoint_latest.pth')\n    if os.path.exists(latest_checkpoint):\n        print('\\\\n' + '='*60)\n        print(' CHECKPOINT FOUND - Will auto-resume')\n        print('='*60)\n        try:\n            checkpoint = torch.load(latest_checkpoint, map_location=args.device, weights_only=False)\n            completed_rounds = checkpoint.get('round', 0)\n            best_acc = checkpoint.get('best_global_acc', 0)\n            print(f' Checkpoint Details:')\n            print(f'   - Completed Rounds: {completed_rounds}/{args.r}')\n            print(f'   - Best Accuracy: {best_acc:.2f}%')\n        except Exception as e:\n            print(f'  Error reading checkpoint: {e}')\n    else:\n        print('\\\\n' + '='*60)\n        print(' No checkpoint found. Starting fresh training...')\n        print('='*60)\n    \n    # Initialize FedBN server (auto-resumes if checkpoint exists)\n    server = FedBNServer(args)\n    \n    # Run federated training\n    final_model = server.run()\n    \n    # Save final global model\n    final_model_path = os.path.join(args.checkpoint_dir, 'final_global_model.pth')\n    torch.save(final_model.state_dict(), final_model_path)\n    print(f'\\\\n Final global model saved at: {final_model_path}')\n    \n\n    \n    # Plot training curves\n    plot_path = os.path.join(args.checkpoint_dir, 'training_plot.png')\n    plot_training_history(server.history, plot_path)\n    \n    # Print summary\n    print('\\\\n' + '='*60)\n    print(' TRAINING SUMMARY')\n    print('='*60)\n    print(f' Best Global Accuracy: {server.best_global_acc:.2f}%')\n    if server.history['avg_accuracy']:\n        print(f' Final Round Accuracy: {server.history[\"avg_accuracy\"][-1]:.2f}%')\n        if len(server.history['avg_accuracy']) > 1:\n            improvement = server.history['avg_accuracy'][-1] - server.history['avg_accuracy'][0]\n            print(f' Total Improvement: {improvement:.2f}%')\n    print(f' All checkpoints saved in: {args.checkpoint_dir}')\n    print('='*60)\n\nif __name__ == '__main__':\n    main()\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" FedBN main.py saved successfully!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:45:25.512508Z","iopub.execute_input":"2025-11-11T06:45:25.512768Z","iopub.status.idle":"2025-11-11T06:45:25.529898Z","shell.execute_reply.started":"2025-11-11T06:45:25.512747Z","shell.execute_reply":"2025-11-11T06:45:25.529072Z"},"trusted":true},"outputs":[{"name":"stdout","text":" FedBN main.py saved successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install medmnist --quiet","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:45:25.530702Z","iopub.execute_input":"2025-11-11T06:45:25.531000Z","iopub.status.idle":"2025-11-11T06:46:41.012118Z","shell.execute_reply.started":"2025-11-11T06:45:25.530974Z","shell.execute_reply":"2025-11-11T06:46:41.011123Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:46:41.013260Z","iopub.execute_input":"2025-11-11T06:46:41.014326Z","iopub.status.idle":"2025-11-11T06:46:51.812538Z","shell.execute_reply.started":"2025-11-11T06:46:41.014288Z","shell.execute_reply":"2025-11-11T06:46:51.811484Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-dk2ydheh\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-dk2ydheh\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=56dfd9f67d4483512338965fefd20a53e154adefc036e112e19b505b08f44b5c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-annf6ndu/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python /kaggle/working/FedBn-PyTorch/main.py","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:46:51.813757Z","iopub.execute_input":"2025-11-11T06:46:51.814083Z","execution_failed":"2025-11-11T07:24:20.056Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n============================================================\n FEDBN TRAINING CONFIGURATION (CLIP Model)\n============================================================\nDevice: cuda\nClients: 5 | Rounds: 50 | Local Epochs: 5\nBatch Size: 32 | Learning Rate: 0.003\nNon-IID: 70% dominant class per client\nCheckpoint Directory: ./checkpoints_fedbn\n============================================================\n\n============================================================\n No checkpoint found. Starting fresh training...\n============================================================\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:01<00:00, 185MiB/s]\n Excluded BN parameters: 10 params\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.2M/38.2M [00:52<00:00, 722kB/s]\n No checkpoint found. Starting training from scratch.\n============================================================\nRound 1/50\n============================================================\nSelected clients: [4, 0, 2, 1, 3]\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 1.0989 | Train Acc: 82.45%                                                     \n  Epoch  2/5 | Loss: 0.7396 | Train Acc: 83.74%                                                     \n  Epoch  3/5 | Loss: 0.6574 | Train Acc: 84.14%                                                     \n  Epoch  4/5 | Loss: 0.6405 | Train Acc: 85.40%                                                     \n  Epoch  5/5 | Loss: 0.5972 | Train Acc: 82.83%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 1.0382 | Train Acc: 83.79%                                                     \n  Epoch  2/5 | Loss: 0.7177 | Train Acc: 87.08%                                                     \n  Epoch  3/5 | Loss: 0.6262 | Train Acc: 87.11%                                                     \n  Epoch  4/5 | Loss: 0.5999 | Train Acc: 87.56%                                                     \n  Epoch  5/5 | Loss: 0.5664 | Train Acc: 87.56%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 0.9975 | Train Acc: 85.55%                                                     \n  Epoch  2/5 | Loss: 0.6925 | Train Acc: 87.33%                                                     \n  Epoch  3/5 | Loss: 0.6241 | Train Acc: 87.56%                                                     \n  Epoch  4/5 | Loss: 0.5725 | Train Acc: 87.46%                                                     \n  Epoch  5/5 | Loss: 0.5356 | Train Acc: 88.80%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 1.0162 | Train Acc: 85.02%                                                     \n  Epoch  2/5 | Loss: 0.7019 | Train Acc: 86.36%                                                     \n  Epoch  3/5 | Loss: 0.6158 | Train Acc: 87.11%                                                     \n  Epoch  4/5 | Loss: 0.5914 | Train Acc: 87.29%                                                     \n  Epoch  5/5 | Loss: 0.5640 | Train Acc: 88.14%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 0.9524 | Train Acc: 85.27%                                                     \n  Epoch  2/5 | Loss: 0.6610 | Train Acc: 87.69%                                                     \n  Epoch  3/5 | Loss: 0.5708 | Train Acc: 88.82%                                                     \n  Epoch  4/5 | Loss: 0.5588 | Train Acc: 89.05%                                                     \n  Epoch  5/5 | Loss: 0.5224 | Train Acc: 89.37%                                                     \n Client local training complete (FedBN)\n\n Global Test â€” Acc: 32.69% | Prec: 0.698 | Recall: 0.342 | F1: 0.281 | RMSE: 2.980\n NEW BEST: 32.69%\n============================================================\nRound 2/50\n============================================================\nSelected clients: [3, 4, 1, 0, 2]\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 0.5407 | Train Acc: 88.66%                                                     \n  Epoch  2/5 | Loss: 0.5212 | Train Acc: 86.54%                                                     \n  Epoch  3/5 | Loss: 0.4984 | Train Acc: 88.71%                                                     \n  Epoch  4/5 | Loss: 0.4929 | Train Acc: 89.63%                                                     \n  Epoch  5/5 | Loss: 0.4718 | Train Acc: 89.35%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 0.6137 | Train Acc: 85.16%                                                     \n  Epoch  2/5 | Loss: 0.6113 | Train Acc: 85.02%                                                     \n  Epoch  3/5 | Loss: 0.5864 | Train Acc: 85.95%                                                     \n  Epoch  4/5 | Loss: 0.5871 | Train Acc: 86.52%                                                     \n  Epoch  5/5 | Loss: 0.5791 | Train Acc: 85.21%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 0.5912 | Train Acc: 87.25%                                                     \n  Epoch  2/5 | Loss: 0.5355 | Train Acc: 88.07%                                                     \n  Epoch  3/5 | Loss: 0.5372 | Train Acc: 87.83%                                                     \n  Epoch  4/5 | Loss: 0.5254 | Train Acc: 88.30%                                                     \n  Epoch  5/5 | Loss: 0.5256 | Train Acc: 88.93%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 0.5970 | Train Acc: 86.06%                                                     \n  Epoch  2/5 | Loss: 0.5761 | Train Acc: 87.27%                                                     \n  Epoch  3/5 | Loss: 0.5505 | Train Acc: 87.40%                                                     \n  Epoch  4/5 | Loss: 0.5375 | Train Acc: 88.10%                                                     \n  Epoch  5/5 | Loss: 0.5408 | Train Acc: 88.96%                                                     \n Client local training complete (FedBN)\n\n\n Training client model with FedBN objective...\n  Epoch  1/5 | Loss: 0.5632 | Train Acc: 87.45%                                                     \n  Epoch  2/5 | Loss: 0.5413 | Train Acc: 86.97%                                                     \n  Epoch  3/5 | Loss: 0.5178 | Train Acc: 88.49%                                                     \n  Epoch  4/5 | Loss: 0.5168 | Train Acc: 89.54%                                                     \n  Epoch  5/5 | Loss: 0.5037 | Train Acc: 89.27%                                                     \n Client local training complete (FedBN)\n\n Global Test â€” Acc: 36.03% | Prec: 0.628 | Recall: 0.357 | F1: 0.313 | RMSE: 3.024\n NEW BEST: 36.03%\n============================================================\nRound 3/50\n============================================================\nSelected clients: [4, 2, 0, 3, 1]\n\n Training client model with FedBN objective...\n  Epoch 1/5:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 184/299 [00:16<00:09, 11.64it/s, loss=0.5489]","output_type":"stream"}],"execution_count":null}]}