{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"How to Use This Version for Checkpoint Resume in Kaggle\n\nBefore running this notebook for the second time (to resume training from a checkpoint), follow the steps below carefully.\n\n1. Download the latest checkpoint file\nDownload the file named checkpoint_latest.pth from the previous version of your notebook or experiment.\n\n2. Upload the checkpoint to Kaggle Input Directory\nPlace the downloaded file inside your Kaggle input path, for example:\n/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\n\n3. Run the following code cell before starting training\nThis code will copy the checkpoint file to the working directory (/kaggle/working/checkpoints) so that training can resume from the saved state.\n\n4. Resume Training\nAfter the checkpoint file is copied successfully, running the rest of the notebook will automatically start training from the previous checkpoint instead of starting from scratch.\n\n\n\n## you can change the dataset and the attack type by simply changing the name in the args.py file.no need to modify anything else.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Source and destination paths\nsrc = \"/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\"\ndst_dir = \"/kaggle/working/checkpoints\"\ndst = os.path.join(dst_dir, \"checkpoint_latest.pth\")\n\n# Step 1: Check if source file exists\nif not os.path.exists(src):\n    print(f\"âŒ Source file not found: {src}\")\nelse:\n    print(f\"âœ… Found source file: {src}\")\n\n    # Step 2: Ensure destination directory exists\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(f\"ğŸ“‚ Created destination directory: {dst_dir}\")\n    else:\n        print(f\"ğŸ“ Destination directory already exists: {dst_dir}\")\n\n    # Step 3: Copy the file\n    shutil.copy(src, dst)\n    print(f\"âœ… Copied file to: {dst}\")\n\n    # Step 4: List all files in destination\n    files = os.listdir(dst_dir)\n    if files:\n        print(\"\\nğŸ“„ Files in /kaggle/working/checkpoints:\")\n        for f in files:\n            print(\" â”œâ”€â”€\", f)\n    else:\n        print(\"âš  Destination directory is empty (unexpected).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Target directory\nbase_dir = \"/kaggle/working/FedPer-PyTorch\"\nos.makedirs(base_dir, exist_ok=True)\n\n# Python files to create\nfiles = [\"main.py\", \"server.py\", \"client.py\", \"model.py\", \"get_data.py\", \"args.py\"]\n\n# Create each file if not exists\nfor file in files:\n    file_path = os.path.join(base_dir, file)\n    if not os.path.exists(file_path):\n        with open(file_path, \"w\") as f:\n            f.write(f\"# {file} â€” auto-created placeholder\\n\")\n        print(f\" Created: {file_path}\")\n    else:\n        print(f\" Already exists: {file_path}\")\n\nprint(\"\\n Folder and files ready in:\", base_dir)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-11T05:59:56.514318Z","iopub.execute_input":"2025-11-11T05:59:56.515117Z","iopub.status.idle":"2025-11-11T05:59:56.525363Z","shell.execute_reply.started":"2025-11-11T05:59:56.515089Z","shell.execute_reply":"2025-11-11T05:59:56.524238Z"},"trusted":true},"outputs":[{"name":"stdout","text":" Created: /kaggle/working/FedPer-PyTorch/main.py\n Created: /kaggle/working/FedPer-PyTorch/server.py\n Created: /kaggle/working/FedPer-PyTorch/client.py\n Created: /kaggle/working/FedPer-PyTorch/model.py\n Created: /kaggle/working/FedPer-PyTorch/get_data.py\n Created: /kaggle/working/FedPer-PyTorch/args.py\n\n Folder and files ready in: /kaggle/working/FedPer-PyTorch\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedPer-PyTorch/args.py\"\n\nnew_code = '''\n# ========================================\n# args.py â€” FedPer \n# ========================================\nimport argparse\nimport torch\n\n# Dataset configurations - Change dataset name here to switch\nDATASET_CONFIGS = {\n    'pathmnist': {\n        'num_classes': 9,\n        'class_names': [\n            \"adipose tissue\", \"background\", \"debris\", \"lymphocytes\",\n            \"mucus\", \"smooth muscle\", \"normal colon mucosa\",\n            \"cancer-associated stroma\", \"colorecal adenocarcinoma epithelium\"\n        ],\n        'input_channels': 3\n    },\n    'tissuemnist': {\n        'num_classes': 8,\n        'class_names': [\n            \"collecting duct\", \"distal convoluted tubule\",\n            \"glomerular endothelial cells\", \"interstitial endothelial cells\",\n            \"leukocytes\", \"podocytes\", \"proximal tubule\", \"thick ascending limb\"\n        ],\n        'input_channels': 1\n    },\n    'organamnist': {\n        'num_classes': 11,\n        'class_names': [\n            \"bladder\", \"femur-left\", \"femur-right\", \"heart\",\n            \"kidney-left\", \"kidney-right\", \"liver\",\n            \"lung-left\", \"lung-right\", \"spleen\", \"pelvis\"\n        ],\n        'input_channels': 1\n    },\n    'octmnist': {\n        'num_classes': 4,\n        'class_names': [\n            \"choroidal neovascularization\", \"diabetic macular edema\",\n            \"drusen\", \"normal\"\n        ],\n        'input_channels': 1\n    }\n}\n\ndef args_parser():\n    parser = argparse.ArgumentParser(description=\"FedPer - Config File\")\n    \n    # -------------------------------\n    # MAIN: Change this to switch dataset\n    # -------------------------------\n    parser.add_argument('--dataset', type=str, default='organamnist', \n                        choices=['pathmnist', 'tissuemnist', 'organamnist', 'octmnist'],\n                        help='MedMNIST dataset to use')\n    \n    # -------------------------------\n    # Federated Learning Parameters\n    # -------------------------------\n    parser.add_argument('--E', type=int, default=5, help='local epochs')\n    parser.add_argument('--r', type=int, default=50, help='number of communication rounds')\n    parser.add_argument('--K', type=int, default=5, help='total number of clients')\n    parser.add_argument('--C', type=float, default=1, help='client sampling rate per round')\n    parser.add_argument('--B', type=int, default=32, help='batch size')\n    parser.add_argument('--use_combined', action='store_true', \n                        help='Use train+val+test combined (all data)')\n\n    # -------------------------------\n    # Model parameters (auto-configured)\n    # -------------------------------\n    parser.add_argument('--clip_model', type=str, default='ViT-B/32', help='CLIP model variant')\n    parser.add_argument('--freeze_clip', action='store_true', help='Freeze CLIP backbone')\n    parser.add_argument('--dropout', type=float, default=0.5, help='dropout (stronger regularization)')\n    parser.add_argument('--Kp', type=int, default=2, help='number of personalized layers')\n\n    # -------------------------------\n    # Optimizer Settings\n    # -------------------------------\n    parser.add_argument('--lr', type=float, default=0.003, help='learning rate')\n    parser.add_argument('--optimizer', type=str, default='sgd', help='optimizer')\n    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9, help='momentum for SGD')\n\n    # -------------------------------\n    # Checkpoint Settings\n    # -------------------------------\n    parser.add_argument('--checkpoint_dir', type=str,\n                        default='./checkpoints_fedper',\n                        help='directory to save checkpoints')\n    # -------------------------------\n    # Adversarial Attack Settings\n    # -------------------------------\n    parser.add_argument('--enable_attack', action='store_true',\n                        help='Enable adversarial attacks during testing')\n    parser.add_argument('--attack_type', type=str, default='fgsm',\n                        choices=['fgsm', 'pgd', 'cw'],\n                        help='Type of adversarial attack')\n    parser.add_argument('--attack_epsilon', type=float, default=0.03,\n                        help='Perturbation budget for FGSM/PGD (L-inf norm)')\n    parser.add_argument('--pgd_alpha', type=float, default=0.01,\n                        help='Step size for PGD attack')\n    parser.add_argument('--pgd_steps', type=int, default=10,\n                        help='Number of PGD iterations')\n    parser.add_argument('--cw_c', type=float, default=1.0,\n                        help='C&W attack confidence parameter')\n    parser.add_argument('--cw_steps', type=int, default=100,\n                        help='Number of optimization steps for C&W')\n    parser.add_argument('--cw_lr', type=float, default=0.01,\n                        help='Learning rate for C&W optimization')\n    \n    # -------------------------------\n    # Device\n    # -------------------------------\n    parser.add_argument('--device', default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n                        help='cuda or cpu')\n    \n    # -------------------------------\n    # Balanced Non-IID Settings\n    # -------------------------------\n    parser.add_argument('--dominant_ratio', type=float, default=0.7,\n                        help='Fraction of dominant class per client (0-1)')\n\n    args = parser.parse_args(args=[])\n    \n    # Auto-configure based on selected dataset\n    if args.dataset in DATASET_CONFIGS:\n        config = DATASET_CONFIGS[args.dataset]\n        args.num_classes = config['num_classes']\n        args.input_channels = config['input_channels']\n        args.class_names = config['class_names']\n    else:\n        raise ValueError(f\"Dataset {args.dataset} not configured!\")\n    \n    return args\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" args.py updated!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T05:59:56.526787Z","iopub.execute_input":"2025-11-11T05:59:56.527040Z","iopub.status.idle":"2025-11-11T05:59:56.547129Z","shell.execute_reply.started":"2025-11-11T05:59:56.527022Z","shell.execute_reply":"2025-11-11T05:59:56.546458Z"},"trusted":true},"outputs":[{"name":"stdout","text":" args.py updated!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedPer-PyTorch/get_data.py\"\n\nnew_code = r'''\n# ========================================\n# get_data.py â€“ Fixed with Retry Logic for FedPer\n# Supports: pathmnist, tissuemnist, organamnist, octmnist\n# ========================================\nimport os\nimport numpy as np\nimport torch\nimport pickle\nimport time\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset\nfrom torchvision import transforms\nfrom medmnist import INFO\nfrom medmnist.dataset import PathMNIST, TissueMNIST, OrganAMNIST, OCTMNIST\n\n# Map dataset names to classes\nDATASET_MAP = {\n    'pathmnist': PathMNIST,\n    'tissuemnist': TissueMNIST,\n    'organamnist': OrganAMNIST,\n    'octmnist': OCTMNIST\n}\n\ndef get_transforms():\n    \"\"\"Returns train and test transforms (handles grayscale and RGB)\"\"\"\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.Grayscale(num_output_channels=3),  # ensures 3 channels for CLIP\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n    \n    return train_transform, test_transform\n\ndef balanced_noniid_split(dataset, num_clients, dominant_ratio=0.7):\n    \"\"\"Balanced Non-IID split: dominant_ratio% to main client, rest distributed\"\"\"\n    labels = np.array([dataset[i][1].item() for i in range(len(dataset))])\n    num_classes = len(np.unique(labels))\n    client_indices = [[] for _ in range(num_clients)]\n\n    # Prepare class indices\n    class_indices = {c: np.where(labels == c)[0].tolist() for c in range(num_classes)}\n    for c in class_indices:\n        np.random.shuffle(class_indices[c])\n\n    # Step 1: assign dominant_ratio% of dominant class to primary client\n    for client_id in range(num_clients):\n        dominant_class = client_id % num_classes\n        n_dominant = int(len(class_indices[dominant_class]) * dominant_ratio)\n        if n_dominant > 0:\n            client_indices[client_id].extend(class_indices[dominant_class][:n_dominant])\n            class_indices[dominant_class] = class_indices[dominant_class][n_dominant:]\n\n    # Step 2: distribute remaining samples equally among other clients\n    for c in range(num_classes):\n        remaining = class_indices[c]\n        np.random.shuffle(remaining)\n        other_clients = [i for i in range(num_clients) if i % num_classes != c]\n        for i, idx in enumerate(remaining):\n            client_id = other_clients[i % len(other_clients)]\n            client_indices[client_id].append(idx)\n\n    # Shuffle each client's indices\n    for i in range(num_clients):\n        np.random.shuffle(client_indices[i])\n\n    return client_indices\n\ndef download_with_retry(DatasetClass, root, split, transform, max_retries=3, delay=5):\n    \"\"\"Download dataset with retry logic\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"  Attempt {attempt + 1}/{max_retries}: Downloading {split} split...\")\n            dataset = DatasetClass(root=root, split=split, download=True, transform=transform)\n            print(f\"   Successfully loaded {split} split\")\n            return dataset\n        except Exception as e:\n            if attempt < max_retries - 1:\n                print(f\"   Download failed: {str(e)}\")\n                print(f\"  Waiting {delay} seconds before retry...\")\n                time.sleep(delay)\n            else:\n                print(f\"   Failed after {max_retries} attempts\")\n                raise RuntimeError(f\"\"\"\nDataset download failed after {max_retries} attempts.\n\nPossible solutions:\n1. Wait a few minutes and try again (server may be overloaded)\n2. Manually download the dataset:\n   - Go to: https://zenodo.org/records/10519652\n   - Download the .npz file for your dataset\n   - Place it in: {root}/\n3. Change dataset in args.py to one that's already downloaded\n4. Check your internet connection\n\nError: {str(e)}\n\"\"\")\n\ndef load_medmnist_data(args):\n    \"\"\"Load MedMNIST data for FedPer - automatically configured based on args.dataset\"\"\"\n    train_transform, test_transform = get_transforms()\n    data_root = './data/medmnist'\n    os.makedirs(data_root, exist_ok=True)\n\n    data_flag = args.dataset.lower()\n    if data_flag not in DATASET_MAP:\n        raise ValueError(f\"Dataset {data_flag} not supported. Choose from {list(DATASET_MAP.keys())}\")\n\n    n_classes = args.num_classes\n\n    print(\"\\n\" + \"=\"*60)\n    print(f\" Loading {data_flag.upper()} Dataset for FedPer\")\n    print(\"=\"*60)\n    print(f\"Number of classes: {n_classes}\")\n\n    DatasetClass = DATASET_MAP[data_flag]\n\n    # Check if dataset already exists\n    dataset_file = os.path.join(data_root, f'{data_flag}.npz')\n    if os.path.exists(dataset_file):\n        print(f\" Found existing dataset: {dataset_file}\")\n        print(\"  Loading from cache...\")\n        \n    # Load datasets with retry logic\n    print(\"\\n Loading dataset splits...\")\n    train_dataset = download_with_retry(DatasetClass, data_root, 'train', train_transform)\n    val_dataset = download_with_retry(DatasetClass, data_root, 'val', train_transform)\n    test_dataset = download_with_retry(DatasetClass, data_root, 'test', test_transform)\n\n    combined_train = ConcatDataset([train_dataset, val_dataset])\n    print(f\" Using Train+Val: {len(combined_train)} samples for federated learning\")\n\n    # Load or create client indices\n    cache_file = f'./data/medmnist/client_indices_{data_flag}_K{args.K}_dr{args.dominant_ratio}_fedper.pkl'\n    if os.path.exists(cache_file):\n        print(f\"\\n Loading cached split from: {cache_file}\")\n        with open(cache_file, 'rb') as f:\n            client_indices = pickle.load(f)\n    else:\n        print(f\"\\n Creating balanced Non-IID split (dominant_ratio={args.dominant_ratio})...\")\n        client_indices = balanced_noniid_split(combined_train, args.K, dominant_ratio=args.dominant_ratio)\n        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n        with open(cache_file, 'wb') as f:\n            pickle.dump(client_indices, f)\n        print(f\" Split cached to: {cache_file}\")\n\n    # Print client-wise class distribution\n    print(\"\\n\" + \"=\"*60)\n    print(\" Client-wise Image Distribution\")\n    print(\"=\"*60)\n    total_samples = 0\n    for client_id, indices in enumerate(client_indices):\n        client_labels = [combined_train[i][1].item() for i in indices]\n        label_counts = np.bincount(client_labels, minlength=n_classes)\n        total_client = len(indices)\n        total_samples += total_client\n        distribution_str = \", \".join([f\"C{c}:{label_counts[c]}\" for c in range(n_classes)])\n        dominant_class = np.argmax(label_counts)\n        dominant_pct = (label_counts[dominant_class] / total_client) * 100\n        print(f\"Client {client_id:2d}: {total_client:5d} samples | Dominant: Class {dominant_class} ({dominant_pct:.1f}%)\")\n        print(f\"           [{distribution_str}]\")\n\n    print(f\"\\nTotal samples: {total_samples}\")\n    print(\"=\"*60 + \"\\n\")\n\n    # Create DataLoaders\n    client_loaders = []\n    for i, indices in enumerate(client_indices):\n        subset = Subset(combined_train, indices)\n        loader = DataLoader(subset, batch_size=args.B, shuffle=True, num_workers=0, pin_memory=True)\n        client_loaders.append(loader)\n\n    test_loader = DataLoader(test_dataset, batch_size=args.B, shuffle=False, num_workers=0, pin_memory=True)\n    return client_loaders, test_loader\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" get_data.py updated!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T05:59:56.664785Z","iopub.execute_input":"2025-11-11T05:59:56.665478Z","iopub.status.idle":"2025-11-11T05:59:56.676128Z","shell.execute_reply.started":"2025-11-11T05:59:56.665445Z","shell.execute_reply":"2025-11-11T05:59:56.675446Z"},"trusted":true},"outputs":[{"name":"stdout","text":" get_data.py updated!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ========================================\n# STEP 1: Fix model.py\n# ========================================\nfile_path = \"/kaggle/working/FedPer-PyTorch/model.py\"\n\nfixed_model_code = r'''\n# ========================================\n# model.py â€” CLIP-based FedPer Classifier (FIXED)\n# ========================================\nimport torch\nfrom torch import nn\nimport clip\n\nclass CLIPFedPerClassifier(nn.Module):\n    \"\"\"\n    FedPer with CLIP: Proper Implementation\n    ----------------------------------------\n    Shared Layers: Last 2 transformer blocks of CLIP (AGGREGATED)\n    Personalized Layers: MLP head (NOT aggregated)\n    \"\"\"\n    def __init__(self, args, name='clip_fedper_model'):\n        super(CLIPFedPerClassifier, self).__init__()\n        self.name = name\n        self.num_classes = args.num_classes\n        self.Kp = getattr(args, 'Kp', 2)\n        self.dropout = getattr(args, 'dropout', 0.3)\n        \n        # Load pretrained CLIP model\n        self.clip_model, self.preprocess = clip.load(args.clip_model, device=args.device)\n        \n        # Get CLIP feature dimension\n        self.feature_dim = self.clip_model.visual.output_dim\n        \n        # Auto-configured class names\n        self.class_names = args.class_names\n        \n        # FedPer Strategy: Unfreeze last transformer blocks\n        self._setup_fedper_layers()\n        \n        # Personalized MLP Head (NOT aggregated)\n        self.head = nn.Sequential(\n            nn.Linear(self.feature_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(self.dropout),\n            \n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(self.dropout * 0.7),\n            \n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(self.dropout * 0.5),\n            \n            nn.Linear(256, self.num_classes)\n        )\n        \n        self._initialize_weights()\n    \n    def _setup_fedper_layers(self):\n        \"\"\"Freeze most of CLIP, unfreeze last 2 transformer blocks\"\"\"\n        # Freeze all first\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n        \n        # Unfreeze last 2 transformer blocks (shared, will be aggregated)\n        if hasattr(self.clip_model.visual, 'transformer'):\n            num_blocks = len(self.clip_model.visual.transformer.resblocks)\n            num_shared = 2  # Last 2 blocks for aggregation\n            \n            for block in self.clip_model.visual.transformer.resblocks[-num_shared:]:\n                for param in block.parameters():\n                    param.requires_grad = True\n            \n            print(f\"  âœ“ Unfroze last {num_shared}/{num_blocks} transformer blocks (shared layers)\")\n        \n        # Also unfreeze final projection layer\n        if hasattr(self.clip_model.visual, 'proj') and self.clip_model.visual.proj is not None:\n            self.clip_model.visual.proj.requires_grad = True\n            print(f\"  âœ“ Unfroze CLIP projection layer (shared)\")\n    \n    def _initialize_weights(self):\n        for m in self.head.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, images):\n        # CLIP features (last 2 blocks are trainable)\n        image_features = self.clip_model.encode_image(images)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        image_features = image_features.float()\n        \n        # Personalized head\n        logits = self.head(image_features)\n        return logits\n    \n    # ========================================\n    # FedPer Required Methods\n    # ========================================\n    def get_shared_params(self):\n        \"\"\"Returns trainable CLIP parameters for aggregation\"\"\"\n        shared_params = []\n        for param in self.clip_model.parameters():\n            if param.requires_grad:\n                shared_params.append(param)\n        return shared_params\n    \n    def get_personalized_params(self):\n        \"\"\"Returns personalized head parameters (NOT aggregated)\"\"\"\n        return [p for p in self.head.parameters() if p.requires_grad]\n    \n    def get_trainable_params(self):\n        \"\"\"Return all trainable parameters\"\"\"\n        return [p for p in self.parameters() if p.requires_grad]\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(fixed_model_code)\n\nprint(\"âœ… model.py FIXED!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T05:59:56.677678Z","iopub.execute_input":"2025-11-11T05:59:56.677967Z","iopub.status.idle":"2025-11-11T05:59:56.701819Z","shell.execute_reply.started":"2025-11-11T05:59:56.677937Z","shell.execute_reply":"2025-11-11T05:59:56.701013Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… model.py FIXED!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedPer-PyTorch/client.py\"\n\nnew_code = r'''\n# ========================================\n# client.py â€” Client\n# ========================================\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport copy\n\nclass Client:\n    def __init__(self, model, train_loader, device, val_loader=None, lr=0.0001, weight_decay=5e-4, Kp=2, shared_params=None):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.Kp = Kp  # number of personalized layers (head)\n        self.shared_params = [p.clone().detach() for p in shared_params] if shared_params else None\n\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=self.weight_decay)\n\n    def compute_accuracy(self, loader):\n        self.model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for images, labels in loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                labels = labels.squeeze()\n                outputs = self.model(images)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        return 100 * correct / total if total > 0 else 0\n\n    def train(self, epochs=1, shared_params=None):\n        \"\"\"Train client model; keep head personalized, optionally update shared layers\"\"\"\n        print(f\"\\\\n Training {self.model.name} with FedPer (personalized head)...\")\n\n        # Initialize shared layers if provided\n        if shared_params is not None:\n            model_shared = self.model.get_shared_params()\n            for m_p, g_p in zip(model_shared, shared_params):\n                m_p.data = g_p.data.clone()\n\n        for epoch in range(epochs):\n            self.model.train()\n            total_loss = 0.0\n            pbar = tqdm(self.train_loader, desc=f\"  Epoch {epoch+1}/{epochs}\", ncols=100, leave=False)\n            \n            for images, labels in pbar:\n                images, labels = images.to(self.device), labels.to(self.device)\n                labels = labels.squeeze()\n\n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n                total_loss += loss.item()\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n            avg_loss = total_loss / len(self.train_loader)\n            train_acc = self.compute_accuracy(self.train_loader)\n            log_msg = f\"  Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\"\n\n            if self.val_loader is not None:\n                val_acc = self.compute_accuracy(self.val_loader)\n                val_loss = 0\n                self.model.eval()\n                with torch.no_grad():\n                    for images, labels in self.val_loader:\n                        images, labels = images.to(self.device), labels.to(self.device)\n                        labels = labels.squeeze()\n                        outputs = self.model(images)\n                        loss = self.criterion(outputs, labels)\n                        val_loss += loss.item()\n                val_loss /= len(self.val_loader)\n                log_msg += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n\n            print(log_msg)\n\n        print(f\" {self.model.name} local training complete (FedPer)\\\\n\")\n        return self.model.state_dict()\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" client.py updated!\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T05:59:56.702796Z","iopub.execute_input":"2025-11-11T05:59:56.703156Z","iopub.status.idle":"2025-11-11T05:59:56.730826Z","shell.execute_reply.started":"2025-11-11T05:59:56.703129Z","shell.execute_reply":"2025-11-11T05:59:56.730041Z"},"trusted":true},"outputs":[{"name":"stdout","text":" client.py updated!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ========================================\n# STEP 2: Fix server.py - Global model\n# ========================================\nfile_path = \"/kaggle/working/FedPer-PyTorch/server.py\"\n\nfixed_server_code = r'''\n# ========================================\n# server.py â€” FedPer Server (FIXED)\n# ========================================\nimport copy\nimport random\nimport numpy as np\nimport torch\nimport os\nimport json\nfrom model import CLIPFedPerClassifier as ImageClassifier\nfrom get_data import load_medmnist_data\nfrom client import Client\nfrom sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n\nclass FedPerServer:\n    def __init__(self, args, resume_from=None):\n        self.args = args\n        os.makedirs(args.checkpoint_dir, exist_ok=True)\n        self.current_round = 0\n        self.best_global_acc = 0\n        self.history = {\n            'rounds': [], \n            'avg_accuracy': [], \n            'best_accuracy': [],\n            'precision': [],\n            'recall': [],\n            'f1': [],\n            'rmse': []\n        }\n\n        self.global_model = ImageClassifier(args, name=\"server\").to(args.device)\n        self.client_models = []\n        for i in range(self.args.K):\n            model = copy.deepcopy(self.global_model)\n            model.name = f\"Client_{i}\"\n            self.client_models.append(model)\n\n        self.client_loaders, self.test_loader = load_medmnist_data(args)\n        if resume_from:\n            self.load_checkpoint(resume_from)\n\n    def load_checkpoint(self, checkpoint_path):\n        \"\"\"Load checkpoint\"\"\"\n        print(\"\\n Loading checkpoint from:\", checkpoint_path)\n        \n        checkpoint = torch.load(\n            checkpoint_path, \n            map_location=self.args.device,\n            weights_only=False  \n        )\n        \n        self.global_model.load_state_dict(checkpoint['server_state_dict'])\n        \n        if 'client_state_dicts' in checkpoint:\n            for i, state_dict in enumerate(checkpoint['client_state_dicts']):\n                self.client_models[i].load_state_dict(state_dict)\n        \n        self.current_round = checkpoint.get('round', 0)\n        self.best_global_acc = checkpoint.get('best_global_acc', 0)\n        self.history = checkpoint.get('history', {\n            'rounds': [], \n            'avg_accuracy': [], \n            'best_accuracy': [],\n            'precision': [],\n            'recall': [],\n            'f1': [],\n            'rmse': []\n        })\n        \n        print(\" Checkpoint loaded successfully!\")\n        print(f\"   Resuming from Round: {self.current_round}/{self.args.r}\")\n        print(f\"   Best Accuracy: {self.best_global_acc:.2f}%\")\n\n    def dispatch(self, selected_clients):\n        \"\"\"Send global shared params to clients\"\"\"\n        global_shared_params = self.global_model.get_shared_params()\n        \n        if len(global_shared_params) == 0:\n            print(\"  WARNING: No shared parameters to dispatch!\")\n            return\n            \n        for idx in selected_clients:\n            client_model = self.client_models[idx]\n            client_params = client_model.get_shared_params()\n            for c_param, g_param in zip(client_params, global_shared_params):\n                c_param.data = g_param.data.clone()\n\n    def aggregate(self, selected_clients):\n        \"\"\"Aggregate shared layers only\"\"\"\n        global_shared_params = self.global_model.get_shared_params()\n        \n        if len(global_shared_params) == 0:\n            print(\"  WARNING: No shared parameters to aggregate!\")\n            return\n            \n        total_samples = sum([len(self.client_loaders[idx].dataset) for idx in selected_clients])\n        agg_params = [torch.zeros_like(p.data) for p in global_shared_params]\n\n        for idx in selected_clients:\n            client_model = self.client_models[idx]\n            client_shared = client_model.get_shared_params()\n            weight = len(self.client_loaders[idx].dataset) / total_samples\n            for i, p in enumerate(client_shared):\n                agg_params[i] += p.data * weight\n\n        for p, agg_p in zip(global_shared_params, agg_params):\n            p.data = agg_p.data.clone()\n        \n        print(f\"  âœ“ Aggregated {len(global_shared_params)} shared parameter tensors\")\n\n    def client_update(self, idx):\n        client_model = self.client_models[idx]\n        client_loader = self.client_loaders[idx]\n        client_obj = Client(\n            model=client_model,\n            train_loader=client_loader,\n            device=self.args.device,\n            lr=self.args.lr,\n            weight_decay=self.args.weight_decay,\n            Kp=self.args.Kp\n        )\n        global_shared_params = self.global_model.get_shared_params()\n        client_obj.train(epochs=self.args.E, shared_params=global_shared_params)\n\n    def test_all_clients(self):\n        \"\"\"Test each client on global test set and return average\"\"\"\n        client_accuracies = []\n        all_labels_combined = []\n        all_preds_combined = []\n        \n        for idx, client_model in enumerate(self.client_models):\n            client_model.eval()\n            client_labels = []\n            client_preds = []\n            \n            with torch.no_grad():\n                for images, labels in self.test_loader:\n                    images, labels = images.to(self.args.device), labels.to(self.args.device)\n                    labels = labels.squeeze()\n                    \n                    outputs = client_model(images)\n                    _, predicted = torch.max(outputs, 1)\n                    client_labels.extend(labels.cpu().numpy())\n                    client_preds.extend(predicted.cpu().numpy())\n            \n            client_labels = np.array(client_labels)\n            client_preds = np.array(client_preds)\n            client_acc = 100 * np.mean(client_labels == client_preds)\n            client_accuracies.append(client_acc)\n            \n            all_labels_combined.extend(client_labels)\n            all_preds_combined.extend(client_preds)\n        \n        # Average accuracy across all clients\n        avg_acc = np.mean(client_accuracies)\n        \n        # Compute metrics on combined predictions\n        all_labels_combined = np.array(all_labels_combined)\n        all_preds_combined = np.array(all_preds_combined)\n        \n        precision = precision_score(all_labels_combined, all_preds_combined, average='macro', zero_division=0)\n        recall = recall_score(all_labels_combined, all_preds_combined, average='macro', zero_division=0)\n        f1 = f1_score(all_labels_combined, all_preds_combined, average='macro', zero_division=0)\n        rmse = np.sqrt(mean_squared_error(all_labels_combined, all_preds_combined))\n        \n        print(f\" Client Avg Test â€” Acc: {avg_acc:.2f}% | Prec: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | RMSE: {rmse:.3f}\")\n        print(f\"   Individual: {[f'{acc:.1f}%' for acc in client_accuracies]}\")\n        \n        return {\n            'accuracy': avg_acc,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n            'rmse': rmse\n        }\n\n    def save_checkpoint(self, round_num, metrics):\n        \"\"\"Save checkpoint\"\"\"\n        checkpoint = {\n            'round': round_num,\n            'server_state_dict': self.global_model.state_dict(),\n            'client_state_dicts': [model.state_dict() for model in self.client_models],\n            'best_global_acc': self.best_global_acc,\n            'history': self.history,\n            'args': vars(self.args)\n        }\n        \n        \n        \n        latest_path = os.path.join(self.args.checkpoint_dir, 'checkpoint_latest.pth')\n        torch.save(checkpoint, latest_path)\n        \n    \n        history_path = os.path.join(self.args.checkpoint_dir, 'training_history.json')\n        with open(history_path, 'w') as f:\n            json.dump(self.history, f, indent=4)\n\n    def run(self):\n        \"\"\"Main training loop\"\"\"\n        start_round = self.current_round\n        \n        separator_line = \"=\" * 80\n        dash_line = \"-\" * 80\n        \n        print(f\"\\n{separator_line}\")\n        print(\" STARTING FEDERATED TRAINING (FedPer)\")\n        print(separator_line)\n        print(f\"Starting from Round: {start_round + 1}/{self.args.r}\")\n        print(f\"Current Best Accuracy: {self.best_global_acc:.2f}%\")\n        \n        # Check shared params\n        shared_params = self.global_model.get_shared_params()\n        print(f\"Shared parameters: {len(shared_params)} tensors\")\n        print(f\"{separator_line}\\n\")\n        \n        for r in range(start_round, self.args.r):\n            print(f\"\\n{dash_line}\")\n            print(f\"Round {r+1}/{self.args.r}\")\n            print(dash_line)\n            \n            m = max(int(self.args.C * self.args.K), 1)\n            selected_clients = random.sample(range(self.args.K), m)\n            print(f\"Selected {m} clients: {selected_clients}\")\n            \n            self.dispatch(selected_clients)\n            \n            for idx in selected_clients:\n                print(f\"  Training Client {idx}...\", end=\" \")\n                self.client_update(idx)\n                print(\"âœ“\")\n            \n            self.aggregate(selected_clients)\n            \n            # Test using client models (FedPer approach)\n            metrics = self.test_all_clients()\n            avg_acc = metrics['accuracy']\n            \n            self.history['rounds'].append(r+1)\n            self.history['avg_accuracy'].append(avg_acc)\n            self.history['best_accuracy'].append(max(self.best_global_acc, avg_acc))\n            self.history['precision'].append(metrics['precision'])\n            self.history['recall'].append(metrics['recall'])\n            self.history['f1'].append(metrics['f1'])\n            self.history['rmse'].append(metrics['rmse'])\n            \n            if avg_acc > self.best_global_acc:\n                self.best_global_acc = avg_acc\n            \n            self.current_round = r + 1\n            \n            self.save_checkpoint(r+1, metrics)\n            \n            print(f\"\\n Round {r+1} Results:\")\n            print(f\"   Accuracy: {avg_acc:.2f}%\")\n            print(f\"   Best Accuracy: {self.best_global_acc:.2f}%\")\n            print(f\"   Precision: {metrics['precision']:.3f}\")\n            print(f\"   Recall: {metrics['recall']:.3f}\")\n            print(f\"   F1-Score: {metrics['f1']:.3f}\")\n            print(f\"   RMSE: {metrics['rmse']:.3f}\")\n        \n        print(f\"\\n{separator_line}\")\n        print(\" TRAINING COMPLETE\")\n        print(separator_line)\n        print(f\"Best Accuracy Achieved: {self.best_global_acc:.2f}%\")\n        print(f\"{separator_line}\\n\")\n        \n        return self.global_model\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(fixed_server_code)\n\nprint(\"âœ… server.py FIXED!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T05:59:57.057268Z","iopub.execute_input":"2025-11-11T05:59:57.057748Z","iopub.status.idle":"2025-11-11T05:59:57.067480Z","shell.execute_reply.started":"2025-11-11T05:59:57.057728Z","shell.execute_reply":"2025-11-11T05:59:57.066860Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… server.py FIXED!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedPer-PyTorch/main.py\"\n\nnew_code = r\"\"\"\n# ========================================\n# main.py â€” Run FedPer with Resume Support (FedProx style auto-resume)\n# ========================================\nfrom args import args_parser\nfrom server import FedPerServer\nfrom get_data import load_medmnist_data\nimport torch\nimport os\nimport json\nimport matplotlib.pyplot as plt\n\ndef plot_training_history(history, save_path):\n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Average Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history['rounds'], history['avg_accuracy'], 'b-', label='Avg Accuracy', linewidth=2)\n    plt.plot(history['rounds'], history['best_accuracy'], 'r--', label='Best Accuracy', linewidth=2)\n    plt.xlabel('Communication Round')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Federated Learning Accuracy Progress')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Accuracy Improvement\n    plt.subplot(1, 2, 2)\n    if len(history['avg_accuracy']) > 1:\n        improvements = [history['avg_accuracy'][i] - history['avg_accuracy'][i-1] \n                        for i in range(1, len(history['avg_accuracy']))]\n        plt.bar(history['rounds'][1:], improvements, alpha=0.7)\n        plt.axhline(y=0, color='r', linestyle='-', linewidth=0.5)\n        plt.xlabel('Communication Round')\n        plt.ylabel('Accuracy Change (%)')\n        plt.title('Round-to-Round Accuracy Change')\n        plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f' Training plot saved at: {save_path}')\n\ndef main():\n    # Load arguments from args.py\n    args = args_parser()\n    \n    # Print configuration\n    print('\\\\n' + '='*60)\n    print(' FEDERATED LEARNING CONFIGURATION (FedPer)')\n    print('='*60)\n    print(f'Dataset: {args.dataset.upper()}')\n    print(f'Clients: {args.K} | Rounds: {args.r} | Local Epochs: {args.E}')\n    print(f'Batch Size: {args.B} | Learning Rate: {args.lr}')\n    print(f'CLIP Model: {args.clip_model} | Device: {args.device}')\n    print(f'Non-IID: {args.dominant_ratio*100:.0f}% dominant class per client')\n    print(f'Personalized Layers (Kp): {args.Kp}')\n    print('='*60)\n    \n    # Load client loaders & test loader\n    client_loaders, test_loader = load_medmnist_data(args)\n    \n    # Check for existing checkpoint (FedProx style auto-resume)\n    latest_checkpoint = os.path.join(args.checkpoint_dir, 'checkpoint_latest.pth')\n    resume_from = None\n    if os.path.exists(latest_checkpoint):\n        try:\n            checkpoint = torch.load(latest_checkpoint, map_location=args.device, weights_only=False)\n            completed_rounds = checkpoint.get('round', 0)\n            best_acc = checkpoint.get('best_global_acc', 0)\n            print('\\\\n' + '='*60)\n            print(' CHECKPOINT DETECTED â€” auto-resuming!')\n            print('='*60)\n            print(f' Completed Rounds: {completed_rounds}/{args.r}')\n            print(f' Best Accuracy: {best_acc:.2f}%')\n            resume_from = latest_checkpoint\n        except Exception as e:\n            print(f' Error loading checkpoint: {e}')\n            print(' Starting fresh training...')\n    else:\n        print('\\\\n' + '='*60)\n        print(' No checkpoint found. Starting fresh training...')\n        print('='*60)\n    \n    # Initialize FedPer server\n    server = FedPerServer(args, resume_from=resume_from)\n    \n    # Run federated training\n    final_model = server.run()\n    \n    # Save final global model (shared layers)\n    final_model_path = os.path.join(args.checkpoint_dir, 'final_global_model.pth')\n    torch.save(final_model.state_dict(), final_model_path)\n    print(f'\\\\n Final global model (shared layers) saved at: {final_model_path}')\n    \n    # Save best model if exists\n    best_model_path = os.path.join(args.checkpoint_dir, 'best_model.pth')\n    if os.path.exists(best_model_path):\n        print(f' Best global model saved at: {best_model_path}')\n    \n    # Save training history\n    history_path = os.path.join(args.checkpoint_dir, 'training_history.json')\n    with open(history_path, 'w') as f:\n        json.dump(server.history, f, indent=4)\n    print(f' Training history saved at: {history_path}')\n    \n    # Plot training curves\n    plot_path = os.path.join(args.checkpoint_dir, 'training_plot.png')\n    plot_training_history(server.history, plot_path)\n    \n    # Print summary\n    print('\\\\n' + '='*60)\n    print(' TRAINING SUMMARY (FedPer)')\n    print('='*60)\n    print(f' Best Global Accuracy: {server.best_global_acc:.2f}%')\n    print(f' Final Round Accuracy: {server.history[\"avg_accuracy\"][-1]:.2f}%')\n    print(f' Total Improvement: {server.history[\"avg_accuracy\"][-1] - server.history[\"avg_accuracy\"][0]:.2f}%')\n    print(f' All checkpoints saved in: {args.checkpoint_dir}')\n    print('='*60)\n\nif __name__ == '__main__':\n    main()\n\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\"âœ… main.py updated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T05:59:57.068709Z","iopub.execute_input":"2025-11-11T05:59:57.068907Z","iopub.status.idle":"2025-11-11T05:59:57.089243Z","shell.execute_reply.started":"2025-11-11T05:59:57.068892Z","shell.execute_reply":"2025-11-11T05:59:57.088512Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… main.py updated!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install medmnist --quiet","metadata":{"execution":{"iopub.status.busy":"2025-11-11T05:59:57.090106Z","iopub.execute_input":"2025-11-11T05:59:57.090367Z","iopub.status.idle":"2025-11-11T06:01:23.964699Z","shell.execute_reply.started":"2025-11-11T05:59:57.090344Z","shell.execute_reply":"2025-11-11T06:01:23.963668Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:01:23.965890Z","iopub.execute_input":"2025-11-11T06:01:23.966160Z","iopub.status.idle":"2025-11-11T06:01:35.449611Z","shell.execute_reply.started":"2025-11-11T06:01:23.966135Z","shell.execute_reply":"2025-11-11T06:01:35.448662Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-iky5rbah\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-iky5rbah\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=66c302bbfcfb97f3f5d553f6550aefb4196c0d65037fc2e7b569c66b5a4fa630\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_cilrcp6/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python -u /kaggle/working/FedPer-PyTorch/main.py","metadata":{"execution":{"iopub.status.busy":"2025-11-11T06:01:35.451486Z","iopub.execute_input":"2025-11-11T06:01:35.451718Z","execution_failed":"2025-11-11T06:43:00.479Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\\n============================================================\n FEDERATED LEARNING CONFIGURATION (FedPer)\n============================================================\nDataset: ORGANAMNIST\nClients: 5 | Rounds: 50 | Local Epochs: 5\nBatch Size: 32 | Learning Rate: 0.003\nCLIP Model: ViT-B/32 | Device: cuda\nNon-IID: 70% dominant class per client\nPersonalized Layers (Kp): 2\n============================================================\n\n============================================================\n Loading ORGANAMNIST Dataset for FedPer\n============================================================\nNumber of classes: 11\n\n Loading dataset splits...\n  Attempt 1/3: Downloading train split...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.2M/38.2M [00:17<00:00, 2.13MB/s]\n   Successfully loaded train split\n  Attempt 1/3: Downloading val split...\n   Successfully loaded val split\n  Attempt 1/3: Downloading test split...\n   Successfully loaded test split\n Using Train+Val: 41052 samples for federated learning\n\n Creating balanced Non-IID split (dominant_ratio=0.7)...\n Split cached to: ./data/medmnist/client_indices_organamnist_K5_dr0.7_fedper.pkl\n\n============================================================\n Client-wise Image Distribution\n============================================================\nClient  0:  8151 samples | Dominant: Class 0 (19.5%)\n           [C0:1593, C1:122, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\nClient  1:  7743 samples | Dominant: Class 6 (18.6%)\n           [C0:171, C1:1136, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\nClient  2:  7714 samples | Dominant: Class 6 (18.7%)\n           [C0:171, C1:122, C2:1107, C3:140, C4:340, C5:891, C6:1439, C7:990, C8:988, C9:712, C10:814]\nClient  3:  7891 samples | Dominant: Class 6 (18.2%)\n           [C0:171, C1:122, C2:119, C3:1306, C4:340, C5:891, C6:1439, C7:990, C8:987, C9:712, C10:814]\nClient  4:  9553 samples | Dominant: Class 4 (33.2%)\n           [C0:171, C1:121, C2:118, C3:140, C4:3171, C5:890, C6:1439, C7:990, C8:987, C9:712, C10:814]\n\nTotal samples: 41052\n============================================================\n\n\\n============================================================\n No checkpoint found. Starting fresh training...\n============================================================\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:01<00:00, 183MiB/s]\n  âœ“ Unfroze last 2/12 transformer blocks (shared layers)\n  âœ“ Unfroze CLIP projection layer (shared)\n\n============================================================\n Loading ORGANAMNIST Dataset for FedPer\n============================================================\nNumber of classes: 11\n Found existing dataset: ./data/medmnist/organamnist.npz\n  Loading from cache...\n\n Loading dataset splits...\n  Attempt 1/3: Downloading train split...\n   Successfully loaded train split\n  Attempt 1/3: Downloading val split...\n   Successfully loaded val split\n  Attempt 1/3: Downloading test split...\n   Successfully loaded test split\n Using Train+Val: 41052 samples for federated learning\n\n Loading cached split from: ./data/medmnist/client_indices_organamnist_K5_dr0.7_fedper.pkl\n\n============================================================\n Client-wise Image Distribution\n============================================================\nClient  0:  8151 samples | Dominant: Class 0 (19.5%)\n           [C0:1593, C1:122, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\nClient  1:  7743 samples | Dominant: Class 6 (18.6%)\n           [C0:171, C1:1136, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\nClient  2:  7714 samples | Dominant: Class 6 (18.7%)\n           [C0:171, C1:122, C2:1107, C3:140, C4:340, C5:891, C6:1439, C7:990, C8:988, C9:712, C10:814]\nClient  3:  7891 samples | Dominant: Class 6 (18.2%)\n           [C0:171, C1:122, C2:119, C3:1306, C4:340, C5:891, C6:1439, C7:990, C8:987, C9:712, C10:814]\nClient  4:  9553 samples | Dominant: Class 4 (33.2%)\n           [C0:171, C1:121, C2:118, C3:140, C4:3171, C5:890, C6:1439, C7:990, C8:987, C9:712, C10:814]\n\nTotal samples: 41052\n============================================================\n\n\n================================================================================\n STARTING FEDERATED TRAINING (FedPer)\n================================================================================\nStarting from Round: 1/50\nCurrent Best Accuracy: 0.00%\nShared parameters: 25 tensors\n================================================================================\n\n\n--------------------------------------------------------------------------------\nRound 1/50\n--------------------------------------------------------------------------------\nSelected 5 clients: [4, 2, 0, 3, 1]\n  Training Client 4... \\n Training Client_4 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.8664 | Train Acc: 75.15%                                                     \n  Epoch  2/5 | Loss: 0.4631 | Train Acc: 75.70%                                                     \n  Epoch  3/5 | Loss: 0.3727 | Train Acc: 87.97%                                                     \n  Epoch  4/5 | Loss: 0.3150 | Train Acc: 88.42%                                                     \n  Epoch  5/5 | Loss: 0.2670 | Train Acc: 85.96%                                                     \n Client_4 local training complete (FedPer)\\n\nâœ“\n  Training Client 2... \\n Training Client_2 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.8917 | Train Acc: 70.21%                                                     \n  Epoch  2/5 | Loss: 0.4595 | Train Acc: 16.62%                                                     \n  Epoch  3/5 | Loss: 1.3814 | Train Acc: 30.50%                                                     \n  Epoch  4/5 | Loss: 0.9289 | Train Acc: 60.51%                                                     \n  Epoch  5/5 | Loss: 0.7359 | Train Acc: 12.79%                                                     \n Client_2 local training complete (FedPer)\\n\nâœ“\n  Training Client 0... \\n Training Client_0 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.8479 | Train Acc: 76.05%                                                     \n  Epoch  2/5 | Loss: 0.4181 | Train Acc: 80.54%                                                     \n  Epoch  3/5 | Loss: 0.3254 | Train Acc: 88.95%                                                     \n  Epoch  4/5 | Loss: 0.2799 | Train Acc: 90.46%                                                     \n  Epoch  5/5 | Loss: 0.2561 | Train Acc: 94.31%                                                     \n Client_0 local training complete (FedPer)\\n\nâœ“\n  Training Client 3... \\n Training Client_3 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.8101 | Train Acc: 80.45%                                                     \n  Epoch  2/5 | Loss: 0.4105 | Train Acc: 88.77%                                                     \n  Epoch  3/5 | Loss: 0.3239 | Train Acc: 89.90%                                                     \n  Epoch  4/5 | Loss: 0.2728 | Train Acc: 89.42%                                                     \n  Epoch  5/5 | Loss: 0.2529 | Train Acc: 91.86%                                                     \n Client_3 local training complete (FedPer)\\n\nâœ“\n  Training Client 1... \\n Training Client_1 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.8553 | Train Acc: 83.38%                                                     \n  Epoch  2/5 | Loss: 0.4520 | Train Acc: 89.47%                                                     \n  Epoch  3/5 | Loss: 0.3719 | Train Acc: 86.71%                                                     \n  Epoch  4/5 | Loss: 0.3104 | Train Acc: 90.49%                                                     \n  Epoch  5/5 | Loss: 0.2703 | Train Acc: 93.94%                                                     \n Client_1 local training complete (FedPer)\\n\nâœ“\n  âœ“ Aggregated 25 shared parameter tensors\n Client Avg Test â€” Acc: 63.38% | Prec: 0.689 | Recall: 0.600 | F1: 0.614 | RMSE: 2.520\n   Individual: ['82.2%', '81.1%', '12.0%', '71.5%', '70.1%']\n\n Round 1 Results:\n   Accuracy: 63.38%\n   Best Accuracy: 63.38%\n   Precision: 0.689\n   Recall: 0.600\n   F1-Score: 0.614\n   RMSE: 2.520\n\n--------------------------------------------------------------------------------\nRound 2/50\n--------------------------------------------------------------------------------\nSelected 5 clients: [1, 4, 0, 3, 2]\n  Training Client 1... \\n Training Client_1 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.5152 | Train Acc: 89.32%                                                     \n  Epoch  2/5 | Loss: 0.3179 | Train Acc: 94.02%                                                     \n  Epoch  3/5 | Loss: 0.2607 | Train Acc: 93.76%                                                     \n  Epoch  4/5 | Loss: 0.2399 | Train Acc: 94.43%                                                     \n  Epoch  5/5 | Loss: 0.2079 | Train Acc: 95.82%                                                     \n Client_1 local training complete (FedPer)\\n\nâœ“\n  Training Client 4... \\n Training Client_4 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.4970 | Train Acc: 88.74%                                                     \n  Epoch  2/5 | Loss: 0.3267 | Train Acc: 91.73%                                                     \n  Epoch  3/5 | Loss: 0.2762 | Train Acc: 92.98%                                                     \n  Epoch  4/5 | Loss: 0.2495 | Train Acc: 92.98%                                                     \n  Epoch  5/5 | Loss: 0.2285 | Train Acc: 95.29%                                                     \n Client_4 local training complete (FedPer)\\n\nâœ“\n  Training Client 0... \\n Training Client_0 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.4664 | Train Acc: 92.22%                                                     \n  Epoch  2/5 | Loss: 0.2977 | Train Acc: 92.64%                                                     \n  Epoch  3/5 | Loss: 0.2396 | Train Acc: 94.55%                                                     \n  Epoch  4/5 | Loss: 0.2092 | Train Acc: 93.83%                                                     \n  Epoch  5/5 | Loss: 0.1983 | Train Acc: 95.85%                                                     \n Client_0 local training complete (FedPer)\\n\nâœ“\n  Training Client 3... \\n Training Client_3 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.4732 | Train Acc: 91.10%                                                     \n  Epoch  2/5 | Loss: 0.2766 | Train Acc: 93.36%                                                     \n  Epoch  3/5 | Loss: 0.2373 | Train Acc: 95.03%                                                     \n  Epoch  4/5 | Loss: 0.2096 | Train Acc: 94.74%                                                     \n  Epoch  5/5 | Loss: 0.2085 | Train Acc: 96.45%                                                     \n Client_3 local training complete (FedPer)\\n\nâœ“\n  Training Client 2... \\n Training Client_2 with FedPer (personalized head)...\n  Epoch  1/5 | Loss: 0.5632 | Train Acc: 89.88%                                                     \n  Epoch  2/5 | Loss: 0.3660 | Train Acc: 86.09%                                                     \n  Epoch  3/5 | Loss: 0.3251 | Train Acc: 90.10%                                                     \n  Epoch  4/5 | Loss: 0.3410 | Train Acc: 36.82%                                                     \n  Epoch  5/5 | Loss: 0.6861 | Train Acc: 74.68%                                                     \n Client_2 local training complete (FedPer)\\n\nâœ“\n  âœ“ Aggregated 25 shared parameter tensors\n Client Avg Test â€” Acc: 79.18% | Prec: 0.779 | Recall: 0.764 | F1: 0.761 | RMSE: 1.957\n   Individual: ['84.8%', '85.9%', '56.3%', '84.0%', '85.0%']\n\n Round 2 Results:\n   Accuracy: 79.18%\n   Best Accuracy: 79.18%\n   Precision: 0.779\n   Recall: 0.764\n   F1-Score: 0.761\n   RMSE: 1.957\n\n--------------------------------------------------------------------------------\nRound 3/50\n--------------------------------------------------------------------------------\nSelected 5 clients: [2, 3, 1, 0, 4]\n  Training Client 2... \\n Training Client_2 with FedPer (personalized head)...\n  Epoch 1/5:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 63/242 [00:05<00:14, 12.61it/s, loss=0.1716]","output_type":"stream"}],"execution_count":null}]}
