{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:07.857338Z",
     "iopub.status.busy": "2025-10-31T16:27:07.857077Z",
     "iopub.status.idle": "2025-10-31T16:27:07.868061Z",
     "shell.execute_reply": "2025-10-31T16:27:07.867048Z",
     "shell.execute_reply.started": "2025-10-31T16:27:07.857317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created: /kaggle/working/FedPer-PyTorch/main.py\n",
      "âœ… Created: /kaggle/working/FedPer-PyTorch/server.py\n",
      "âœ… Created: /kaggle/working/FedPer-PyTorch/client.py\n",
      "âœ… Created: /kaggle/working/FedPer-PyTorch/model.py\n",
      "âœ… Created: /kaggle/working/FedPer-PyTorch/get_data.py\n",
      "âœ… Created: /kaggle/working/FedPer-PyTorch/args.py\n",
      "\n",
      "ðŸŽ¯ Folder and files ready in: /kaggle/working/FedPer-PyTorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Target directory\n",
    "base_dir = \"/kaggle/working/FedPer-PyTorch\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Python files to create\n",
    "files = [\"main.py\", \"server.py\", \"client.py\", \"model.py\", \"get_data.py\", \"args.py\"]\n",
    "\n",
    "# Create each file if not exists\n",
    "for file in files:\n",
    "    file_path = os.path.join(base_dir, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(f\"# {file} â€” auto-created placeholder\\n\")\n",
    "        print(f\" Created: {file_path}\")\n",
    "    else:\n",
    "        print(f\" Already exists: {file_path}\")\n",
    "\n",
    "print(\"\\n Folder and files ready in:\", base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:07.869504Z",
     "iopub.status.busy": "2025-10-31T16:27:07.869234Z",
     "iopub.status.idle": "2025-10-31T16:27:07.884109Z",
     "shell.execute_reply": "2025-10-31T16:27:07.883413Z",
     "shell.execute_reply.started": "2025-10-31T16:27:07.869482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… args.py updated for FedPer with auto-configuration!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedPer-PyTorch/args.py\"\n",
    "\n",
    "new_code = '''\n",
    "# ========================================\n",
    "# args.py â€” FedPer Configuration (with dataset configs)\n",
    "# ========================================\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "# Dataset configurations - Change dataset name here to switch\n",
    "DATASET_CONFIGS = {\n",
    "    'pathmnist': {\n",
    "        'num_classes': 9,\n",
    "        'class_names': [\n",
    "            \"adipose tissue\", \"background\", \"debris\", \"lymphocytes\",\n",
    "            \"mucus\", \"smooth muscle\", \"normal colon mucosa\",\n",
    "            \"cancer-associated stroma\", \"colorecal adenocarcinoma epithelium\"\n",
    "        ],\n",
    "        'input_channels': 3\n",
    "    },\n",
    "    'tissuemnist': {\n",
    "        'num_classes': 8,\n",
    "        'class_names': [\n",
    "            \"collecting duct\", \"distal convoluted tubule\",\n",
    "            \"glomerular endothelial cells\", \"interstitial endothelial cells\",\n",
    "            \"leukocytes\", \"podocytes\", \"proximal tubule\", \"thick ascending limb\"\n",
    "        ],\n",
    "        'input_channels': 1\n",
    "    },\n",
    "    'organamnist': {\n",
    "        'num_classes': 11,\n",
    "        'class_names': [\n",
    "            \"bladder\", \"femur-left\", \"femur-right\", \"heart\",\n",
    "            \"kidney-left\", \"kidney-right\", \"liver\",\n",
    "            \"lung-left\", \"lung-right\", \"spleen\", \"pelvis\"\n",
    "        ],\n",
    "        'input_channels': 1\n",
    "    },\n",
    "    'octmnist': {\n",
    "        'num_classes': 4,\n",
    "        'class_names': [\n",
    "            \"choroidal neovascularization\", \"diabetic macular edema\",\n",
    "            \"drusen\", \"normal\"\n",
    "        ],\n",
    "        'input_channels': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"FedPer - Config File\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # MAIN: Change this to switch dataset\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--dataset', type=str, default='octmnist', \n",
    "                        choices=['pathmnist', 'tissuemnist', 'organamnist', 'octmnist'],\n",
    "                        help='MedMNIST dataset to use')\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Federated Learning Parameters\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--E', type=int, default=5, help='local epochs')\n",
    "    parser.add_argument('--r', type=int, default=50, help='number of communication rounds')\n",
    "    parser.add_argument('--K', type=int, default=5, help='total number of clients')\n",
    "    parser.add_argument('--C', type=float, default=1, help='client sampling rate per round')\n",
    "    parser.add_argument('--B', type=int, default=32, help='batch size')\n",
    "    parser.add_argument('--use_combined', action='store_true', \n",
    "                        help='Use train+val+test combined (all data)')\n",
    "\n",
    "    # -------------------------------\n",
    "    # Model parameters (auto-configured)\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--clip_model', type=str, default='ViT-B/32', help='CLIP model variant')\n",
    "    parser.add_argument('--freeze_clip', action='store_true', help='Freeze CLIP backbone')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='dropout (stronger regularization)')\n",
    "    parser.add_argument('--Kp', type=int, default=2, help='number of personalized layers')\n",
    "\n",
    "    # -------------------------------\n",
    "    # Optimizer Settings\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--lr', type=float, default=0.003, help='learning rate')\n",
    "    parser.add_argument('--optimizer', type=str, default='sgd', help='optimizer')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum for SGD')\n",
    "\n",
    "    # -------------------------------\n",
    "    # Checkpoint Settings\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--checkpoint_dir', type=str,\n",
    "                        default='./checkpoints_fedper',\n",
    "                        help='directory to save checkpoints')\n",
    "    # -------------------------------\n",
    "    # Adversarial Attack Settings\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--enable_attack', action='store_true',\n",
    "                        help='Enable adversarial attacks during testing')\n",
    "    parser.add_argument('--attack_type', type=str, default='fgsm',\n",
    "                        choices=['fgsm', 'pgd', 'cw'],\n",
    "                        help='Type of adversarial attack')\n",
    "    parser.add_argument('--attack_epsilon', type=float, default=0.03,\n",
    "                        help='Perturbation budget for FGSM/PGD (L-inf norm)')\n",
    "    parser.add_argument('--pgd_alpha', type=float, default=0.01,\n",
    "                        help='Step size for PGD attack')\n",
    "    parser.add_argument('--pgd_steps', type=int, default=10,\n",
    "                        help='Number of PGD iterations')\n",
    "    parser.add_argument('--cw_c', type=float, default=1.0,\n",
    "                        help='C&W attack confidence parameter')\n",
    "    parser.add_argument('--cw_steps', type=int, default=100,\n",
    "                        help='Number of optimization steps for C&W')\n",
    "    parser.add_argument('--cw_lr', type=float, default=0.01,\n",
    "                        help='Learning rate for C&W optimization')\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Device\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--device', default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                        help='cuda or cpu')\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Balanced Non-IID Settings\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--dominant_ratio', type=float, default=0.7,\n",
    "                        help='Fraction of dominant class per client (0-1)')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    # Auto-configure based on selected dataset\n",
    "    if args.dataset in DATASET_CONFIGS:\n",
    "        config = DATASET_CONFIGS[args.dataset]\n",
    "        args.num_classes = config['num_classes']\n",
    "        args.input_channels = config['input_channels']\n",
    "        args.class_names = config['class_names']\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {args.dataset} not configured!\")\n",
    "    \n",
    "    return args\n",
    "'''\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" args.py updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:07.982050Z",
     "iopub.status.busy": "2025-10-31T16:27:07.981811Z",
     "iopub.status.idle": "2025-10-31T16:27:07.990615Z",
     "shell.execute_reply": "2025-10-31T16:27:07.989754Z",
     "shell.execute_reply.started": "2025-10-31T16:27:07.982032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… get_data.py updated for FedPer with auto-configuration!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedPer-PyTorch/get_data.py\"\n",
    "\n",
    "new_code = r'''\n",
    "# ========================================\n",
    "# get_data.py â€” True Balanced Non-IID Distribution for FedPer\n",
    "# Supports: pathmnist, tissuemnist, organamnist, octmnist\n",
    "# ========================================\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from medmnist import INFO\n",
    "from medmnist.dataset import PathMNIST, TissueMNIST, OrganAMNIST, OCTMNIST\n",
    "\n",
    "# Map dataset names to classes\n",
    "DATASET_MAP = {\n",
    "    'pathmnist': PathMNIST,\n",
    "    'tissuemnist': TissueMNIST,\n",
    "    'organamnist': OrganAMNIST,\n",
    "    'octmnist': OCTMNIST\n",
    "}\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Returns train and test transforms (handles grayscale and RGB)\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.Grayscale(num_output_channels=3),  # ensures 3 channels for CLIP\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, test_transform\n",
    "\n",
    "def balanced_noniid_split(dataset, num_clients, dominant_ratio=0.7):\n",
    "    \"\"\"Balanced Non-IID split: dominant_ratio% to main client, rest distributed\"\"\"\n",
    "    labels = np.array([dataset[i][1].item() for i in range(len(dataset))])\n",
    "    num_classes = len(np.unique(labels))\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "\n",
    "    # Prepare class indices\n",
    "    class_indices = {c: np.where(labels == c)[0].tolist() for c in range(num_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "\n",
    "    # Step 1: assign dominant_ratio% of dominant class to primary client\n",
    "    for client_id in range(num_clients):\n",
    "        dominant_class = client_id % num_classes\n",
    "        n_dominant = int(len(class_indices[dominant_class]) * dominant_ratio)\n",
    "        if n_dominant > 0:\n",
    "            client_indices[client_id].extend(class_indices[dominant_class][:n_dominant])\n",
    "            class_indices[dominant_class] = class_indices[dominant_class][n_dominant:]\n",
    "\n",
    "    # Step 2: distribute remaining samples equally among other clients\n",
    "    for c in range(num_classes):\n",
    "        remaining = class_indices[c]\n",
    "        np.random.shuffle(remaining)\n",
    "        other_clients = [i for i in range(num_clients) if i % num_classes != c]\n",
    "        for i, idx in enumerate(remaining):\n",
    "            client_id = other_clients[i % len(other_clients)]\n",
    "            client_indices[client_id].append(idx)\n",
    "\n",
    "    # Shuffle each client's indices\n",
    "    for i in range(num_clients):\n",
    "        np.random.shuffle(client_indices[i])\n",
    "\n",
    "    return client_indices\n",
    "\n",
    "def load_medmnist_data(args):\n",
    "    \"\"\"Load MedMNIST data for FedPer - automatically configured based on args.dataset\"\"\"\n",
    "    train_transform, test_transform = get_transforms()\n",
    "    data_root = './data/medmnist'\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "    data_flag = args.dataset.lower()\n",
    "    if data_flag not in DATASET_MAP:\n",
    "        raise ValueError(f\"Dataset {data_flag} not supported. Choose from {list(DATASET_MAP.keys())}\")\n",
    "\n",
    "    n_classes = args.num_classes\n",
    "\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(f\" Loading {data_flag.upper()} Dataset for FedPer\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "    DatasetClass = DATASET_MAP[data_flag]\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = DatasetClass(root=data_root, split='train', download=True, transform=train_transform)\n",
    "    val_dataset = DatasetClass(root=data_root, split='val', download=True, transform=train_transform)\n",
    "    test_dataset = DatasetClass(root=data_root, split='test', download=True, transform=test_transform)\n",
    "\n",
    "    combined_train = ConcatDataset([train_dataset, val_dataset])\n",
    "    print(f\" Using Train+Val: {len(combined_train)} samples for federated learning\")\n",
    "\n",
    "    # Load or create client indices\n",
    "    cache_file = f'./data/medmnist/client_indices_{data_flag}_K{args.K}_dr{args.dominant_ratio}_fedper.pkl'\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"\\\\n Loading cached split from: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            client_indices = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"\\\\n Creating balanced Non-IID split (dominant_ratio={args.dominant_ratio})...\")\n",
    "        client_indices = balanced_noniid_split(combined_train, args.K, dominant_ratio=args.dominant_ratio)\n",
    "        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(client_indices, f)\n",
    "        print(f\" Split cached to: {cache_file}\")\n",
    "\n",
    "    # Print client-wise class distribution\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\" Client-wise Image Distribution\")\n",
    "    print(\"=\"*60)\n",
    "    total_samples = 0\n",
    "    for client_id, indices in enumerate(client_indices):\n",
    "        client_labels = [combined_train[i][1].item() for i in indices]\n",
    "        label_counts = np.bincount(client_labels, minlength=n_classes)\n",
    "        total_client = len(indices)\n",
    "        total_samples += total_client\n",
    "        distribution_str = \", \".join([f\"C{c}:{label_counts[c]}\" for c in range(n_classes)])\n",
    "        dominant_class = np.argmax(label_counts)\n",
    "        dominant_pct = (label_counts[dominant_class] / total_client) * 100\n",
    "        print(f\"Client {client_id:2d}: {total_client:5d} samples | Dominant: Class {dominant_class} ({dominant_pct:.1f}%)\")\n",
    "        print(f\"           [{distribution_str}]\")\n",
    "\n",
    "    print(f\"\\\\nTotal samples: {total_samples}\")\n",
    "    print(\"=\"*60 + \"\\\\n\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    client_loaders = []\n",
    "    for i, indices in enumerate(client_indices):\n",
    "        subset = Subset(combined_train, indices)\n",
    "        loader = DataLoader(subset, batch_size=args.B, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        client_loaders.append(loader)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.B, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return client_loaders, test_loader\n",
    "'''\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" get_data.py updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:07.992455Z",
     "iopub.status.busy": "2025-10-31T16:27:07.992145Z",
     "iopub.status.idle": "2025-10-31T16:27:08.011238Z",
     "shell.execute_reply": "2025-10-31T16:27:08.010447Z",
     "shell.execute_reply.started": "2025-10-31T16:27:07.992432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… model.py FIXED with get_shared_params() method!\n",
      "\n",
      "ðŸ“Œ Important Notes:\n",
      "- get_shared_params() now exists (returns empty list since CLIP is frozen)\n",
      "- get_personalized_params() returns the MLP head\n",
      "- Since CLIP is frozen, FedPer = local training only (no aggregation)\n",
      "\n",
      "ðŸš€ You can now run your training script!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedPer-PyTorch/model.py\"\n",
    "new_code = r'''\n",
    "# ========================================\n",
    "# model.py â€” CLIP-based FedPer Classifier (Fixed)\n",
    "# ========================================\n",
    "import torch\n",
    "from torch import nn\n",
    "import clip\n",
    "\n",
    "class CLIPFedPerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CLIP-based Personalized Classifier for Federated Learning (FedPer)\n",
    "    ---------------------------------------------------------------\n",
    "    Shared Backbone: Frozen CLIP visual encoder (NOT aggregated - frozen)\n",
    "    Personalized Layers: Trainable MLP head (unique per client, NOT aggregated)\n",
    "    \n",
    "    NOTE: Since CLIP is frozen, there are NO shared trainable parameters.\n",
    "    In this case, FedPer becomes purely local training with no aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, name='clip_fedper_model'):\n",
    "        super(CLIPFedPerClassifier, self).__init__()\n",
    "        self.name = name\n",
    "        self.num_classes = args.num_classes\n",
    "        self.Kp = getattr(args, 'Kp', 1.0)\n",
    "        self.dropout = getattr(args, 'dropout', 0.3)\n",
    "        \n",
    "        # Load pretrained CLIP model\n",
    "        self.clip_model, self.preprocess = clip.load(args.clip_model, device=args.device)\n",
    "        \n",
    "        # Freeze CLIP encoder\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get CLIP feature dimension\n",
    "        self.feature_dim = self.clip_model.visual.output_dim\n",
    "        \n",
    "        # Auto-configured class names\n",
    "        self.class_names = args.class_names\n",
    "        \n",
    "        # Personalized MLP Head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout * 0.7),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout * 0.5),\n",
    "            \n",
    "            nn.Linear(256, self.num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.head.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            image_features = image_features.float()\n",
    "        \n",
    "        logits = self.head(image_features)\n",
    "        return logits\n",
    "    \n",
    "    # ========================================\n",
    "    # FedPer Required Methods (FIXED)\n",
    "    # ========================================\n",
    "    def get_shared_params(self):\n",
    "        \"\"\"\n",
    "        Returns trainable parameters for aggregation.\n",
    "        Since CLIP is frozen, returns empty list (no aggregation).\n",
    "        \"\"\"\n",
    "        shared_params = []\n",
    "        for param in self.clip_model.parameters():\n",
    "            if param.requires_grad:\n",
    "                shared_params.append(param)\n",
    "        return shared_params\n",
    "    \n",
    "    def get_personalized_params(self):\n",
    "        \"\"\"Returns personalized head parameters (NOT aggregated).\"\"\"\n",
    "        return [p for p in self.head.parameters() if p.requires_grad]\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Return all trainable parameters.\"\"\"\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"  CLIP backbone unfrozen - will be aggregated!\")\n",
    "'''\n",
    "\n",
    "# Save the fixed code\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" model.py created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:08.012407Z",
     "iopub.status.busy": "2025-10-31T16:27:08.012166Z",
     "iopub.status.idle": "2025-10-31T16:27:08.030124Z",
     "shell.execute_reply": "2025-10-31T16:27:08.029344Z",
     "shell.execute_reply.started": "2025-10-31T16:27:08.012371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… client.py updated for FedPer with personalized heads!\n",
      "ðŸ“Š Training will now respect shared + personalized layers.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedPer-PyTorch/client.py\"\n",
    "\n",
    "new_code = r'''\n",
    "# ========================================\n",
    "# client.py â€” Client-side helper for FedPer (personalized heads + tqdm)\n",
    "# ========================================\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, model, train_loader, device, val_loader=None, lr=0.0001, weight_decay=5e-4, Kp=2, shared_params=None):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.Kp = Kp  # number of personalized layers (head)\n",
    "        self.shared_params = [p.clone().detach() for p in shared_params] if shared_params else None\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=self.weight_decay)\n",
    "\n",
    "    def compute_accuracy(self, loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                labels = labels.squeeze()\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        return 100 * correct / total if total > 0 else 0\n",
    "\n",
    "    def train(self, epochs=1, shared_params=None):\n",
    "        \"\"\"Train client model; keep head personalized, optionally update shared layers\"\"\"\n",
    "        print(f\"\\\\n Training {self.model.name} with FedPer (personalized head)...\")\n",
    "\n",
    "        # Initialize shared layers if provided\n",
    "        if shared_params is not None:\n",
    "            model_shared = self.model.get_shared_params()\n",
    "            for m_p, g_p in zip(model_shared, shared_params):\n",
    "                m_p.data = g_p.data.clone()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            pbar = tqdm(self.train_loader, desc=f\"  Epoch {epoch+1}/{epochs}\", ncols=100, leave=False)\n",
    "            \n",
    "            for images, labels in pbar:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                labels = labels.squeeze()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            train_acc = self.compute_accuracy(self.train_loader)\n",
    "            log_msg = f\"  Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\"\n",
    "\n",
    "            if self.val_loader is not None:\n",
    "                val_acc = self.compute_accuracy(self.val_loader)\n",
    "                val_loss = 0\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in self.val_loader:\n",
    "                        images, labels = images.to(self.device), labels.to(self.device)\n",
    "                        labels = labels.squeeze()\n",
    "                        outputs = self.model(images)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "                val_loss /= len(self.val_loader)\n",
    "                log_msg += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n",
    "\n",
    "            print(log_msg)\n",
    "\n",
    "        print(f\" {self.model.name} local training complete (FedPer)\\\\n\")\n",
    "        return self.model.state_dict()\n",
    "'''\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" client.py updated!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:08.031217Z",
     "iopub.status.busy": "2025-10-31T16:27:08.030979Z",
     "iopub.status.idle": "2025-10-31T16:27:08.049651Z",
     "shell.execute_reply": "2025-10-31T16:27:08.048921Z",
     "shell.execute_reply.started": "2025-10-31T16:27:08.031154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fixed server.py...\n",
      "âœ… server.py updated with PyTorch 2.6 compatibility fix!\n",
      "\n",
      "Key changes:\n",
      "1. Added weights_only=False to torch.load()\n",
      "2. Added better checkpoint loading feedback\n",
      "3. Improved training progress display\n",
      "\n",
      "You can now run your training script again.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "COMPLETE_SERVER_CODE = '''\n",
    "# ========================================\n",
    "# server.py â€” FedPer Server (aggregate shared layers only)\n",
    "# ========================================\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from model import CLIPFedPerClassifier as ImageClassifier\n",
    "from get_data import load_medmnist_data\n",
    "from client import Client\n",
    "\n",
    "class FedPerServer:\n",
    "    def __init__(self, args, resume_from=None):\n",
    "        self.args = args\n",
    "        os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "        self.current_round = 0\n",
    "        self.best_global_acc = 0\n",
    "        self.history = {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []}\n",
    "\n",
    "        self.global_model = ImageClassifier(args, name=\"server\").to(args.device)\n",
    "        self.client_models = []\n",
    "        for i in range(self.args.K):\n",
    "            model = copy.deepcopy(self.global_model)\n",
    "            model.name = f\"Client_{i}\"\n",
    "            self.client_models.append(model)\n",
    "\n",
    "        self.client_loaders, self.test_loader = load_medmnist_data(args)\n",
    "        if resume_from:\n",
    "            self.load_checkpoint(resume_from)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load checkpoint with weights_only=False for compatibility\"\"\"\n",
    "        print(f\"\\\\n Loading checkpoint from: {checkpoint_path}\")\n",
    "        \n",
    "        # Fix for PyTorch 2.6: set weights_only=False to load numpy objects\n",
    "        checkpoint = torch.load(\n",
    "            checkpoint_path, \n",
    "            map_location=self.args.device,\n",
    "            weights_only=False  \n",
    "        )\n",
    "        \n",
    "        # Load server model\n",
    "        self.global_model.load_state_dict(checkpoint['server_state_dict'])\n",
    "        \n",
    "        # Load client models if available\n",
    "        if 'client_state_dicts' in checkpoint:\n",
    "            for i, state_dict in enumerate(checkpoint['client_state_dicts']):\n",
    "                self.client_models[i].load_state_dict(state_dict)\n",
    "        \n",
    "        # Restore training state\n",
    "        self.current_round = checkpoint.get('round', 0)\n",
    "        self.best_global_acc = checkpoint.get('best_global_acc', 0)\n",
    "        self.history = checkpoint.get('history', {\n",
    "            'rounds': [], \n",
    "            'avg_accuracy': [], \n",
    "            'best_accuracy': []\n",
    "        })\n",
    "        \n",
    "        print(f\" Checkpoint loaded successfully!\")\n",
    "        print(f\"   Resuming from Round: {self.current_round}/{self.args.r}\")\n",
    "        print(f\"   Best Accuracy: {self.best_global_acc:.2f}%\")\n",
    "        print(f\"   History: {len(self.history['rounds'])} rounds recorded\")\n",
    "\n",
    "    def dispatch(self, selected_clients):\n",
    "        \"\"\"Send global shared params to clients\"\"\"\n",
    "        global_shared_params = self.global_model.get_shared_params()\n",
    "        for idx in selected_clients:\n",
    "            client_model = self.client_models[idx]\n",
    "            client_params = client_model.get_shared_params()\n",
    "            for c_param, g_param in zip(client_params, global_shared_params):\n",
    "                c_param.data = g_param.data.clone()\n",
    "\n",
    "    def aggregate(self, selected_clients):\n",
    "        \"\"\"Aggregate shared layers only\"\"\"\n",
    "        total_samples = sum([len(self.client_loaders[idx].dataset) for idx in selected_clients])\n",
    "        shared_keys = [p for p in self.global_model.get_shared_params()]\n",
    "        # Initialize aggregation\n",
    "        agg_params = [torch.zeros_like(p.data) for p in shared_keys]\n",
    "\n",
    "        for idx in selected_clients:\n",
    "            client_model = self.client_models[idx]\n",
    "            client_shared = client_model.get_shared_params()\n",
    "            weight = len(self.client_loaders[idx].dataset) / total_samples\n",
    "            for i, p in enumerate(client_shared):\n",
    "                agg_params[i] += p.data * weight\n",
    "\n",
    "        # Update global model shared params\n",
    "        for p, agg_p in zip(shared_keys, agg_params):\n",
    "            p.data = agg_p.data.clone()\n",
    "\n",
    "    def client_update(self, idx):\n",
    "        client_model = self.client_models[idx]\n",
    "        client_loader = self.client_loaders[idx]\n",
    "        client_obj = Client(\n",
    "            model=client_model,\n",
    "            train_loader=client_loader,\n",
    "            device=self.args.device,\n",
    "            lr=self.args.lr,\n",
    "            weight_decay=self.args.weight_decay,\n",
    "            Kp=self.args.Kp\n",
    "        )\n",
    "        # Pass global shared params for dispatching\n",
    "        global_shared_params = self.global_model.get_shared_params()\n",
    "        client_obj.train(epochs=self.args.E, shared_params=global_shared_params)\n",
    "\n",
    "    def test_global_model(self):\n",
    "        self.global_model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to(self.args.device), labels.to(self.args.device)\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                outputs = self.global_model(images)\n",
    "                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n",
    "                    outputs = outputs.logits\n",
    "    \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "    \n",
    "        # Accuracy\n",
    "        acc = 100 * np.mean(all_labels == all_preds)\n",
    "        # Precision, Recall, F1-score (macro avg)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        # RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "    \n",
    "        print(f\" Global Test â€” Acc: {acc:.2f}% | Prec: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | RMSE: {rmse:.3f}\")\n",
    "    \n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'rmse': rmse\n",
    "        }\n",
    "\n",
    "    def save_checkpoint(self, round_num, avg_acc):\n",
    "        \"\"\"Save checkpoint with proper formatting\"\"\"\n",
    "        checkpoint = {\n",
    "            'round': round_num,\n",
    "            'server_state_dict': self.global_model.state_dict(),\n",
    "            'client_state_dicts': [model.state_dict() for model in self.client_models],\n",
    "            'best_global_acc': self.best_global_acc,\n",
    "            'history': self.history,\n",
    "            'args': vars(self.args)\n",
    "        }\n",
    "        \n",
    "        # Save periodic checkpoints\n",
    "        if round_num % 5 == 0:\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.args.checkpoint_dir, \n",
    "                f'checkpoint_round_{round_num}.pth'\n",
    "            )\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        latest_path = os.path.join(self.args.checkpoint_dir, 'checkpoint_latest.pth')\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_acc >= self.best_global_acc:\n",
    "            best_path = os.path.join(self.args.checkpoint_dir, 'best_model.pth')\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"    New best accuracy: {avg_acc:.2f}%\")\n",
    "        \n",
    "        # Save training history as JSON\n",
    "        history_path = os.path.join(self.args.checkpoint_dir, 'training_history.json')\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        start_round = self.current_round\n",
    "        \n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(f\" STARTING FEDERATED TRAINING\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Starting from Round: {start_round + 1}/{self.args.r}\")\n",
    "        print(f\"Current Best Accuracy: {self.best_global_acc:.2f}%\")\n",
    "        print(f\"{'='*80}\\\\n\")\n",
    "        \n",
    "        for r in range(start_round, self.args.r):\n",
    "            print(f\"\\\\n{'â”€'*80}\")\n",
    "            print(f\"Round {r+1}/{self.args.r}\")\n",
    "            print(f\"{'â”€'*80}\")\n",
    "            \n",
    "            # Select clients\n",
    "            m = max(int(self.args.C * self.args.K), 1)\n",
    "            selected_clients = random.sample(range(self.args.K), m)\n",
    "            print(f\"Selected {m} clients: {selected_clients}\")\n",
    "            \n",
    "            # Dispatch global model\n",
    "            self.dispatch(selected_clients)\n",
    "            \n",
    "            # Client updates\n",
    "            for idx in selected_clients:\n",
    "                print(f\"  Training Client {idx}...\", end=\" \")\n",
    "                self.client_update(idx)\n",
    "                print(\"âœ“\")\n",
    "            \n",
    "            # Aggregate\n",
    "            self.aggregate(selected_clients)\n",
    "            \n",
    "            # Evaluate\n",
    "            avg_acc = self.test_global_model()\n",
    "            \n",
    "            # Update history\n",
    "            self.history['rounds'].append(r+1)\n",
    "            self.history['avg_accuracy'].append(avg_acc)\n",
    "            self.history['best_accuracy'].append(max(self.best_global_acc, avg_acc))\n",
    "            \n",
    "            # Update best accuracy\n",
    "            if avg_acc > self.best_global_acc:\n",
    "                self.best_global_acc = avg_acc\n",
    "            \n",
    "            # Update current round\n",
    "            self.current_round = r + 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(r+1, avg_acc)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\\\n Round {r+1} Results:\")\n",
    "            print(f\"   Average Accuracy: {avg_acc:.2f}%\")\n",
    "            print(f\"   Best Accuracy: {self.best_global_acc:.2f}%\")\n",
    "        \n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(f\" TRAINING COMPLETE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best Accuracy Achieved: {self.best_global_acc:.2f}%\")\n",
    "        print(f\"{'='*80}\\\\n\")\n",
    "        \n",
    "        return self.global_model\n",
    "'''\n",
    "\n",
    "# Write the fixed code\n",
    "print(\"Writing fixed server.py...\")\n",
    "with open(\"/kaggle/working/FedPer-PyTorch/server.py\", \"w\") as f:\n",
    "    f.write(COMPLETE_SERVER_CODE)\n",
    "\n",
    "print(\" server.py updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:08.102711Z",
     "iopub.status.busy": "2025-10-31T16:27:08.102518Z",
     "iopub.status.idle": "2025-10-31T16:27:08.109848Z",
     "shell.execute_reply": "2025-10-31T16:27:08.108910Z",
     "shell.execute_reply.started": "2025-10-31T16:27:08.102697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… main.py updated for FedPer with personalized heads and proper string literal!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedPer-PyTorch/main.py\"\n",
    "\n",
    "new_code = r\"\"\"\n",
    "# ========================================\n",
    "# main.py â€” Run FedPer with Resume Support (FedProx style auto-resume)\n",
    "# ========================================\n",
    "from args import args_parser\n",
    "from server import FedPerServer\n",
    "from get_data import load_medmnist_data\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Average Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['rounds'], history['avg_accuracy'], 'b-', label='Avg Accuracy', linewidth=2)\n",
    "    plt.plot(history['rounds'], history['best_accuracy'], 'r--', label='Best Accuracy', linewidth=2)\n",
    "    plt.xlabel('Communication Round')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Federated Learning Accuracy Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy Improvement\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if len(history['avg_accuracy']) > 1:\n",
    "        improvements = [history['avg_accuracy'][i] - history['avg_accuracy'][i-1] \n",
    "                        for i in range(1, len(history['avg_accuracy']))]\n",
    "        plt.bar(history['rounds'][1:], improvements, alpha=0.7)\n",
    "        plt.axhline(y=0, color='r', linestyle='-', linewidth=0.5)\n",
    "        plt.xlabel('Communication Round')\n",
    "        plt.ylabel('Accuracy Change (%)')\n",
    "        plt.title('Round-to-Round Accuracy Change')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f' Training plot saved at: {save_path}')\n",
    "\n",
    "def main():\n",
    "    # Load arguments from args.py\n",
    "    args = args_parser()\n",
    "    \n",
    "    # Print configuration\n",
    "    print('\\\\n' + '='*60)\n",
    "    print(' FEDERATED LEARNING CONFIGURATION (FedPer)')\n",
    "    print('='*60)\n",
    "    print(f'Dataset: {args.dataset.upper()}')\n",
    "    print(f'Clients: {args.K} | Rounds: {args.r} | Local Epochs: {args.E}')\n",
    "    print(f'Batch Size: {args.B} | Learning Rate: {args.lr}')\n",
    "    print(f'CLIP Model: {args.clip_model} | Device: {args.device}')\n",
    "    print(f'Non-IID: {args.dominant_ratio*100:.0f}% dominant class per client')\n",
    "    print(f'Personalized Layers (Kp): {args.Kp}')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load client loaders & test loader\n",
    "    client_loaders, test_loader = load_medmnist_data(args)\n",
    "    \n",
    "    # Check for existing checkpoint (FedProx style auto-resume)\n",
    "    latest_checkpoint = os.path.join(args.checkpoint_dir, 'checkpoint_latest.pth')\n",
    "    resume_from = None\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        try:\n",
    "            checkpoint = torch.load(latest_checkpoint, map_location=args.device, weights_only=False)\n",
    "            completed_rounds = checkpoint.get('round', 0)\n",
    "            best_acc = checkpoint.get('best_global_acc', 0)\n",
    "            print('\\\\n' + '='*60)\n",
    "            print(' CHECKPOINT DETECTED â€” auto-resuming!')\n",
    "            print('='*60)\n",
    "            print(f' Completed Rounds: {completed_rounds}/{args.r}')\n",
    "            print(f' Best Accuracy: {best_acc:.2f}%')\n",
    "            resume_from = latest_checkpoint\n",
    "        except Exception as e:\n",
    "            print(f' Error loading checkpoint: {e}')\n",
    "            print(' Starting fresh training...')\n",
    "    else:\n",
    "        print('\\\\n' + '='*60)\n",
    "        print(' No checkpoint found. Starting fresh training...')\n",
    "        print('='*60)\n",
    "    \n",
    "    # Initialize FedPer server\n",
    "    server = FedPerServer(args, resume_from=resume_from)\n",
    "    \n",
    "    # Run federated training\n",
    "    final_model = server.run()\n",
    "    \n",
    "    # Save final global model (shared layers)\n",
    "    final_model_path = os.path.join(args.checkpoint_dir, 'final_global_model.pth')\n",
    "    torch.save(final_model.state_dict(), final_model_path)\n",
    "    print(f'\\\\n Final global model (shared layers) saved at: {final_model_path}')\n",
    "    \n",
    "    # Save best model if exists\n",
    "    best_model_path = os.path.join(args.checkpoint_dir, 'best_model.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f' Best global model saved at: {best_model_path}')\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(args.checkpoint_dir, 'training_history.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(server.history, f, indent=4)\n",
    "    print(f' Training history saved at: {history_path}')\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_path = os.path.join(args.checkpoint_dir, 'training_plot.png')\n",
    "    plot_training_history(server.history, plot_path)\n",
    "    \n",
    "    # Print summary\n",
    "    print('\\\\n' + '='*60)\n",
    "    print(' TRAINING SUMMARY (FedPer)')\n",
    "    print('='*60)\n",
    "    print(f' Best Global Accuracy: {server.best_global_acc:.2f}%')\n",
    "    print(f' Final Round Accuracy: {server.history[\"avg_accuracy\"][-1]:.2f}%')\n",
    "    print(f' Total Improvement: {server.history[\"avg_accuracy\"][-1] - server.history[\"avg_accuracy\"][0]:.2f}%')\n",
    "    print(f' All checkpoints saved in: {args.checkpoint_dir}')\n",
    "    print('='*60)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" main.py updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:27:08.111378Z",
     "iopub.status.busy": "2025-10-31T16:27:08.111179Z",
     "iopub.status.idle": "2025-10-31T16:28:22.160969Z",
     "shell.execute_reply": "2025-10-31T16:28:22.160109Z",
     "shell.execute_reply.started": "2025-10-31T16:27:08.111363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install medmnist --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:28:22.162419Z",
     "iopub.status.busy": "2025-10-31T16:28:22.162081Z",
     "iopub.status.idle": "2025-10-31T16:28:29.174860Z",
     "shell.execute_reply": "2025-10-31T16:28:29.174186Z",
     "shell.execute_reply.started": "2025-10-31T16:28:22.162365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_48u8q3f\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_48u8q3f\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=1dd8f6b1c3170fc8b5fd80a1d4d4ffccb06284ee044e21bf8e003226d7d73728\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hbb3cdbn/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T16:28:29.176462Z",
     "iopub.status.busy": "2025-10-31T16:28:29.176214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "ðŸš€ FEDERATED LEARNING CONFIGURATION (FedPer)\n",
      "============================================================\n",
      "Dataset: OCTMNIST\n",
      "Clients: 5 | Rounds: 50 | Local Epochs: 5\n",
      "Batch Size: 32 | Learning Rate: 0.003\n",
      "CLIP Model: ViT-B/32 | Device: cuda\n",
      "Non-IID: 70% dominant class per client\n",
      "Personalized Layers (Kp): 2\n",
      "============================================================\n",
      "\\n============================================================\n",
      "ðŸ“Š Loading OCTMNIST Dataset for FedPer\n",
      "============================================================\n",
      "Number of classes: 4\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54.9M/54.9M [00:40<00:00, 1.37MB/s]\n",
      "âœ… Using Train+Val: 108309 samples for federated learning\n",
      "\\nðŸ”§ Creating balanced Non-IID split (dominant_ratio=0.7)...\n",
      "âœ… Split cached to: ./data/medmnist/client_indices_octmnist_K5_dr0.7_fedper.pkl\n",
      "\\n============================================================\n",
      "ðŸ“ˆ Client-wise Image Distribution\n",
      "============================================================\n",
      "Client  0: 31378 samples | Dominant: Class 0 (83.0%)\n",
      "           [C0:26043, C1:852, C2:647, C3:3836]\n",
      "Client  1: 13542 samples | Dominant: Class 1 (58.7%)\n",
      "           [C0:1117, C1:7943, C2:646, C3:3836]\n",
      "Client  2: 11833 samples | Dominant: Class 2 (51.0%)\n",
      "           [C0:1116, C1:851, C2:6031, C3:3835]\n",
      "Client  3: 38411 samples | Dominant: Class 3 (93.2%)\n",
      "           [C0:1116, C1:851, C2:646, C3:35798]\n",
      "Client  4: 13145 samples | Dominant: Class 0 (59.4%)\n",
      "           [C0:7813, C1:851, C2:646, C3:3835]\n",
      "\\nTotal samples: 108309\n",
      "============================================================\\n\n",
      "\\n============================================================\n",
      "ðŸš€ No checkpoint found. Starting fresh training...\n",
      "============================================================\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:02<00:00, 146MiB/s]\n",
      "\\n============================================================\n",
      "ðŸ“Š Loading OCTMNIST Dataset for FedPer\n",
      "============================================================\n",
      "Number of classes: 4\n",
      "âœ… Using Train+Val: 108309 samples for federated learning\n",
      "\\nðŸ“‚ Loading cached split from: ./data/medmnist/client_indices_octmnist_K5_dr0.7_fedper.pkl\n",
      "\\n============================================================\n",
      "ðŸ“ˆ Client-wise Image Distribution\n",
      "============================================================\n",
      "Client  0: 31378 samples | Dominant: Class 0 (83.0%)\n",
      "           [C0:26043, C1:852, C2:647, C3:3836]\n",
      "Client  1: 13542 samples | Dominant: Class 1 (58.7%)\n",
      "           [C0:1117, C1:7943, C2:646, C3:3836]\n",
      "Client  2: 11833 samples | Dominant: Class 2 (51.0%)\n",
      "           [C0:1116, C1:851, C2:6031, C3:3835]\n",
      "Client  3: 38411 samples | Dominant: Class 3 (93.2%)\n",
      "           [C0:1116, C1:851, C2:646, C3:35798]\n",
      "Client  4: 13145 samples | Dominant: Class 0 (59.4%)\n",
      "           [C0:7813, C1:851, C2:646, C3:3835]\n",
      "\\nTotal samples: 108309\n",
      "============================================================\\n\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ STARTING FEDERATED TRAINING\n",
      "================================================================================\n",
      "Starting from Round: 1/50\n",
      "Current Best Accuracy: 0.00%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Round 1/50\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Selected 5 clients: [3, 2, 4, 0, 1]\n",
      "  Training Client 3... \\nðŸ“Š Training Client_3 with FedPer (personalized head)...\n",
      "  Epoch  1/5 | Loss: 0.3068 | Train Acc: 94.05%                                                     \n",
      "  Epoch  2/5 | Loss: 0.2604 | Train Acc: 94.41%                                                     \n",
      "  Epoch  3/5 | Loss: 0.2448 | Train Acc: 94.47%                                                     \n",
      "  Epoch  4/5 | Loss: 0.2367 | Train Acc: 94.58%                                                     \n",
      "  Epoch  5/5 | Loss: 0.2338 | Train Acc: 94.79%                                                     \n",
      "âœ… Client_3 local training complete (FedPer)\\n\n",
      "âœ“\n",
      "  Training Client 2... \\nðŸ“Š Training Client_2 with FedPer (personalized head)...\n",
      "  Epoch  1/5 | Loss: 1.1358 | Train Acc: 57.19%                                                     \n",
      "  Epoch  2/5 | Loss: 1.0134 | Train Acc: 58.90%                                                     \n",
      "  Epoch 3/5:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 110/370 [00:07<00:18, 13.93it/s, loss=0.9646]"
     ]
    }
   ],
   "source": [
    "!python -u /kaggle/working/FedPer-PyTorch/main.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
