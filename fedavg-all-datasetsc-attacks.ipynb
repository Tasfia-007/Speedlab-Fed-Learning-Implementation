{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Use This Version for Checkpoint Resume in Kaggle\n",
    "\n",
    "Before running this notebook for the second time (to resume training from a checkpoint), follow the steps below carefully.\n",
    "\n",
    "1. Download the latest checkpoint file\n",
    "Download the file named checkpoint_latest.pth from the previous version of your notebook or experiment.\n",
    "\n",
    "2. Upload the checkpoint to Kaggle Input Directory\n",
    "Place the downloaded file inside your Kaggle input path, for example:\n",
    "/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\n",
    "\n",
    "3. Run the following code cell before starting training\n",
    "This code will copy the checkpoint file to the working directory (/kaggle/working/checkpoints) so that training can resume from the saved state.\n",
    "\n",
    "4. Resume Training\n",
    "After the checkpoint file is copied successfully, running the rest of the notebook will automatically start training from the previous checkpoint instead of starting from scratch.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## you can change the dataset and the attack type by simply changing the name in the args.py file.no need to modify anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:54.965264Z",
     "iopub.status.busy": "2025-11-23T15:01:54.964496Z",
     "iopub.status.idle": "2025-11-23T15:01:54.974232Z",
     "shell.execute_reply": "2025-11-23T15:01:54.973382Z",
     "shell.execute_reply.started": "2025-11-23T15:01:54.965237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Source file not found: /kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Source and destination paths\n",
    "src = \"/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\"\n",
    "dst_dir = \"/kaggle/working/checkpoints\"\n",
    "dst = os.path.join(dst_dir, \"checkpoint_latest.pth\")\n",
    "\n",
    "# Step 1: Check if source file exists\n",
    "if not os.path.exists(src):\n",
    "    print(f\"âŒ Source file not found: {src}\")\n",
    "else:\n",
    "    print(f\"âœ… Found source file: {src}\")\n",
    "\n",
    "    # Step 2: Ensure destination directory exists\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "        print(f\"ðŸ“‚ Created destination directory: {dst_dir}\")\n",
    "    else:\n",
    "        print(f\"ðŸ“ Destination directory already exists: {dst_dir}\")\n",
    "\n",
    "    # Step 3: Copy the file\n",
    "    shutil.copy(src, dst)\n",
    "    print(f\"âœ… Copied file to: {dst}\")\n",
    "\n",
    "    # Step 4: List all files in destination\n",
    "    files = os.listdir(dst_dir)\n",
    "    if files:\n",
    "        print(\"\\nðŸ“„ Files in /kaggle/working/checkpoints:\")\n",
    "        for f in files:\n",
    "            print(\" â”œâ”€â”€\", f)\n",
    "    else:\n",
    "        print(\"âš  Destination directory is empty (unexpected).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:54.975417Z",
     "iopub.status.busy": "2025-11-23T15:01:54.975225Z",
     "iopub.status.idle": "2025-11-23T15:01:54.996181Z",
     "shell.execute_reply": "2025-11-23T15:01:54.995594Z",
     "shell.execute_reply.started": "2025-11-23T15:01:54.975402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Already exists: /kaggle/working/FedAvg-PyTorch/main.py\n",
      " Already exists: /kaggle/working/FedAvg-PyTorch/server.py\n",
      " Already exists: /kaggle/working/FedAvg-PyTorch/client.py\n",
      " Already exists: /kaggle/working/FedAvg-PyTorch/model.py\n",
      " Already exists: /kaggle/working/FedAvg-PyTorch/get_data.py\n",
      " Already exists: /kaggle/working/FedAvg-PyTorch/args.py\n",
      "\n",
      " Folder and files ready in: /kaggle/working/FedAvg-PyTorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Target directory\n",
    "base_dir = \"/kaggle/working/FedAvg-PyTorch\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Python files to create\n",
    "files = [\"main.py\", \"server.py\", \"client.py\", \"model.py\", \"get_data.py\", \"args.py\"]\n",
    "\n",
    "# Create each file if not exists\n",
    "for file in files:\n",
    "    file_path = os.path.join(base_dir, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(f\"# {file} â€” auto-created placeholder\\n\")\n",
    "        print(f\" Created: {file_path}\")\n",
    "    else:\n",
    "        print(f\" Already exists: {file_path}\")\n",
    "\n",
    "print(\"\\n Folder and files ready in:\", base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:54.996981Z",
     "iopub.status.busy": "2025-11-23T15:01:54.996763Z",
     "iopub.status.idle": "2025-11-23T15:01:55.123369Z",
     "shell.execute_reply": "2025-11-23T15:01:55.122446Z",
     "shell.execute_reply.started": "2025-11-23T15:01:54.996958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rm -rf /kaggle/working/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.125225Z",
     "iopub.status.busy": "2025-11-23T15:01:55.124924Z",
     "iopub.status.idle": "2025-11-23T15:01:55.132616Z",
     "shell.execute_reply": "2025-11-23T15:01:55.131972Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.125201Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " args.py updated with attack parameters!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedAvg-PyTorch/args.py\"\n",
    "\n",
    "new_code = '''\n",
    "# ========================================\n",
    "# args.py â€“ FedAvg Configuration (with Attacks)\n",
    "# ========================================\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "DATASET_CONFIGS = {\n",
    "    'pathmnist': {\n",
    "        'num_classes': 9,\n",
    "        'class_names': [\n",
    "            \"adipose tissue\", \"background\", \"debris\", \"lymphocytes\",\n",
    "            \"mucus\", \"smooth muscle\", \"normal colon mucosa\",\n",
    "            \"cancer-associated stroma\", \"colorecal adenocarcinoma epithelium\"\n",
    "        ],\n",
    "        'input_channels': 3\n",
    "    },\n",
    "    'tissuemnist': {\n",
    "        'num_classes': 8,\n",
    "        'class_names': [\n",
    "            \"collecting duct\", \"distal convoluted tubule\",\n",
    "            \"glomerular endothelial cells\", \"interstitial endothelial cells\",\n",
    "            \"leukocytes\", \"podocytes\", \"proximal tubule\", \"thick ascending limb\"\n",
    "        ],\n",
    "        'input_channels': 1\n",
    "    },\n",
    "    'organamnist': {\n",
    "        'num_classes': 11,\n",
    "        'class_names': [\n",
    "            \"bladder\", \"femur-left\", \"femur-right\", \"heart\",\n",
    "            \"kidney-left\", \"kidney-right\", \"liver\",\n",
    "            \"lung-left\", \"lung-right\", \"spleen\", \"pelvis\"\n",
    "        ],\n",
    "        'input_channels': 1\n",
    "    },\n",
    "    'octmnist': {\n",
    "        'num_classes': 4,\n",
    "        'class_names': [\n",
    "            \"choroidal neovascularization\", \"diabetic macular edema\",\n",
    "            \"drusen\", \"normal\"\n",
    "        ],\n",
    "        'input_channels': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"FedAvg - Config File\")\n",
    "    \n",
    "    # Dataset\n",
    "    parser.add_argument('--dataset', type=str, default='organamnist', \n",
    "                        choices=['pathmnist', 'tissuemnist', 'organamnist', 'octmnist'],\n",
    "                        help='MedMNIST dataset to use')\n",
    "    \n",
    "    # Federated Learning Parameters\n",
    "    parser.add_argument('--E', type=int, default=5, help='local epochs')\n",
    "    parser.add_argument('--r', type=int, default=50, help='number of communication rounds')\n",
    "    parser.add_argument('--K', type=int, default=5, help='total number of clients')\n",
    "    parser.add_argument('--C', type=float, default=1, help='client sampling rate per round')\n",
    "    parser.add_argument('--B', type=int, default=32, help='batch size')\n",
    "    parser.add_argument('--use_combined', action='store_true')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--clip_model', type=str, default='ViT-B/32')\n",
    "    parser.add_argument('--freeze_clip', action='store_true')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5)\n",
    "\n",
    "    # Optimizer Settings\n",
    "    parser.add_argument('--lr', type=float, default=0.003)\n",
    "    parser.add_argument('--optimizer', type=str, default='sgd')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9)\n",
    "\n",
    "    # Checkpoint Settings\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints_fedavg_organamnist')\n",
    "    parser.add_argument('--device', default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    parser.add_argument('--dominant_ratio', type=float, default=0.7)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Adversarial Attack Parameters\n",
    "    # -------------------------------\n",
    "    parser.add_argument('--attack_type', type=str, default='none', \n",
    "                        choices=['none', 'fgsm', 'pgd', 'cw'],\n",
    "                        help='Type of adversarial attack during testing')\n",
    "    parser.add_argument('--attack_epsilon', type=float, default=0.06,\n",
    "                        help='Epsilon for FGSM/PGD attacks')\n",
    "    parser.add_argument('--pgd_alpha', type=float, default=0.006,\n",
    "                        help='Step size for PGD')\n",
    "    parser.add_argument('--pgd_iters', type=int, default=10,\n",
    "                        help='Number of PGD iterations')\n",
    "    parser.add_argument('--cw_c', type=float, default=1.0,\n",
    "                        help='C parameter for CW attack')\n",
    "    parser.add_argument('--cw_max_iter', type=int, default=100,\n",
    "                        help='Max iterations for CW attack')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    # Auto-configure based on selected dataset\n",
    "    if args.dataset in DATASET_CONFIGS:\n",
    "        config = DATASET_CONFIGS[args.dataset]\n",
    "        args.num_classes = config['num_classes']\n",
    "        args.input_channels = config['input_channels']\n",
    "        args.class_names = config['class_names']\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {args.dataset} not configured!\")\n",
    "    \n",
    "    return args\n",
    "'''\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" args.py updated with attack parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.133610Z",
     "iopub.status.busy": "2025-11-23T15:01:55.133381Z",
     "iopub.status.idle": "2025-11-23T15:01:55.151068Z",
     "shell.execute_reply": "2025-11-23T15:01:55.150462Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.133590Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " get_data.py updated!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedAvg-PyTorch/get_data.py\"\n",
    "\n",
    "new_code = r'''\n",
    "# ========================================\n",
    "# get_data.py â€” True Balanced Non-IID Distribution\n",
    "# Supports: pathmnist, tissuemnist, organamnist, octmnist\n",
    "# ========================================\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from medmnist import INFO\n",
    "from medmnist.dataset import PathMNIST, TissueMNIST, OrganAMNIST, OCTMNIST\n",
    "\n",
    "# Map dataset names to classes\n",
    "DATASET_MAP = {\n",
    "    'pathmnist': PathMNIST,\n",
    "    'tissuemnist': TissueMNIST,\n",
    "    'organamnist': OrganAMNIST,\n",
    "    'octmnist': OCTMNIST\n",
    "}\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Returns train and test transforms (handles grayscale and RGB)\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.Grayscale(num_output_channels=3),  # ensures 3 channels\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, test_transform\n",
    "\n",
    "def balanced_noniid_split(dataset, num_clients, dominant_ratio=0.7):\n",
    "    \"\"\"Balanced Non-IID split: dominant_ratio% to main client, rest distributed\"\"\"\n",
    "    labels = np.array([dataset[i][1].item() for i in range(len(dataset))])\n",
    "    num_classes = len(np.unique(labels))\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "\n",
    "    # Prepare class indices\n",
    "    class_indices = {c: np.where(labels == c)[0].tolist() for c in range(num_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "\n",
    "    # Step 1: assign dominant_ratio% of dominant class to primary client\n",
    "    for client_id in range(num_clients):\n",
    "        dominant_class = client_id % num_classes\n",
    "        n_dominant = int(len(class_indices[dominant_class]) * dominant_ratio)\n",
    "        if n_dominant > 0:\n",
    "            client_indices[client_id].extend(class_indices[dominant_class][:n_dominant])\n",
    "            class_indices[dominant_class] = class_indices[dominant_class][n_dominant:]\n",
    "\n",
    "    # Step 2: distribute remaining samples equally among other clients\n",
    "    for c in range(num_classes):\n",
    "        remaining = class_indices[c]\n",
    "        np.random.shuffle(remaining)\n",
    "        other_clients = [i for i in range(num_clients) if i % num_classes != c]\n",
    "        for i, idx in enumerate(remaining):\n",
    "            client_id = other_clients[i % len(other_clients)]\n",
    "            client_indices[client_id].append(idx)\n",
    "\n",
    "    # Shuffle each client's indices\n",
    "    for i in range(num_clients):\n",
    "        np.random.shuffle(client_indices[i])\n",
    "\n",
    "    return client_indices\n",
    "\n",
    "def load_medmnist_data(args):\n",
    "    \"\"\"Load MedMNIST data - automatically configured based on args.dataset\"\"\"\n",
    "    train_transform, test_transform = get_transforms()\n",
    "    data_root = './data/medmnist'\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "    data_flag = args.dataset.lower()\n",
    "    if data_flag not in DATASET_MAP:\n",
    "        raise ValueError(f\"Dataset {data_flag} not supported. Choose from {list(DATASET_MAP.keys())}\")\n",
    "\n",
    "    n_classes = args.num_classes\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" Loading {data_flag.upper()} Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "    DatasetClass = DATASET_MAP[data_flag]\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = DatasetClass(root=data_root, split='train', download=True, transform=train_transform)\n",
    "    val_dataset = DatasetClass(root=data_root, split='val', download=True, transform=train_transform)\n",
    "    test_dataset = DatasetClass(root=data_root, split='test', download=True, transform=test_transform)\n",
    "\n",
    "    combined_train = ConcatDataset([train_dataset, val_dataset])\n",
    "    print(f\" Using Train+Val: {len(combined_train)} samples for federated learning\")\n",
    "\n",
    "    # Load or create client indices\n",
    "    cache_file = f'./data/medmnist/client_indices_{data_flag}_K{args.K}_dr{args.dominant_ratio}.pkl'\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"\\n Loading cached split from: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            client_indices = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"\\n Creating balanced Non-IID split (dominant_ratio={args.dominant_ratio})...\")\n",
    "        client_indices = balanced_noniid_split(combined_train, args.K, dominant_ratio=args.dominant_ratio)\n",
    "        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(client_indices, f)\n",
    "        print(f\" Split cached to: {cache_file}\")\n",
    "\n",
    "    # Print client-wise class distribution\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" Client-wise Image Distribution\")\n",
    "    print(\"=\"*60)\n",
    "    total_samples = 0\n",
    "    for client_id, indices in enumerate(client_indices):\n",
    "        client_labels = [combined_train[i][1].item() for i in indices]\n",
    "        label_counts = np.bincount(client_labels, minlength=n_classes)\n",
    "        total_client = len(indices)\n",
    "        total_samples += total_client\n",
    "        distribution_str = \", \".join([f\"C{c}:{label_counts[c]}\" for c in range(n_classes)])\n",
    "        dominant_class = np.argmax(label_counts)\n",
    "        dominant_pct = (label_counts[dominant_class] / total_client) * 100\n",
    "        print(f\"Client {client_id:2d}: {total_client:5d} samples | Dominant: Class {dominant_class} ({dominant_pct:.1f}%)\")\n",
    "        print(f\"           [{distribution_str}]\")\n",
    "\n",
    "    print(f\"\\nTotal samples: {total_samples}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    client_loaders = []\n",
    "    for i, indices in enumerate(client_indices):\n",
    "        subset = Subset(combined_train, indices)\n",
    "        loader = DataLoader(subset, batch_size=args.B, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        client_loaders.append(loader)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.B, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return client_loaders, test_loader\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" get_data.py updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.152704Z",
     "iopub.status.busy": "2025-11-23T15:01:55.152472Z",
     "iopub.status.idle": "2025-11-23T15:01:55.173193Z",
     "shell.execute_reply": "2025-11-23T15:01:55.172575Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.152682Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " client.py updated!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedAvg-PyTorch/client.py\"\n",
    "new_code = \"\"\"\n",
    "# ========================================\n",
    "# client.py â€” Client-side helper for FedAvg \n",
    "# ========================================\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, model, train_loader, device, val_loader=None, lr=0.0001, weight_decay=5e-4):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=self.weight_decay)\n",
    "    \n",
    "    def compute_accuracy(self, loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                labels = labels.squeeze()\n",
    "                outputs = self.model(images)\n",
    "                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n",
    "                    outputs = outputs.logits\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        return 100 * correct / total if total > 0 else 0\n",
    "    \n",
    "    def train(self, epochs=1):\n",
    "        print(f\"\\\\n Training {self.model.name} with FedAvg objective...\")\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            pbar = tqdm(self.train_loader,\n",
    "                        desc=f\"  Epoch {epoch+1}/{epochs}\",\n",
    "                        ncols=100,\n",
    "                        leave=False)\n",
    "            \n",
    "            for images, labels in pbar:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n",
    "                    outputs = outputs.logits\n",
    "                \n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            train_acc = self.compute_accuracy(self.train_loader)\n",
    "            log_msg = f\"  Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\"\n",
    "            \n",
    "            if self.val_loader is not None:\n",
    "                val_acc = self.compute_accuracy(self.val_loader)\n",
    "                val_loss = 0\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in self.val_loader:\n",
    "                        images, labels = images.to(self.device), labels.to(self.device)\n",
    "                        labels = labels.squeeze()\n",
    "                        outputs = self.model(images)\n",
    "                        if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n",
    "                            outputs = outputs.logits\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "                val_loss /= len(self.val_loader)\n",
    "                log_msg += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n",
    "            \n",
    "            print(log_msg)\n",
    "        \n",
    "        print(f\" {self.model.name} local training complete (FedAvg)\\\\n\")\n",
    "        return self.model.state_dict()\n",
    "\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" client.py updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.174067Z",
     "iopub.status.busy": "2025-11-23T15:01:55.173830Z",
     "iopub.status.idle": "2025-11-23T15:01:55.194661Z",
     "shell.execute_reply": "2025-11-23T15:01:55.194132Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.174046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " attacks.py created for FedAvg!\n",
      " File path: /kaggle/working/FedAvg-PyTorch/attacks.py\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedAvg-PyTorch/attacks.py\"\n",
    "\n",
    "new_code = \"\"\"\n",
    "# ========================================\n",
    "# attacks.py â€“ Adversarial Attacks for FedAvg\n",
    "# ========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdversarialAttacks:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def fgsm_attack(self, images, labels, epsilon=0.03):\n",
    "        '''FGSM Attack - Fixed version'''\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Clone and enable gradients\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        images.requires_grad = True\n",
    "        \n",
    "        # Forward pass with gradient tracking\n",
    "        outputs = self.model(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        # Compute gradients\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check if gradients exist\n",
    "        if images.grad is None:\n",
    "            print(\"    No gradients computed - attack failed\")\n",
    "            return images.detach()\n",
    "        \n",
    "        # Create perturbation\n",
    "        perturbation = epsilon * images.grad.sign()\n",
    "        \n",
    "        # Create adversarial images\n",
    "        adv_images = images + perturbation\n",
    "        adv_images = torch.clamp(adv_images, -1, 1)\n",
    "        \n",
    "        return adv_images.detach()\n",
    "    \n",
    "    def pgd_attack(self, images, labels, epsilon=0.03, alpha=0.007, iters=10):\n",
    "        '''PGD Attack - Fixed version'''\n",
    "        self.model.eval()\n",
    "        \n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        \n",
    "        # Initialize with random perturbation\n",
    "        adv_images = images.clone()\n",
    "        adv_images = adv_images + torch.empty_like(adv_images).uniform_(-epsilon, epsilon)\n",
    "        adv_images = torch.clamp(adv_images, -1, 1)\n",
    "        \n",
    "        for i in range(iters):\n",
    "            adv_images.requires_grad = True\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(adv_images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Compute gradients\n",
    "            self.model.zero_grad()\n",
    "            if adv_images.grad is not None:\n",
    "                adv_images.grad.zero_()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Check gradients\n",
    "            if adv_images.grad is None:\n",
    "                print(f\"    No gradients at iteration {i+1}\")\n",
    "                break\n",
    "            \n",
    "            # Update with gradient ascent\n",
    "            with torch.no_grad():\n",
    "                adv_images = adv_images.detach() + alpha * adv_images.grad.sign()\n",
    "                perturbation = torch.clamp(adv_images - images, -epsilon, epsilon)\n",
    "                adv_images = images + perturbation\n",
    "                adv_images = torch.clamp(adv_images, -1, 1)\n",
    "        \n",
    "        return adv_images.detach()\n",
    "    \n",
    "    def cw_attack(self, images, labels, c=1.0, kappa=0, max_iter=100, learning_rate=0.01):\n",
    "        '''Carlini-Wagner (CW) Attack'''\n",
    "        self.model.eval()\n",
    "        \n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Use tanh space for better optimization\n",
    "        w = torch.zeros_like(images, requires_grad=True, device=self.device)\n",
    "        optimizer = torch.optim.Adam([w], lr=learning_rate)\n",
    "        \n",
    "        best_adv = images.clone()\n",
    "        best_loss = float('inf') * torch.ones(batch_size, device=self.device)\n",
    "        \n",
    "        for step in range(max_iter):\n",
    "            # Transform w to valid image range\n",
    "            adv_images = torch.tanh(w) * 1.0\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(adv_images)\n",
    "            \n",
    "            # CW loss formulation\n",
    "            real = outputs.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Get second highest logit\n",
    "            other = outputs.clone()\n",
    "            other.scatter_(1, labels.unsqueeze(1), -float('inf'))\n",
    "            other_max = other.max(1)[0]\n",
    "            \n",
    "            # Loss: want other_max > real (misclassification)\n",
    "            f_loss = torch.clamp(real - other_max + kappa, min=0)\n",
    "            \n",
    "            # L2 distance loss\n",
    "            l2_dist = torch.sum((adv_images - images) ** 2, dim=[1, 2, 3])\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = (c * f_loss + l2_dist).sum()\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track best adversarial examples\n",
    "            with torch.no_grad():\n",
    "                pred = outputs.argmax(1)\n",
    "                successful = (pred != labels)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    if successful[i] and l2_dist[i] < best_loss[i]:\n",
    "                        best_loss[i] = l2_dist[i]\n",
    "                        best_adv[i] = adv_images[i]\n",
    "        \n",
    "        return best_adv.detach()\n",
    "\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" attacks.py created for FedAvg!\")\n",
    "print(f\" File path: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.195580Z",
     "iopub.status.busy": "2025-11-23T15:01:55.195291Z",
     "iopub.status.idle": "2025-11-23T15:01:55.214024Z",
     "shell.execute_reply": "2025-11-23T15:01:55.213293Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.195564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model.py updated!\n",
      " File saved at: /kaggle/working/FedAvg-PyTorch/model.py\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedAvg-PyTorch/model.py\"\n",
    "new_code = '''\n",
    "# ========================================\n",
    "# model.py â€” CLIP-based Classifier for MedMNIST (Auto-configured)\n",
    "# ========================================\n",
    "import torch\n",
    "from torch import nn\n",
    "import clip\n",
    "\n",
    "class CLIPMedMNISTClassifier(nn.Module):\n",
    "    def __init__(self, args, name='clip_model'):\n",
    "        super(CLIPMedMNISTClassifier, self).__init__()\n",
    "        self.name = name\n",
    "        \n",
    "        # Load pretrained CLIP\n",
    "        self.clip_model, self.preprocess = clip.load(\"ViT-B/32\", device=args.device)\n",
    "        \n",
    "        # Get class names from args (auto-configured based on dataset)\n",
    "        self.class_names = args.class_names\n",
    "        self.num_classes = args.num_classes\n",
    "        \n",
    "        # Freeze text encoder completely\n",
    "        for param in self.clip_model.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.clip_model.token_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.clip_model.ln_final.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.clip_model.positional_embedding.requires_grad = False\n",
    "        self.clip_model.text_projection.requires_grad = False\n",
    "        \n",
    "        # Precompute and freeze text embeddings\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize([f\"a microscopic image of {c}\" for c in self.class_names]).to(args.device)\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Register as buffer\n",
    "        self.register_buffer('text_features', text_features)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        image_features = self.clip_model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100.0 * image_features @ self.text_features.T\n",
    "        return logits\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "    \n",
    "    def freeze_layers(self, num_layers_to_freeze):\n",
    "        if num_layers_to_freeze > 0:\n",
    "            for i, layer in enumerate(self.clip_model.visual.transformer.resblocks):\n",
    "                if i < num_layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "'''\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" model.py updated!\")\n",
    "print(f\" File saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.215142Z",
     "iopub.status.busy": "2025-11-23T15:01:55.214899Z",
     "iopub.status.idle": "2025-11-23T15:01:55.234592Z",
     "shell.execute_reply": "2025-11-23T15:01:55.234065Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.215118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " server.py updated with attack testing!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedAvg-PyTorch/server.py\"\n",
    "\n",
    "new_code = \"\"\"\n",
    "# ========================================\n",
    "# server.py â€“ FedAvg with Attack Testing\n",
    "# ========================================\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
    "from model import CLIPMedMNISTClassifier as ImageClassifier\n",
    "from get_data import load_medmnist_data\n",
    "from client import Client\n",
    "from attacks import AdversarialAttacks\n",
    "\n",
    "class FedAvgServer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "        self.current_round = 0\n",
    "        self.best_global_acc = 0\n",
    "        self.history = {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []}\n",
    "        self.global_model = ImageClassifier(args, name=\"server\").to(args.device)\n",
    "        self.client_models = []\n",
    "        for i in range(self.args.K):\n",
    "            model = copy.deepcopy(self.global_model)\n",
    "            model.name = f\"Client_{i}\"\n",
    "            self.client_models.append(model)\n",
    "        self.client_loaders, self.test_loader = load_medmnist_data(args)\n",
    "\n",
    "        latest_ckpt = os.path.join(args.checkpoint_dir, \"checkpoint_latest.pth\")\n",
    "        if os.path.exists(latest_ckpt):\n",
    "            self.load_checkpoint(latest_ckpt)\n",
    "            print(f\"\\\\n Resuming training from Round {self.current_round}\")\n",
    "        else:\n",
    "            print(\"\\\\n Starting new training session\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        print(f\"\\\\n Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.args.device, weights_only=False)\n",
    "        self.global_model.load_state_dict(checkpoint['server_state_dict'])\n",
    "        if 'client_state_dicts' in checkpoint:\n",
    "            for i, state_dict in enumerate(checkpoint['client_state_dicts']):\n",
    "                self.client_models[i].load_state_dict(state_dict)\n",
    "        self.current_round = checkpoint.get('round', 0)\n",
    "        self.best_global_acc = checkpoint.get('best_global_acc', 0)\n",
    "        self.history = checkpoint.get('history', {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []})\n",
    "\n",
    "    def dispatch(self, selected_clients):\n",
    "        for idx in selected_clients:\n",
    "            client_model = self.client_models[idx]\n",
    "            for client_param, global_param in zip(client_model.parameters(), self.global_model.parameters()):\n",
    "                client_param.data = global_param.data.clone()\n",
    "\n",
    "    def aggregate(self, selected_clients):\n",
    "        total_samples = sum([len(self.client_loaders[idx].dataset) for idx in selected_clients])\n",
    "        global_params = {k: torch.zeros_like(v.data) for k, v in self.global_model.named_parameters()}\n",
    "        for idx in selected_clients:\n",
    "            weight = len(self.client_loaders[idx].dataset) / total_samples\n",
    "            client_params = dict(self.client_models[idx].named_parameters())\n",
    "            for k in global_params.keys():\n",
    "                global_params[k] += client_params[k].data * weight\n",
    "        for k, v in self.global_model.named_parameters():\n",
    "            v.data = global_params[k].data.clone()\n",
    "\n",
    "    def client_update(self, idx):\n",
    "        client_model = self.client_models[idx]\n",
    "        client_loader = self.client_loaders[idx]\n",
    "        client_obj = Client(model=client_model,\n",
    "                            train_loader=client_loader,\n",
    "                            device=self.args.device,\n",
    "                            lr=self.args.lr,\n",
    "                            weight_decay=self.args.weight_decay)\n",
    "        client_obj.train(epochs=self.args.E)\n",
    "\n",
    "    def test_global_model(self):\n",
    "        '''Test global model with optional adversarial attacks'''\n",
    "        self.global_model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        # Initialize attacker if needed\n",
    "        attacker = None\n",
    "        if self.args.attack_type != 'none':\n",
    "            attacker = AdversarialAttacks(self.global_model, self.args.device)\n",
    "            print(f\"\\\\n  Testing with {self.args.attack_type.upper()} attack (Îµ={self.args.attack_epsilon})\")\n",
    "\n",
    "        context_manager = torch.no_grad() if self.args.attack_type == 'none' else torch.enable_grad()\n",
    "\n",
    "        with context_manager:\n",
    "            for images, labels in tqdm(self.test_loader, desc=\"  Testing\", leave=False, ncols=100):\n",
    "                #  Store ORIGINAL clean images\n",
    "                original_images = images.clone()\n",
    "                \n",
    "                images, labels = images.to(self.args.device), labels.to(self.args.device)\n",
    "                labels = labels.squeeze()\n",
    "\n",
    "                #  Apply attack on ORIGINAL images\n",
    "                if self.args.attack_type == 'fgsm':\n",
    "                    images = attacker.fgsm_attack(original_images.to(self.args.device), labels,\n",
    "                                                   self.args.attack_epsilon)\n",
    "                elif self.args.attack_type == 'pgd':\n",
    "                    images = attacker.pgd_attack(original_images.to(self.args.device), labels,\n",
    "                                                  epsilon=self.args.attack_epsilon,\n",
    "                                                  alpha=self.args.pgd_alpha,\n",
    "                                                  iters=self.args.pgd_iters)\n",
    "                elif self.args.attack_type == 'cw':\n",
    "                    images = attacker.cw_attack(original_images.to(self.args.device), labels,\n",
    "                                                 c=self.args.cw_c,\n",
    "                                                 max_iter=self.args.cw_max_iter)\n",
    "\n",
    "                # Get predictions\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.global_model(images)\n",
    "\n",
    "                    if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n",
    "                        if hasattr(outputs, 'logits'):\n",
    "                            outputs = outputs.logits\n",
    "                        else:\n",
    "                            outputs = outputs[0]\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "\n",
    "        acc = 100.0 * np.mean(all_labels == all_preds)\n",
    "        try:\n",
    "            precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "            recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "            f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        except Exception:\n",
    "            precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "\n",
    "        rmse = float(np.sqrt(mean_squared_error(all_labels, all_preds)))\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'rmse': rmse\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def save_checkpoint(self, round_num, metrics):\n",
    "        checkpoint = {\n",
    "            'round': round_num,\n",
    "            'server_state_dict': self.global_model.state_dict(),\n",
    "            'client_state_dicts': [model.state_dict() for model in self.client_models],\n",
    "            'best_global_acc': self.best_global_acc,\n",
    "            'history': self.history,\n",
    "            'args': vars(self.args)\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, os.path.join(self.args.checkpoint_dir, 'checkpoint_latest.pth'))\n",
    "        \n",
    "        with open(os.path.join(self.args.checkpoint_dir, 'training_history.json'), 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "    def run(self):\n",
    "        start_round = self.current_round\n",
    "        for r in range(start_round, self.args.r):\n",
    "            print(f\"\\\\n{'='*60}\")\n",
    "            print(f\" Round [{r+1}/{self.args.r}]\")\n",
    "            print('='*60)\n",
    "            \n",
    "            m = max(int(self.args.C * self.args.K), 1)\n",
    "            selected_clients = random.sample(range(self.args.K), m)\n",
    "            print(f\" Selected Clients: {selected_clients}\")\n",
    "            \n",
    "            self.dispatch(selected_clients)\n",
    "            \n",
    "            for idx in selected_clients:\n",
    "                print(f\"\\\\n Training Client {idx}...\")\n",
    "                self.client_update(idx)\n",
    "            \n",
    "            self.aggregate(selected_clients)\n",
    "            \n",
    "            metrics = self.test_global_model()\n",
    "            avg_acc = metrics['accuracy']\n",
    "            \n",
    "            self.history['rounds'].append(r+1)\n",
    "            self.history['avg_accuracy'].append(avg_acc)\n",
    "            self.history['precision'] = self.history.get('precision', []) + [metrics['precision']]\n",
    "            self.history['recall'] = self.history.get('recall', []) + [metrics['recall']]\n",
    "            self.history['f1'] = self.history.get('f1', []) + [metrics['f1']]\n",
    "            self.history['rmse'] = self.history.get('rmse', []) + [metrics['rmse']]\n",
    "            \n",
    "            if avg_acc > self.best_global_acc:\n",
    "                self.best_global_acc = avg_acc\n",
    "            \n",
    "            self.current_round = r + 1\n",
    "            self.save_checkpoint(r+1, metrics)\n",
    "            \n",
    "            print(f\"\\\\n{'='*60}\")\n",
    "            print(f\" Round [{r+1}/{self.args.r}] Results:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"   Accuracy:  {metrics['accuracy']:.2f}%\")\n",
    "            print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"   Recall:    {metrics['recall']:.4f}\")\n",
    "            print(f\"   F1-Score:  {metrics['f1']:.4f}\")\n",
    "            print(f\"   RMSE:      {metrics['rmse']:.4f}\")\n",
    "            print(f\"   Best Acc:  {self.best_global_acc:.2f}%\")\n",
    "            print('='*60)\n",
    "            \n",
    "        return self.global_model\n",
    "\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\" server.py updated with attack testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.278635Z",
     "iopub.status.busy": "2025-11-23T15:01:55.278465Z",
     "iopub.status.idle": "2025-11-23T15:01:55.285835Z",
     "shell.execute_reply": "2025-11-23T15:01:55.285051Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.278623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… main.py updated with attack testing!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/FedAvg-PyTorch/main.py\"\n",
    "\n",
    "new_code = \"\"\"\n",
    "# ========================================\n",
    "# main.py â€“ Run FedAvg with Attack Testing\n",
    "# ========================================\n",
    "from args import args_parser\n",
    "from server import FedAvgServer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['rounds'], history['avg_accuracy'], 'b-', label='Avg Accuracy', linewidth=2)\n",
    "    plt.plot(history['rounds'], history['best_accuracy'], 'r--', label='Best Accuracy', linewidth=2)\n",
    "    plt.xlabel('Communication Round')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Federated Learning Accuracy Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if len(history['avg_accuracy']) > 1:\n",
    "        improvements = [history['avg_accuracy'][i] - history['avg_accuracy'][i-1] \n",
    "                        for i in range(1, len(history['avg_accuracy']))]\n",
    "        plt.bar(history['rounds'][1:], improvements, alpha=0.7)\n",
    "        plt.axhline(y=0, color='r', linestyle='-', linewidth=0.5)\n",
    "        plt.xlabel('Communication Round')\n",
    "        plt.ylabel('Accuracy Change (%)')\n",
    "        plt.title('Round-to-Round Accuracy Change')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f' Training plot saved at: {save_path}')\n",
    "\n",
    "def test_with_all_attacks(server, args):\n",
    "    '''Test model with all attack types'''\n",
    "    attacks = ['none', 'fgsm', 'pgd', 'cw']\n",
    "    results = {}\n",
    "    \n",
    "    print('\\\\n' + '='*60)\n",
    "    print('  ADVERSARIAL ROBUSTNESS TESTING')\n",
    "    print('='*60)\n",
    "    \n",
    "    for attack in attacks:\n",
    "        original_attack = args.attack_type\n",
    "        args.attack_type = attack\n",
    "        \n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\" Testing with {attack.upper()} attack\")\n",
    "        print('='*60)\n",
    "        \n",
    "        metrics = server.test_global_model()\n",
    "        results[attack] = metrics\n",
    "        \n",
    "        print(f\"\\\\n Results:\")\n",
    "        print(f\"   Accuracy:  {metrics['accuracy']:.2f}%\")\n",
    "        print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {metrics['f1']:.4f}\")\n",
    "        print(f\"   RMSE:      {metrics['rmse']:.4f}\")\n",
    "    \n",
    "    args.attack_type = original_attack\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(args.checkpoint_dir, 'attack_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"\\\\n Attack results saved at: {results_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    args = args_parser()\n",
    "    \n",
    "    print('\\\\n' + '='*60)\n",
    "    print(' FEDAVG CONFIGURATION')\n",
    "    print('='*60)\n",
    "    print(f'Dataset: {args.dataset.upper()}')\n",
    "    print(f'Classes: {args.num_classes} | Clients: {args.K} | Rounds: {args.r}')\n",
    "    print(f'Batch: {args.B} | LR: {args.lr} | Device: {args.device}')\n",
    "    print('='*60)\n",
    "    \n",
    "    server = FedAvgServer(args)\n",
    "    \n",
    "    # Check if training complete\n",
    "    if server.current_round == args.r:\n",
    "        print('\\\\n Training already complete!')\n",
    "        print('  Running adversarial robustness testing...')\n",
    "        attack_results = test_with_all_attacks(server, args)\n",
    "        \n",
    "        # Print comparison table\n",
    "        print('\\\\n' + '='*60)\n",
    "        print(' ATTACK RESULTS COMPARISON')\n",
    "        print('='*60)\n",
    "        print(f\"{'Attack':<15} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1'}\") \n",
    "        print('-'*60)\n",
    "        for attack, metrics in attack_results.items():\n",
    "            print(f\"{attack.upper():<15} {metrics['accuracy']:>6.2f}%    {metrics['precision']:>7.4f}    {metrics['recall']:>7.4f}    {metrics['f1']:>7.4f}\")\n",
    "        print('='*60)\n",
    "    else:\n",
    "        # Run training\n",
    "        final_model = server.run()\n",
    "        \n",
    "        # Save final model\n",
    "        final_model_path = os.path.join(args.checkpoint_dir, 'final_global_model.pth')\n",
    "        torch.save(final_model.state_dict(), final_model_path)\n",
    "        print(f'\\\\n Final model saved: {final_model_path}')\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_path = os.path.join(args.checkpoint_dir, 'training_plot.png')\n",
    "        plot_training_history(server.history, plot_path)\n",
    "        \n",
    "        # Print summary\n",
    "        print('\\\\n' + '='*60)\n",
    "        print(' TRAINING SUMMARY')\n",
    "        print('='*60)\n",
    "        print(f'Best Accuracy: {server.best_global_acc:.2f}%')\n",
    "        print(f'Final Accuracy: {server.history[\\\"avg_accuracy\\\"][-1]:.2f}%')\n",
    "        print('='*60)\n",
    "        \n",
    "        # Test with attacks\n",
    "        print('\\\\n  Testing adversarial robustness...')\n",
    "        attack_results = test_with_all_attacks(server, args)\n",
    "        \n",
    "        # Print comparison\n",
    "        print('\\\\n' + '='*60)\n",
    "        print(' ATTACK RESULTS COMPARISON')\n",
    "        print('='*60)\n",
    "        print(f\"{'Attack':<15} {'Accuracy':<10} {'Drop'}\") \n",
    "        print('-'*60)\n",
    "        clean_acc = attack_results['none']['accuracy']\n",
    "        for attack, metrics in attack_results.items():\n",
    "            drop = clean_acc - metrics['accuracy'] if attack != 'none' else 0\n",
    "            print(f\"{attack.upper():<15} {metrics['accuracy']:>6.2f}%    {drop:>5.2f}%\")\n",
    "        print('='*60)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\"âœ… main.py updated with attack testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:01:55.287507Z",
     "iopub.status.busy": "2025-11-23T15:01:55.287115Z",
     "iopub.status.idle": "2025-11-23T15:03:08.779196Z",
     "shell.execute_reply": "2025-11-23T15:03:08.778471Z",
     "shell.execute_reply.started": "2025-11-23T15:01:55.287492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install medmnist --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:03:08.780287Z",
     "iopub.status.busy": "2025-11-23T15:03:08.780094Z",
     "iopub.status.idle": "2025-11-23T15:03:19.119265Z",
     "shell.execute_reply": "2025-11-23T15:03:19.118559Z",
     "shell.execute_reply.started": "2025-11-23T15:03:08.780267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-v08abj7f\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-v08abj7f\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=e5b7cd1f16a5f72d1311baee79673bb6e131b57bfbd33b18a916d5ccdf0818ed\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-m3ua_wz1/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:03:19.120483Z",
     "iopub.status.busy": "2025-11-23T15:03:19.120262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " FEDAVG CONFIGURATION\n",
      "============================================================\n",
      "Dataset: ORGANAMNIST\n",
      "Classes: 11 | Clients: 5 | Rounds: 1\n",
      "Batch: 32 | LR: 0.003 | Device: cuda\n",
      "============================================================\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:03<00:00, 102MiB/s]\n",
      "\n",
      "============================================================\n",
      " Loading ORGANAMNIST Dataset\n",
      "============================================================\n",
      "Number of classes: 11\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.2M/38.2M [00:34<00:00, 1.11MB/s]\n",
      " Using Train+Val: 41052 samples for federated learning\n",
      "\n",
      " Creating balanced Non-IID split (dominant_ratio=0.7)...\n",
      " Split cached to: ./data/medmnist/client_indices_organamnist_K5_dr0.7.pkl\n",
      "\n",
      "============================================================\n",
      " Client-wise Image Distribution\n",
      "============================================================\n",
      "Client  0:  8151 samples | Dominant: Class 0 (19.5%)\n",
      "           [C0:1593, C1:122, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\n",
      "Client  1:  7743 samples | Dominant: Class 6 (18.6%)\n",
      "           [C0:171, C1:1136, C2:119, C3:140, C4:340, C5:891, C6:1440, C7:991, C8:988, C9:712, C10:815]\n",
      "Client  2:  7714 samples | Dominant: Class 6 (18.7%)\n",
      "           [C0:171, C1:122, C2:1107, C3:140, C4:340, C5:891, C6:1439, C7:990, C8:988, C9:712, C10:814]\n",
      "Client  3:  7891 samples | Dominant: Class 6 (18.2%)\n",
      "           [C0:171, C1:122, C2:119, C3:1306, C4:340, C5:891, C6:1439, C7:990, C8:987, C9:712, C10:814]\n",
      "Client  4:  9553 samples | Dominant: Class 4 (33.2%)\n",
      "           [C0:171, C1:121, C2:118, C3:140, C4:3171, C5:890, C6:1439, C7:990, C8:987, C9:712, C10:814]\n",
      "\n",
      "Total samples: 41052\n",
      "============================================================\n",
      "\n",
      "\n",
      " Starting new training session\n",
      "\n",
      "============================================================\n",
      " Round [1/1]\n",
      "============================================================\n",
      " Selected Clients: [3, 0, 2, 4, 1]\n",
      "\n",
      " Training Client 3...\n",
      "\n",
      " Training Client_3 with FedAvg objective...\n",
      "  Epoch  1/5 | Loss: 2.1427 | Train Acc: 28.64%                                                     \n",
      "  Epoch  2/5 | Loss: 1.7920 | Train Acc: 39.48%                                                     \n",
      "  Epoch  3/5 | Loss: 1.6313 | Train Acc: 42.54%                                                     \n",
      "  Epoch  4/5 | Loss: 1.5490 | Train Acc: 47.65%                                                     \n",
      "  Epoch  5/5 | Loss: 1.4575 | Train Acc: 48.88%                                                     \n",
      " Client_3 local training complete (FedAvg)\n",
      "\n",
      "\n",
      " Training Client 0...\n",
      "\n",
      " Training Client_0 with FedAvg objective...\n",
      "  Epoch  1/5 | Loss: 1.8698 | Train Acc: 46.37%                                                     \n",
      "  Epoch  2/5 | Loss: 1.4023 | Train Acc: 49.33%                                                     \n",
      "  Epoch  3/5 | Loss: 1.2695 | Train Acc: 56.83%                                                     \n",
      "  Epoch  4/5 | Loss: 1.0508 | Train Acc: 60.07%                                                     \n",
      "  Epoch  5/5 | Loss: 0.9361 | Train Acc: 73.59%                                                     \n",
      " Client_0 local training complete (FedAvg)\n",
      "\n",
      "\n",
      " Training Client 2...\n",
      "\n",
      " Training Client_2 with FedAvg objective...\n",
      "  Epoch  1/5 | Loss: 2.1240 | Train Acc: 32.69%                                                     \n",
      "  Epoch  2/5 | Loss: 1.7445 | Train Acc: 38.51%                                                     \n",
      "  Epoch  3/5 | Loss: 1.7045 | Train Acc: 39.56%                                                     \n",
      "  Epoch  4/5 | Loss: 1.7104 | Train Acc: 38.35%                                                     \n",
      "  Epoch  5/5 | Loss: 1.6884 | Train Acc: 28.35%                                                     \n",
      " Client_2 local training complete (FedAvg)\n",
      "\n",
      "\n",
      " Training Client 4...\n",
      "\n",
      " Training Client_4 with FedAvg objective...\n",
      "  Epoch  1/5 | Loss: 2.2018 | Train Acc: 34.98%                                                     \n",
      "  Epoch  2/5 | Loss: 1.7323 | Train Acc: 35.70%                                                     \n",
      "  Epoch  3/5 | Loss: 1.7042 | Train Acc: 40.29%                                                     \n",
      "  Epoch  4/5 | Loss: 1.6072 | Train Acc: 43.07%                                                     \n",
      "  Epoch  5/5 | Loss: 1.4504 | Train Acc: 52.25%                                                     \n",
      " Client_4 local training complete (FedAvg)\n",
      "\n",
      "\n",
      " Training Client 1...\n",
      "\n",
      " Training Client_1 with FedAvg objective...\n",
      "  Epoch  1/5 | Loss: 2.1780 | Train Acc: 39.52%                                                     \n",
      "  Epoch  2/5 | Loss: 1.7351 | Train Acc: 37.75%                                                     \n",
      "  Epoch  3/5 | Loss: 1.7012 | Train Acc: 37.80%                                                     \n",
      "  Epoch  4/5 | Loss: 1.6807 | Train Acc: 40.85%                                                     \n",
      "  Epoch  5/5 | Loss: 1.6572 | Train Acc: 39.29%                                                     \n",
      " Client_1 local training complete (FedAvg)\n",
      "\n",
      "                                                                                                    \n",
      "============================================================\n",
      " Round [1/1] Results:\n",
      "============================================================\n",
      "   Accuracy:  11.13%\n",
      "   Precision: 0.0210\n",
      "   Recall:    0.0917\n",
      "   F1-Score:  0.0216\n",
      "   RMSE:      2.9273\n",
      "   Best Acc:  11.13%\n",
      "============================================================\n",
      "\n",
      " Final model saved: ./checkpoints/final_global_model.pth\n",
      " Training plot saved at: ./checkpoints/training_plot.png\n",
      "\n",
      "============================================================\n",
      " TRAINING SUMMARY\n",
      "============================================================\n",
      "Best Accuracy: 11.13%\n",
      "Final Accuracy: 11.13%\n",
      "============================================================\n",
      "\n",
      "  Testing adversarial robustness...\n",
      "\n",
      "============================================================\n",
      "  ADVERSARIAL ROBUSTNESS TESTING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      " Testing with NONE attack\n",
      "============================================================\n",
      "                                                                                                    \n",
      " Results:\n",
      "   Accuracy:  11.13%\n",
      "   Precision: 0.0210\n",
      "   Recall:    0.0917\n",
      "   F1-Score:  0.0216\n",
      "   RMSE:      2.9273\n",
      "\n",
      "============================================================\n",
      " Testing with FGSM attack\n",
      "============================================================\n",
      "\n",
      "  Testing with FGSM attack (Îµ=0.06)\n",
      "                                                                                                    \n",
      " Results:\n",
      "   Accuracy:  10.81%\n",
      "   Precision: 0.0129\n",
      "   Recall:    0.0890\n",
      "   F1-Score:  0.0190\n",
      "   RMSE:      2.9404\n",
      "\n",
      "============================================================\n",
      " Testing with PGD attack\n",
      "============================================================\n",
      "\n",
      "  Testing with PGD attack (Îµ=0.06)\n",
      "  Testing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 259/556 [03:45<04:18,  1.15it/s]"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/FedAvg-PyTorch/main.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
