{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"How to Use This Version for Checkpoint Resume in Kaggle\n\nBefore running this notebook for the second time (to resume training from a checkpoint), follow the steps below carefully.\n\n1. Download the latest checkpoint file\nDownload the file named checkpoint_latest.pth from the previous version of your notebook or experiment.\n\n2. Upload the checkpoint to Kaggle Input Directory\nPlace the downloaded file inside your Kaggle input path, for example:\n/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\n\n3. Run the following code cell before starting training\nThis code will copy the checkpoint file to the working directory (/kaggle/working/checkpoints) so that training can resume from the saved state.\n\n4. Resume Training\nAfter the checkpoint file is copied successfully, running the rest of the notebook will automatically start training from the previous checkpoint instead of starting from scratch.\n\n\n\n## you can change the dataset and the attack type by simply changing the name in the args.py file.no need to modify anything else.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Source and destination paths\nsrc = \"/kaggle/input/path1/pytorch/default/1/checkpoint_latest.pth\"\ndst_dir = \"/kaggle/working/checkpoints\"\ndst = os.path.join(dst_dir, \"checkpoint_latest.pth\")\n\n# Step 1: Check if source file exists\nif not os.path.exists(src):\n    print(f\"âŒ Source file not found: {src}\")\nelse:\n    print(f\"âœ… Found source file: {src}\")\n\n    # Step 2: Ensure destination directory exists\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(f\"ðŸ“‚ Created destination directory: {dst_dir}\")\n    else:\n        print(f\"ðŸ“ Destination directory already exists: {dst_dir}\")\n\n    # Step 3: Copy the file\n    shutil.copy(src, dst)\n    print(f\"âœ… Copied file to: {dst}\")\n\n    # Step 4: List all files in destination\n    files = os.listdir(dst_dir)\n    if files:\n        print(\"\\nðŸ“„ Files in /kaggle/working/checkpoints:\")\n        for f in files:\n            print(\" â”œâ”€â”€\", f)\n    else:\n        print(\"âš  Destination directory is empty (unexpected).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Target directory\nbase_dir = \"/kaggle/working/FedProx-PyTorch\"\nos.makedirs(base_dir, exist_ok=True)\n\n# Python files to create\nfiles = [\"main.py\", \"server.py\", \"client.py\", \"model.py\", \"get_data.py\", \"args.py\"]\n\n# Create each file if not exists\nfor file in files:\n    file_path = os.path.join(base_dir, file)\n    if not os.path.exists(file_path):\n        with open(file_path, \"w\") as f:\n            f.write(f\"# {file} â€” auto-created placeholder\\n\")\n        print(f\" Created: {file_path}\")\n    else:\n        print(f\" Already exists: {file_path}\")\n\nprint(\"\\n Folder and files ready in:\", base_dir)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-11T02:36:10.632389Z","iopub.execute_input":"2025-11-11T02:36:10.632929Z","iopub.status.idle":"2025-11-11T02:36:10.642787Z","shell.execute_reply.started":"2025-11-11T02:36:10.632903Z","shell.execute_reply":"2025-11-11T02:36:10.642075Z"},"trusted":true},"outputs":[{"name":"stdout","text":" Created: /kaggle/working/FedProx-PyTorch/main.py\n Created: /kaggle/working/FedProx-PyTorch/server.py\n Created: /kaggle/working/FedProx-PyTorch/client.py\n Created: /kaggle/working/FedProx-PyTorch/model.py\n Created: /kaggle/working/FedProx-PyTorch/get_data.py\n Created: /kaggle/working/FedProx-PyTorch/args.py\n\n Folder and files ready in: /kaggle/working/FedProx-PyTorch\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedProx-PyTorch/args.py\"\n\nnew_code = '''\n# ========================================\n# args.py â€” FedProx Configuration with Dataset Auto-config\n# ========================================\nimport argparse\nimport torch\n\n# Dataset configurations\nDATASET_CONFIGS = {\n    'pathmnist': {\n        'num_classes': 9,\n        'class_names': [\n            \"adipose tissue\", \"background\", \"debris\", \"lymphocytes\",\n            \"mucus\", \"smooth muscle\", \"normal colon mucosa\",\n            \"cancer-associated stroma\", \"colorectal adenocarcinoma epithelium\"\n        ],\n        'input_channels': 3\n    },\n    'tissuemnist': {\n        'num_classes': 8,\n        'class_names': [\n            \"collecting duct\", \"distal convoluted tubule\",\n            \"glomerular endothelial cells\", \"interstitial endothelial cells\",\n            \"leukocytes\", \"podocytes\", \"proximal tubule\", \"thick ascending limb\"\n        ],\n        'input_channels': 1\n    },\n    'organamnist': {\n        'num_classes': 11,\n        'class_names': [\n            \"bladder\", \"femur-left\", \"femur-right\", \"heart\",\n            \"kidney-left\", \"kidney-right\", \"liver\",\n            \"lung-left\", \"lung-right\", \"spleen\", \"pelvis\"\n        ],\n        'input_channels': 1\n    },\n    'octmnist': {\n        'num_classes': 4,\n        'class_names': [\n            \"choroidal neovascularization\", \"diabetic macular edema\",\n            \"drusen\", \"normal\"\n        ],\n        'input_channels': 1\n    }\n}\n\ndef args_parser():\n    parser = argparse.ArgumentParser(description=\"FedProx - Config File\")\n    \n    # -------------------------------\n    # Dataset Selection\n    # -------------------------------\n    parser.add_argument('--dataset', type=str, default='organamnist',\n                        choices=list(DATASET_CONFIGS.keys()),\n                        help='Select MedMNIST dataset')\n    \n    # -------------------------------\n    # Federated Learning Parameters\n    # -------------------------------\n    parser.add_argument('--E', type=int, default=5, help='local epochs')\n    parser.add_argument('--r', type=int, default=50, help='communication rounds')\n    parser.add_argument('--K', type=int, default=5, help='total number of clients')\n    parser.add_argument('--C', type=float, default=1, help='client sampling rate per round')\n    parser.add_argument('--B', type=int, default=32, help='batch size')\n    parser.add_argument('--use_combined', action='store_true', help='Use train+val+test combined')\n\n    # -------------------------------\n    # Model parameters\n    # -------------------------------\n    parser.add_argument('--clip_model', type=str, default='ViT-B/32', help='CLIP model variant')\n    parser.add_argument('--freeze_clip', action='store_true', help='Freeze CLIP backbone')\n    parser.add_argument('--dropout', type=float, default=0.5, help='dropout rate')\n\n    # -------------------------------\n    # Optimizer Settings\n    # -------------------------------\n    parser.add_argument('--lr', type=float, default=0.003, help='learning rate')\n    parser.add_argument('--optimizer', type=str, default='sgd', help='optimizer')\n    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9, help='momentum for SGD')\n\n    # -------------------------------\n    # FedProx Specific Parameter\n    # -------------------------------\n    parser.add_argument('--mu', type=float, default=0.05, help='FedProx proximal term coefficient')\n\n    # -------------------------------\n    # Checkpoint Settings\n    # -------------------------------\n    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='directory for checkpoints')\n    \n    # -------------------------------\n    # Device\n    # -------------------------------\n    parser.add_argument('--device', default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), help='cuda or cpu')\n\n    # -------------------------------\n    # Balanced Non-IID\n    # -------------------------------\n    parser.add_argument('--dominant_ratio', type=float, default=0.7, help='Fraction of dominant class per client (0-1)')\n\n    args = parser.parse_args(args=[])\n\n    # Auto-configure dataset\n    if args.dataset in DATASET_CONFIGS:\n        config = DATASET_CONFIGS[args.dataset]\n        args.num_classes = config['num_classes']\n        args.input_channels = config['input_channels']\n        args.class_names = config['class_names']\n    else:\n        raise ValueError(f\"Dataset {args.dataset} not configured!\")\n\n    return args\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" args.py updated!\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:36:10.644110Z","iopub.execute_input":"2025-11-11T02:36:10.644354Z","iopub.status.idle":"2025-11-11T02:36:10.654640Z","shell.execute_reply.started":"2025-11-11T02:36:10.644329Z","shell.execute_reply":"2025-11-11T02:36:10.653780Z"},"trusted":true},"outputs":[{"name":"stdout","text":" args.py updated!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedProx-PyTorch/get_data.py\"\n\nnew_code = r'''\n# ========================================\n# get_data.py â€” FedProx MedMNIST Loader with True Balanced Non-IID\n# Supports: pathmnist, tissuemnist, organamnist, octmnist\n# ========================================\nimport os\nimport numpy as np\nimport torch\nimport pickle\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset\nfrom torchvision import transforms\nfrom medmnist.dataset import PathMNIST, TissueMNIST, OrganAMNIST, OCTMNIST\n\nDATASET_MAP = {\n    'pathmnist': PathMNIST,\n    'tissuemnist': TissueMNIST,\n    'organamnist': OrganAMNIST,\n    'octmnist': OCTMNIST\n}\n\ndef get_transforms():\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.Grayscale(num_output_channels=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ])\n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ])\n    return train_transform, test_transform\n\ndef balanced_noniid_split(dataset, num_clients, dominant_ratio=0.7):\n    labels = np.array([dataset[i][1].item() for i in range(len(dataset))])\n    num_classes = len(np.unique(labels))\n    client_indices = [[] for _ in range(num_clients)]\n\n    class_indices = {c: np.where(labels==c)[0].tolist() for c in range(num_classes)}\n    for c in class_indices:\n        np.random.shuffle(class_indices[c])\n\n    # dominant_ratio to main client\n    for client_id in range(num_clients):\n        dominant_class = client_id % num_classes\n        n_dom = int(len(class_indices[dominant_class]) * dominant_ratio)\n        client_indices[client_id].extend(class_indices[dominant_class][:n_dom])\n        class_indices[dominant_class] = class_indices[dominant_class][n_dom:]\n\n    # remaining samples distributed\n    for c in range(num_classes):\n        rem = class_indices[c]\n        other_clients = [i for i in range(num_clients) if i%num_classes != c]\n        for i, idx in enumerate(rem):\n            client_indices[other_clients[i % len(other_clients)]].append(idx)\n\n    for i in range(num_clients):\n        np.random.shuffle(client_indices[i])\n    return client_indices\n\ndef load_medmnist_data(args):\n    train_transform, test_transform = get_transforms()\n    data_root = './data/medmnist'\n    os.makedirs(data_root, exist_ok=True)\n\n    DatasetClass = DATASET_MAP[args.dataset.lower()]\n\n    train_dataset = DatasetClass(root=data_root, split='train', download=True, transform=train_transform)\n    val_dataset = DatasetClass(root=data_root, split='val', download=True, transform=train_transform)\n    test_dataset = DatasetClass(root=data_root, split='test', download=True, transform=test_transform)\n\n    combined_train = ConcatDataset([train_dataset, val_dataset])\n\n    cache_file = f'./data/medmnist/client_indices_{args.dataset}_K{args.K}_dr{args.dominant_ratio}_fedprox.pkl'\n    if os.path.exists(cache_file):\n        with open(cache_file,'rb') as f:\n            client_indices = pickle.load(f)\n    else:\n        client_indices = balanced_noniid_split(combined_train, args.K, dominant_ratio=args.dominant_ratio)\n        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n        with open(cache_file,'wb') as f:\n            pickle.dump(client_indices,f)\n\n    client_loaders = [DataLoader(Subset(combined_train, idx), batch_size=args.B, shuffle=True, num_workers=0, pin_memory=True)\n                      for idx in client_indices]\n    test_loader = DataLoader(test_dataset, batch_size=args.B, shuffle=False, num_workers=0, pin_memory=True)\n    return client_loaders, test_loader\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" get_data.py updated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:36:10.816497Z","iopub.execute_input":"2025-11-11T02:36:10.816766Z","iopub.status.idle":"2025-11-11T02:36:10.823603Z","shell.execute_reply.started":"2025-11-11T02:36:10.816748Z","shell.execute_reply":"2025-11-11T02:36:10.822882Z"},"trusted":true},"outputs":[{"name":"stdout","text":" get_data.py updated!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install torch torchvision ftfy regex tqdm","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:36:10.824971Z","iopub.execute_input":"2025-11-11T02:36:10.825259Z","iopub.status.idle":"2025-11-11T02:37:25.278105Z","shell.execute_reply.started":"2025-11-11T02:36:10.825236Z","shell.execute_reply":"2025-11-11T02:37:25.276964Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nCollecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedProx-PyTorch/client.py\"\n\nnew_code = \"\"\"\n# ========================================\n# client.py â€” Client-side helper for FedProx (with Proximal Term + tqdm)\n# ========================================\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport copy\n\n\nclass Client:\n    def __init__(self, model, train_loader, device, val_loader=None, lr=0.0001, \n                 weight_decay=5e-4, mu=0.01, global_params=None):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.mu = mu\n        \n        if global_params is not None:\n            self.global_params = [param.clone().detach() for param in global_params]\n        else:\n            self.global_params = None\n\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.lr, \n            momentum=0.9, \n            weight_decay=self.weight_decay\n        )\n\n    def compute_accuracy(self, loader):\n        self.model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for images, labels in loader:\n                images = images.to(self.device)\n                labels = labels.to(self.device).squeeze()\n                \n                outputs = self.model(images)\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                \n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        return 100 * correct / total if total > 0 else 0\n\n    def train(self, epochs=1):\n        model_name = self.model.name\n        mu_value = self.mu\n        print(f\"\\\\n Training {model_name} with FedProx objective (Î¼={mu_value})...\")\n        \n        for epoch in range(epochs):\n            self.model.train()\n            total_loss = 0.0\n\n            pbar = tqdm(\n                self.train_loader,\n                desc=f\"  Epoch {epoch+1}/{epochs}\",\n                ncols=100,\n                leave=False\n            )\n            \n            for images, labels in pbar:\n                images = images.to(self.device)\n                labels = labels.to(self.device).squeeze()\n\n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                \n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                \n                loss = self.criterion(outputs, labels)\n\n                # FedProx proximal term\n                if self.global_params is not None:\n                    prox_term = 0.0\n                    for param, global_param in zip(self.model.parameters(), self.global_params):\n                        prox_term += torch.norm(param - global_param.to(self.device)) ** 2\n                    loss += (self.mu / 2) * prox_term\n\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n            avg_loss = total_loss / len(self.train_loader)\n            train_acc = self.compute_accuracy(self.train_loader)\n            log_msg = f\"  Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\"\n\n            if self.val_loader is not None:\n                val_acc = self.compute_accuracy(self.val_loader)\n                val_loss = 0\n                self.model.eval()\n                with torch.no_grad():\n                    for images, labels in self.val_loader:\n                        images = images.to(self.device)\n                        labels = labels.to(self.device).squeeze()\n                        \n                        outputs = self.model(images)\n                        if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                            outputs = outputs.logits\n                        \n                        loss = self.criterion(outputs, labels)\n                        val_loss += loss.item()\n                \n                val_loss /= len(self.val_loader)\n                log_msg += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n\n            print(log_msg)\n\n        final_msg = f\" {model_name} local training complete (FedProx)\"\n        print(final_msg)\n        print(\"\")\n        return self.model.state_dict()\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n    \nprint(\"client.py updated!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:37:25.279310Z","iopub.execute_input":"2025-11-11T02:37:25.279707Z","iopub.status.idle":"2025-11-11T02:37:25.289801Z","shell.execute_reply.started":"2025-11-11T02:37:25.279679Z","shell.execute_reply":"2025-11-11T02:37:25.289043Z"},"trusted":true},"outputs":[{"name":"stdout","text":"client.py updated!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# First, install CLIP\n\n\n# Then update model.py\nfile_path = \"/kaggle/working/FedProx-PyTorch/model.py\"\n\nnew_code = '''\n# ========================================\n# model.py â€” CLIP-based FedProx Classifier (Fixed dtype)\n# ========================================\nimport torch\nfrom torch import nn\nimport clip\n\nclass CLIPFedProxClassifier(nn.Module):\n    \"\"\"\n    CLIP-based classifier for FedProx\n    Shared CLIP backbone + trainable head\n    \"\"\"\n\n    def __init__(self, args, name='clip_fedprox_model'):\n        super().__init__()\n        self.name = name\n        self.num_classes = args.num_classes\n        self.dropout = getattr(args, 'dropout', 0.5)\n\n        # Load CLIP\n        self.clip_model, self.preprocess = clip.load(args.clip_model, device=args.device)\n        # Freeze CLIP backbone\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n\n        self.feature_dim = self.clip_model.visual.output_dim\n        self.class_names = args.class_names\n\n        # Trainable head\n        self.head = nn.Sequential(\n            nn.Linear(self.feature_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(self.dropout),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(self.dropout * 0.7),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(self.dropout * 0.5),\n\n            nn.Linear(256, self.num_classes)\n        )\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.head.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, images):\n        with torch.no_grad():\n            feats = self.clip_model.encode_image(images)\n            feats = feats / feats.norm(dim=-1, keepdim=True)\n        \n        # **FIX: Convert features to float32 to match head dtype**\n        feats = feats.float()\n        \n        logits = self.head(feats)\n        return logits\n\n    def get_trainable_params(self):\n        return [p for p in self.head.parameters() if p.requires_grad]\n\n'''\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" model.py created!\")\nprint(f\" File saved at: {file_path}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:37:25.290746Z","iopub.execute_input":"2025-11-11T02:37:25.291007Z","iopub.status.idle":"2025-11-11T02:37:29.383970Z","shell.execute_reply.started":"2025-11-11T02:37:25.290984Z","shell.execute_reply":"2025-11-11T02:37:29.383146Z"},"trusted":true},"outputs":[{"name":"stdout","text":" model.py created!\n File saved at: /kaggle/working/FedProx-PyTorch/model.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedProx-PyTorch/server.py\"\n\nnew_code = \"\"\"\n# ========================================\n# server.py â€” FedProx Server with Resume Support\n# ========================================\nimport copy\nimport random\nimport numpy as np\nimport torch\nimport os\nimport json\nfrom model import CLIPFedProxClassifier as ImageClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\nfrom get_data import load_medmnist_data\nfrom client import Client\n\nclass FedProxServer:\n    def __init__(self, args, resume_from=None):\n        self.args = args\n        os.makedirs(args.checkpoint_dir, exist_ok=True)\n        self.current_round = 0\n        self.best_global_acc = 0\n        self.history = {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []}\n        self.global_model = ImageClassifier(args, name=\"server\").to(args.device)\n        self.client_models = []\n        for i in range(self.args.K):\n            model = copy.deepcopy(self.global_model)\n            model.name = f\"Client_{i}\"\n            self.client_models.append(model)\n        self.client_loaders, self.test_loader = load_medmnist_data(args)\n        if resume_from:\n            self.load_checkpoint(resume_from)\n\n    def load_checkpoint(self, checkpoint_path):\n        print(f\"\\\\n Loading checkpoint from: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location=self.args.device, weights_only=False)\n        self.global_model.load_state_dict(checkpoint['server_state_dict'])\n        if 'client_state_dicts' in checkpoint:\n            for i, state_dict in enumerate(checkpoint['client_state_dicts']):\n                self.client_models[i].load_state_dict(state_dict)\n        self.current_round = checkpoint.get('round', 0)\n        self.best_global_acc = checkpoint.get('best_global_acc', 0)\n        self.history = checkpoint.get('history', {'rounds': [], 'avg_accuracy': [], 'best_accuracy': []})\n\n    def dispatch(self, selected_clients):\n        for idx in selected_clients:\n            client_model = self.client_models[idx]\n            for client_param, global_param in zip(client_model.parameters(), self.global_model.parameters()):\n                client_param.data = global_param.data.clone()\n\n    def aggregate(self, selected_clients):\n        total_samples = sum([len(self.client_loaders[idx].dataset) for idx in selected_clients])\n        global_params = {k: torch.zeros_like(v.data) for k, v in self.global_model.named_parameters()}\n        for idx in selected_clients:\n            weight = len(self.client_loaders[idx].dataset) / total_samples\n            client_params = dict(self.client_models[idx].named_parameters())\n            for k in global_params.keys():\n                global_params[k] += client_params[k].data * weight\n        for k, v in self.global_model.named_parameters():\n            v.data = global_params[k].data.clone()\n\n    def client_update(self, idx):\n        client_model = self.client_models[idx]\n        client_loader = self.client_loaders[idx]\n        client_obj = Client(model=client_model,\n                            train_loader=client_loader,\n                            device=self.args.device,\n                            lr=self.args.lr,\n                            weight_decay=self.args.weight_decay,\n                            mu=self.args.mu,\n                            global_params=self.global_model.parameters())\n        client_obj.train(epochs=self.args.E)\n\n    def test_global_model(self):\n        self.global_model.eval()\n        all_labels = []\n        all_preds = []\n        with torch.no_grad():\n            for images, labels in self.test_loader:\n                images, labels = images.to(self.args.device), labels.to(self.args.device)\n                labels = labels.squeeze()\n                outputs = self.global_model(images)\n                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):\n                    outputs = outputs.logits\n                _, predicted = torch.max(outputs, 1)\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(predicted.cpu().numpy())\n        all_labels = np.array(all_labels)\n        all_preds = np.array(all_preds)\n        acc = 100 * np.mean(all_labels == all_preds)\n        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n        print(f\" Global Test â€” Acc: {acc:.2f}% | Prec: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | RMSE: {rmse:.3f}\")\n        return acc\n    \n\n    def save_checkpoint(self, round_num, avg_acc):\n        checkpoint = {\n            'round': round_num,\n            'server_state_dict': self.global_model.state_dict(),\n            'client_state_dicts': [model.state_dict() for model in self.client_models],\n            'best_global_acc': self.best_global_acc,\n            'history': self.history,\n            'args': vars(self.args)\n        }\n        \n        torch.save(checkpoint, os.path.join(self.args.checkpoint_dir, 'checkpoint_latest.pth'))\n        \n        with open(os.path.join(self.args.checkpoint_dir, 'training_history.json'), 'w') as f:\n            json.dump(self.history, f, indent=4)\n\n    def run(self):\n        start_round = self.current_round\n        for r in range(start_round, self.args.r):\n            m = max(int(self.args.C * self.args.K), 1)\n            selected_clients = random.sample(range(self.args.K), m)\n            self.dispatch(selected_clients)\n            for idx in selected_clients:\n                self.client_update(idx)\n            self.aggregate(selected_clients)\n            avg_acc = self.test_global_model()\n            self.history['rounds'].append(r+1)\n            self.history['avg_accuracy'].append(avg_acc)\n            self.history['best_accuracy'].append(max(self.best_global_acc, avg_acc))\n            if avg_acc > self.best_global_acc:\n                self.best_global_acc = avg_acc\n            self.current_round = r + 1\n            self.save_checkpoint(r+1, avg_acc)\n        return self.global_model\n\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" server.py created!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:37:29.386688Z","iopub.execute_input":"2025-11-11T02:37:29.386913Z","iopub.status.idle":"2025-11-11T02:37:29.583496Z","shell.execute_reply.started":"2025-11-11T02:37:29.386895Z","shell.execute_reply":"2025-11-11T02:37:29.582550Z"},"trusted":true},"outputs":[{"name":"stdout","text":" server.py created!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"file_path = \"/kaggle/working/FedProx-PyTorch/main.py\"\n\nnew_code = \"\"\"\n# ========================================\n# main.py â€” Run FedProx with Resume Support\n# ========================================\nfrom args import args_parser\nfrom server import FedProxServer\nfrom get_data import load_medmnist_data\nimport torch\nimport os\nimport json\nimport matplotlib.pyplot as plt\n\ndef plot_training_history(history, save_path):\n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Average Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history['rounds'], history['avg_accuracy'], 'b-', label='Avg Accuracy', linewidth=2)\n    plt.plot(history['rounds'], history['best_accuracy'], 'r--', label='Best Accuracy', linewidth=2)\n    plt.xlabel('Communication Round')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Federated Learning Accuracy Progress')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Accuracy Improvement\n    plt.subplot(1, 2, 2)\n    if len(history['avg_accuracy']) > 1:\n        improvements = [history['avg_accuracy'][i] - history['avg_accuracy'][i-1] \n                        for i in range(1, len(history['avg_accuracy']))]\n        plt.bar(history['rounds'][1:], improvements, alpha=0.7)\n        plt.axhline(y=0, color='r', linestyle='-', linewidth=0.5)\n        plt.xlabel('Communication Round')\n        plt.ylabel('Accuracy Change (%)')\n        plt.title('Round-to-Round Accuracy Change')\n        plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f'  Training plot saved at: {save_path}')\n\ndef main():\n    # Load arguments from args.py\n    args = args_parser()\n    \n    # Print configuration\n    print('\\\\n' + '='*60)\n    print(' FEDERATED LEARNING CONFIGURATION')\n    print('='*60)\n    print(f'Dataset: {args.dataset.upper()}')\n    print(f'Clients: {args.K} | Rounds: {args.r} | Local Epochs: {args.E}')\n    print(f'Batch Size: {args.B} | Learning Rate: {args.lr}')\n    print(f'FedProx Î¼: {args.mu} | Weight Decay: {args.weight_decay}')\n    print(f'CLIP Model: {args.clip_model} | Device: {args.device}')\n    print(f'Non-IID: {args.dominant_ratio*100:.0f}% dominant class per client')\n    print('='*60)\n    \n    # Load client loaders & test loader\n    client_loaders, test_loader = load_medmnist_data(args)\n    \n    # Check for existing checkpoint\n    latest_checkpoint = os.path.join(args.checkpoint_dir, 'checkpoint_latest.pth')\n    resume_from = None\n    \n    if os.path.exists(latest_checkpoint):\n        print('\\\\n' + '='*60)\n        print(' CHECKPOINT FOUND!')\n        print('='*60)\n        try:\n            checkpoint = torch.load(latest_checkpoint, map_location=args.device)\n            completed_rounds = checkpoint.get('round', 0)\n            best_acc = checkpoint.get('best_global_acc', 0)\n            print(f' Checkpoint Details:')\n            print(f'   - Completed Rounds: {completed_rounds}/{args.r}')\n            print(f'   - Best Accuracy: {best_acc:.2f}%')\n            user_input = input('\\\\n  Resume from checkpoint? (y/n): ').strip().lower()\n            if user_input == 'y':\n                resume_from = latest_checkpoint\n                print(' Resuming from checkpoint...')\n            else:\n                print(' Starting fresh training...')\n        except Exception as e:\n            print(f' Error loading checkpoint: {e}')\n            print(' Starting fresh training...')\n    else:\n        print('\\\\n' + '='*60)\n        print(' No checkpoint found. Starting fresh training...')\n        print('='*60)\n    \n    # Initialize FedProx server\n    server = FedProxServer(args, resume_from=resume_from)\n    \n    # Run federated training\n    final_model = server.run()\n    \n    # Save final global model\n    final_model_path = os.path.join(args.checkpoint_dir, 'final_global_model.pth')\n    torch.save(final_model.state_dict(), final_model_path)\n    print(f'\\\\n Final global model saved at: {final_model_path}')\n    \n    # Save best model if exists\n    best_model_path = os.path.join(args.checkpoint_dir, 'best_model.pth')\n    if os.path.exists(best_model_path):\n        print(f' Best global model saved at: {best_model_path}')\n    \n    # Save training history\n    history_path = os.path.join(args.checkpoint_dir, 'training_history.json')\n    with open(history_path, 'w') as f:\n        json.dump(server.history, f, indent=4)\n    print(f' Training history saved at: {history_path}')\n    \n    # Plot training curves\n    plot_path = os.path.join(args.checkpoint_dir, 'training_plot.png')\n    plot_training_history(server.history, plot_path)\n    \n    # Print summary\n    print('\\\\n' + '='*60)\n    print(' TRAINING SUMMARY')\n    print('='*60)\n    print(f' Best Global Accuracy: {server.best_global_acc:.2f}%')\n    print(f' Final Round Accuracy: {server.history[\"avg_accuracy\"][-1]:.2f}%')\n    print(f' Total Improvement: {server.history[\"avg_accuracy\"][-1] - server.history[\"avg_accuracy\"][0]:.2f}%')\n    print(f' All checkpoints saved in: {args.checkpoint_dir}')\n    print('='*60)\n\nif __name__ == '__main__':\n    main()\n\"\"\"\n\nwith open(file_path, \"w\") as f:\n    f.write(new_code)\n\nprint(\" main.py created!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:37:29.584406Z","iopub.execute_input":"2025-11-11T02:37:29.584656Z","iopub.status.idle":"2025-11-11T02:37:29.905993Z","shell.execute_reply.started":"2025-11-11T02:37:29.584626Z","shell.execute_reply":"2025-11-11T02:37:29.905164Z"},"trusted":true},"outputs":[{"name":"stdout","text":" main.py created!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\nimport clip\nprint(clip.available_models())\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:37:29.906858Z","iopub.execute_input":"2025-11-11T02:37:29.907131Z","iopub.status.idle":"2025-11-11T02:37:43.311825Z","shell.execute_reply.started":"2025-11-11T02:37:29.907106Z","shell.execute_reply":"2025-11-11T02:37:43.311086Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-lv65amdl\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-lv65amdl\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=d0274ad70ca7c4a00b7d841a790186fabc947ca0e2fac08308c4d53015a44669\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xhwyukhi/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\n['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install medmnist --quiet\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:37:43.312632Z","iopub.execute_input":"2025-11-11T02:37:43.313011Z","iopub.status.idle":"2025-11-11T02:37:47.078072Z","shell.execute_reply.started":"2025-11-11T02:37:43.312987Z","shell.execute_reply":"2025-11-11T02:37:47.077284Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!python /kaggle/working/FedProx-PyTorch/main.py\n","metadata":{"execution":{"iopub.status.busy":"2025-11-11T02:37:47.079172Z","iopub.execute_input":"2025-11-11T02:37:47.079584Z","execution_failed":"2025-11-11T02:58:27.277Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n============================================================\n FEDERATED LEARNING CONFIGURATION\n============================================================\nDataset: ORGANAMNIST\nClients: 5 | Rounds: 50 | Local Epochs: 5\nBatch Size: 32 | Learning Rate: 0.003\nFedProx Î¼: 0.05 | Weight Decay: 0.0005\nCLIP Model: ViT-B/32 | Device: cuda\nNon-IID: 70% dominant class per client\n============================================================\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.2M/38.2M [00:25<00:00, 1.52MB/s]\n\n============================================================\n No checkpoint found. Starting fresh training...\n============================================================\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:03<00:00, 91.9MiB/s]\n\n Training Client_4 with FedProx objective (Î¼=0.05)...\n  Epoch  1/5 | Loss: 1.5745 | Train Acc: 72.98%                                                     \n  Epoch  2/5 | Loss: 1.2934 | Train Acc: 77.78%                                                     \n  Epoch  3/5 | Loss: 1.2739 | Train Acc: 79.50%                                                     \n  Epoch  4/5 | Loss: 1.2722 | Train Acc: 80.27%                                                     \n  Epoch  5/5 | Loss: 1.2733 | Train Acc: 81.18%                                                     \n Client_4 local training complete (FedProx)\n\n\n Training Client_1 with FedProx objective (Î¼=0.05)...\n  Epoch  1/5 | Loss: 1.5769 | Train Acc: 76.55%                                                     \n  Epoch  2/5 | Loss: 1.2709 | Train Acc: 80.89%                                                     \n  Epoch  3/5 | Loss: 1.2303 | Train Acc: 81.64%                                                     \n  Epoch  4/5 | Loss: 1.2236 | Train Acc: 83.71%                                                     \n  Epoch  5/5 | Loss: 1.2054 | Train Acc: 83.79%                                                     \n Client_1 local training complete (FedProx)\n\n\n Training Client_2 with FedProx objective (Î¼=0.05)...\n  Epoch  1/5 | Loss: 1.5917 | Train Acc: 77.07%                                                     \n  Epoch  2/5 | Loss: 1.3379 | Train Acc: 80.58%                                                     \n  Epoch  3/5 | Loss: 2.6495 | Train Acc: 82.05%                                                     \n  Epoch  4/5 | Loss: 9.7382 | Train Acc: 82.64%                                                     \n  Epoch  5/5 | Loss: 5.5779 | Train Acc: 83.28%                                                     \n Client_2 local training complete (FedProx)\n\n\n Training Client_3 with FedProx objective (Î¼=0.05)...\n  Epoch  1/5 | Loss: 1.5529 | Train Acc: 76.35%                                                     \n  Epoch  2/5 | Loss: 1.2276 | Train Acc: 81.02%                                                     \n  Epoch  3/5 | Loss: 1.2032 | Train Acc: 82.89%                                                     \n  Epoch  4/5 | Loss: 1.1838 | Train Acc: 84.50%                                                     \n  Epoch  5/5 | Loss: 1.1829 | Train Acc: 84.32%                                                     \n Client_3 local training complete (FedProx)\n\n\n Training Client_0 with FedProx objective (Î¼=0.05)...\n  Epoch  1/5 | Loss: 1.5877 | Train Acc: 74.68%                                                     \n  Epoch  2/5 | Loss: 1.3068 | Train Acc: 79.21%                                                     \n  Epoch  3/5 | Loss: 1.2817 | Train Acc: 80.58%                                                     \n  Epoch  4/5 | Loss: 1.2633 | Train Acc: 81.89%                                                     \n  Epoch  5/5 | Loss: 1.2855 | Train Acc: 81.54%                                                     \n Client_0 local training complete (FedProx)\n\n Global Test â€” Acc: 21.68% | Prec: 0.421 | Recall: 0.172 | F1: 0.145 | RMSE: 2.829\n\n Training Client_1 with FedProx objective (Î¼=0.05)...\n  Epoch  1/5 | Loss: 0.8152 | Train Acc: 85.04%                                                     \n  Epoch  2/5 | Loss: 0.7902 | Train Acc: 85.99%                                                     \n  Epoch  3/5 | Loss: 0.7872 | Train Acc: 86.14%                                                     \n  Epoch  4/5 | Loss: 0.7944 | Train Acc: 86.44%                                                     \n                                                                                                    \r","output_type":"stream"}],"execution_count":null}]}